[
    {
        "id": "1801.04064",
        "title": "state variation mining: on information divergence with message   importance in big data",
        "abstract": "information transfer which reveals the state variation of variables usually plays a vital role in big data analytics and processing. in fact, the measures for information transfer could reflect the system change by use of the variable distributions, similar to kl divergence and renyi divergence. furthermore, in terms of the information transfer in big data, small probability events usually dominate the importance of the total message to some degree. therefore, it is significant to design an information transfer measure based on the message importance which emphasizes the small probability events. in this paper, we propose a message importance transfer measure (mitm) and investigate its characteristics and applications on three aspects. first, the message importance transfer capacity based on mitm is presented to offer an upper bound for the information transfer process with disturbance. then, we extend the mitm to the continuous case and discuss the robustness by using it to measuring information distance. finally, we utilize the mitm to guide the queue length selection in the caching operation of mobile edge computing.",
        "doi": "10.1109/glocom.2018.8647882",
        "created": "2018-01-12",
        "url": "https://arxiv.org/abs/1801.04064",
        "authors": [
            "rui she",
            "shanyun liu",
            "pingyi fan"
        ]
    },
    {
        "id": "1808.02933",
        "title": "sequential monte carlo bandits",
        "abstract": "we extend bayesian multi-armed bandit (mab) algorithms beyond their original setting by making use of sequential monte carlo (smc) methods.   a mab is a sequential decision making problem where the goal is to learn a policy that maximizes long term payoff, where only the reward of the executed action is observed. in the stochastic mab, the reward for each action is generated from an unknown distribution, often assumed to be stationary. to decide which action to take next, a mab agent must learn the characteristics of the unknown reward distribution, e.g., compute its sufficient statistics. however, closed-form expressions for these statistics are analytically intractable except for simple, stationary cases.   we here utilize smc for estimation of the statistics bayesian mab agents compute, and devise flexible policies that can address a rich class of bandit problems: i.e., mabs with nonlinear, stateless- and context-dependent reward distributions that evolve over time. we showcase how non-stationary bandits, where time dynamics are modeled via linear dynamical systems, can be successfully addressed by smc-based bayesian bandit agents. we empirically demonstrate good regret performance of the proposed smc-based bandit policies in several mab scenarios that have remained elusive, i.e., in non-stationary bandits with nonlinear rewards.",
        "doi": "",
        "created": "2018-08-08",
        "url": "https://arxiv.org/abs/1808.02933",
        "authors": [
            "i\u00f1igo urteaga",
            "chris h. wiggins"
        ]
    },
    {
        "id": "1907.01136",
        "title": "finding outliers in gaussian model-based clustering",
        "abstract": "clustering, or unsupervised classification, is a task often plagued by outliers. yet there is a paucity of work on handling outliers in clustering. outlier identification algorithms tend to fall into three broad categories: outlier inclusion, outlier trimming, and \\textit{post hoc} outlier identification methods, with the former two often requiring pre-specification of the number of outliers. the fact that sample mahalanobis distance is beta-distributed is used to derive an approximate distribution for the log-likelihoods of subset finite gaussian mixture models. an algorithm is then proposed that removes the least plausible points according to the subset log-likelihoods, which are deemed outliers, until the subset log-likelihoods adhere to the reference distribution. this results in a trimming method, called oclust, that inherently estimates the number of outliers.",
        "doi": "",
        "created": "2019-07-01",
        "url": "https://arxiv.org/abs/1907.01136",
        "authors": [
            "katharine m. clark",
            "paul d. mcnicholas"
        ]
    },
    {
        "id": "1907.02652",
        "title": "importance of small probability events in big data: information   measures, applications, and challenges",
        "abstract": "in many applications (e.g., anomaly detection and security systems) of smart cities, rare events dominate the importance of the total information of big data collected by internet of things (iots). that is, it is pretty crucial to explore the valuable information associated with the rare events involved in minority subsets of the voluminous amounts of data. to do so, how to effectively measure the information with importance of the small probability events from the perspective of information theory is a fundamental question. this paper first makes a survey of some theories and models with respect to importance measures and investigates the relationship between subjective or semantic importance and rare events in big data. moreover, some applications for message processing and data analysis are discussed in the viewpoint of information measures. in addition, based on rare events detection, some open challenges related to information measures, such as smart cities, autonomous driving, and anomaly detection in iots, are introduced which can be considered as future research directions.",
        "doi": "10.1109/access.2019.2926518",
        "created": "2019-07-04",
        "url": "https://arxiv.org/abs/1907.02652",
        "authors": [
            "rui she",
            "shanyun liu",
            "shuo wan",
            "ke xiong",
            "pingyi fan"
        ]
    },
    {
        "id": "2012.09561",
        "title": "estimating mixed-memberships using the symmetric laplacian inverse   matrix",
        "abstract": "mixed membership community detection is a challenging problem. in this paper, to detect mixed memberships, we propose a new method mixed-slim which is a spectral clustering method on the symmetrized laplacian inverse matrix under the degree-corrected mixed membership model. we provide theoretical bounds for the estimation error on the proposed algorithm and its regularized version under mild conditions. meanwhile, we provide some extensions of the proposed method to deal with large networks in practice. these mixed-slim methods outperform state-of-art methods in simulations and substantial empirical datasets for both community detection and mixed membership community detection problems.",
        "doi": "",
        "created": "2020-12-17",
        "url": "https://arxiv.org/abs/2012.09561",
        "authors": [
            "huan qing",
            "jingli wang"
        ]
    },
    {
        "id": "2102.00618",
        "title": "monotone additive statistics",
        "abstract": "the expectation is an example of a descriptive statistic that is monotone with respect to stochastic dominance, and additive for sums of independent random variables. we provide a complete characterization of such statistics, and explore a number of applications to models of individual and group decision-making. these include a representation of stationary monotone time preferences, extending the work of fishburn and rubinstein (1982) to time lotteries. this extension offers a new perspective on risk attitudes toward time, as well as on the aggregation of multiple discount factors. we also offer a novel class of nonexpected utility preferences over gambles which satisfy invariance to background risk as well as betweenness, but are versatile enough to capture mixed risk attitudes.",
        "doi": "",
        "created": "2021-01-31",
        "url": "https://arxiv.org/abs/2102.00618",
        "authors": [
            "xiaosheng mu",
            "luciano pomatto",
            "philipp strack",
            "omer tamuz"
        ]
    },
    {
        "id": "2112.04389",
        "title": "mixed membership distribution-free model",
        "abstract": "we consider the problem of community detection in overlapping weighted networks, where nodes can belong to multiple communities and edge weights can be finite real numbers. to model such complex networks, we propose a general framework - the mixed membership distribution-free (mmdf) model. mmdf has no distribution constraints of edge weights and can be viewed as generalizations of some previous models, including the well-known mixed membership stochastic blockmodels. especially, overlapping signed networks with latent community structures can also be generated from our model. we use an efficient spectral algorithm with a theoretical guarantee of convergence rate to estimate community memberships under the model. we also propose the fuzzy weighted modularity to evaluate the quality of community detection for overlapping weighted networks with positive and negative edge weights. we then provide a method to determine the number of communities for weighted networks by taking advantage of our fuzzy weighted modularity. numerical simulations and real data applications are carried out to demonstrate the usefulness of our mixed membership distribution-free model and our fuzzy weighted modularity.",
        "doi": "",
        "created": "2021-12-04",
        "url": "https://arxiv.org/abs/2112.04389",
        "authors": [
            "huan qing",
            "jingli wang"
        ]
    },
    {
        "id": "2206.01012",
        "title": "bayesian high-dimensional covariate selection in non-linear   mixed-effects models using the saem algorithm",
        "abstract": "high-dimensional variable selection, with many more covariates than observations, is widely documented in standard regression models, but there are still few tools to address it in non-linear mixed-effects models where data are collected repeatedly on several individuals. in this work, variable selection is approached from a bayesian perspective and a selection procedure is proposed, combining the use of a spike-and-slab prior and the stochastic approximation version of the expectation maximisation (saem) algorithm. similarly to lasso regression, the set of relevant covariates is selected by exploring a grid of values for the penalisation parameter. the saem approach is much faster than a classical mcmc (markov chain monte carlo) algorithm and our method shows very good selection performances on simulated data. its flexibility is demonstrated by implementing it for a variety of nonlinear mixed effects models. the usefulness of the proposed method is illustrated on a problem of genetic markers identification, relevant for genomic-assisted selection in plant breeding.",
        "doi": "10.1007/s11222-023-10367-4",
        "created": "2022-06-02",
        "url": "https://arxiv.org/abs/2206.01012",
        "authors": [
            "marion naveau",
            "guillaume kon kam king",
            "renaud rincent",
            "laure sansonnet",
            "maud delattre"
        ]
    },
    {
        "id": "2207.14800",
        "title": "contrastive ucb: provably efficient contrastive self-supervised learning   in online reinforcement learning",
        "abstract": "in view of its power in extracting feature representation, contrastive self-supervised learning has been successfully integrated into the practice of (deep) reinforcement learning (rl), leading to efficient policy learning in various applications. despite its tremendous empirical successes, the understanding of contrastive learning for rl remains elusive. to narrow such a gap, we study how rl can be empowered by contrastive learning in a class of markov decision processes (mdps) and markov games (mgs) with low-rank transitions. for both models, we propose to extract the correct feature representations of the low-rank model by minimizing a contrastive loss. moreover, under the online setting, we propose novel upper confidence bound (ucb)-type algorithms that incorporate such a contrastive loss with online rl algorithms for mdps or mgs. we further theoretically prove that our algorithm recovers the true representations and simultaneously achieves sample efficiency in learning the optimal policy and nash equilibrium in mdps and mgs. we also provide empirical studies to demonstrate the efficacy of the ucb-based contrastive learning method for rl. to the best of our knowledge, we provide the first provably efficient online rl algorithm that incorporates contrastive learning for representation learning. our codes are available at https://github.com/baichenjia/contrastive-ucb.",
        "doi": "",
        "created": "2022-07-29",
        "url": "https://arxiv.org/abs/2207.14800",
        "authors": [
            "shuang qiu",
            "lingxiao wang",
            "chenjia bai",
            "zhuoran yang",
            "zhaoran wang"
        ]
    },
    {
        "id": "2208.03017",
        "title": "invisible walls: exploration of microclimate effects on building energy   consumption in new york city",
        "abstract": "the reduction of greenhouse gases from buildings forms the cornerstone of policy to mitigate the effects of climate change. however, the automation of urban scale building energy modeling systems required to meet global urban demand has proven challenging due to the bespoke characteristics of each city. one such point of uniqueness between cities is that of urban microclimate, which may play a major role in altering the performance of energy efficiency in buildings. this research proposes a way to rapidly collect urban microclimate data through the utilization of satellite readings and climate reanalysis. we then demonstrate the potential utility of this data by composing an analysis against three years of monthly building energy consumption data from new york city. as a whole, microclimate in new york city may be responsible for large swings in urban energy consumption. we estimate that central park may reduce the electricity consumption of adjacent buildings by 5-10%, while vegetation overall seems to have no appreciable impact on gas consumption. we find that favorable urban microclimates may decrease the gas consumption of some buildings in new york by 71% while others may increase gas consumption by as much as 221%. additionally, microclimates may be responsible for the decrease of electricity consumption by 28.6% in regions or increases of 77% consumption in others. this work provides a method of curating global, high resolution microclimate data, allowing researchers to explore the invisible walls of urban microclimate which interact with the buildings around them.",
        "doi": "10.1016/j.scs.2022.104364",
        "created": "2022-08-05",
        "url": "https://arxiv.org/abs/2208.03017",
        "authors": [
            "thomas dougherty",
            "rishee jain"
        ]
    },
    {
        "id": "2211.00912",
        "title": "bipartite mixed membership distribution-free model. a novel model for   community detection in overlapping bipartite weighted networks",
        "abstract": "modeling and estimating mixed memberships for overlapping unipartite un-weighted networks has been well studied in recent years. however, to our knowledge, there is no model for a more general case, the overlapping bipartite weighted networks. to close this gap, we introduce a novel model, the bipartite mixed membership distribution-free (bimmdf) model. our model allows an adjacency matrix to follow any distribution as long as its expectation has a block structure related to node membership. in particular, bimmdf can model overlapping bipartite signed networks and it is an extension of many previous models, including the popular mixed membership stochastic blcokmodels. an efficient algorithm with a theoretical guarantee of consistent estimation is applied to fit bimmdf. we then obtain the separation conditions of bimmdf for different distributions. furthermore, we also consider missing edges for sparse networks. the advantage of bimmdf is demonstrated in extensive synthetic networks and eight real-world networks.",
        "doi": "",
        "created": "2022-11-02",
        "url": "https://arxiv.org/abs/2211.00912",
        "authors": [
            "huan qing",
            "jingli wang"
        ]
    },
    {
        "id": "2305.11046",
        "title": "difference of submodular minimization via dc programming",
        "abstract": "minimizing the difference of two submodular (ds) functions is a problem that naturally occurs in various machine learning problems. although it is well known that a ds problem can be equivalently formulated as the minimization of the difference of two convex (dc) functions, existing algorithms do not fully exploit this connection. a classical algorithm for dc problems is called the dc algorithm (dca). we introduce variants of dca and its complete form (cdca) that we apply to the dc program corresponding to ds minimization. we extend existing convergence properties of dca, and connect them to convergence properties on the ds problem. our results on dca match the theoretical guarantees satisfied by existing ds algorithms, while providing a more complete characterization of convergence properties. in the case of cdca, we obtain a stronger local minimality guarantee. our numerical results show that our proposed algorithms outperform existing baselines on two applications: speech corpus selection and feature selection.",
        "doi": "",
        "created": "2023-05-18",
        "url": "https://arxiv.org/abs/2305.11046",
        "authors": [
            "marwa el halabi",
            "george orfanides",
            "tim hoheisel"
        ]
    },
    {
        "id": "2306.05568",
        "title": "maximally machine-learnable portfolios",
        "abstract": "when it comes to stock returns, any form of predictability can bolster risk-adjusted profitability. we develop a collaborative machine learning algorithm that optimizes portfolio weights so that the resulting synthetic security is maximally predictable. precisely, we introduce mace, a multivariate extension of alternating conditional expectations that achieves the aforementioned goal by wielding a random forest on one side of the equation, and a constrained ridge regression on the other. there are two key improvements with respect to lo and mackinlay's original maximally predictable portfolio approach. first, it accommodates for any (nonlinear) forecasting algorithm and predictor set. second, it handles large portfolios. we conduct exercises at the daily and monthly frequency and report significant increases in predictability and profitability using very little conditioning information. interestingly, predictability is found in bad as well as good times, and mace successfully navigates the debacle of 2022.",
        "doi": "",
        "created": "2023-06-08",
        "url": "https://arxiv.org/abs/2306.05568",
        "authors": [
            "philippe goulet coulombe",
            "maximilian goebel"
        ]
    },
    {
        "id": "2306.14975",
        "title": "the underlying scaling laws and universal statistical structure of   complex datasets",
        "abstract": "we study universal traits which emerge both in real-world complex datasets, as well as in artificially generated ones. our approach is to analogize data to a physical system and employ tools from statistical physics and random matrix theory (rmt) to reveal their underlying structure. we focus on the feature-feature covariance matrix, analyzing both its local and global eigenvalue statistics. our main observations are: (i) the power-law scalings that the bulk of its eigenvalues exhibit are vastly different for uncorrelated normally distributed data compared to real-world data, (ii) this scaling behavior can be completely modeled by generating gaussian data with long range correlations, (iii) both generated and real-world datasets lie in the same universality class from the rmt perspective, as chaotic rather than integrable systems, (iv) the expected rmt statistical behavior already manifests for empirical covariance matrices at dataset sizes significantly smaller than those conventionally used for real-world training, and can be related to the number of samples required to approximate the population power-law scaling behavior, (v) the shannon entropy is correlated with local rmt structure and eigenvalues scaling, is substantially smaller in strongly correlated datasets compared to uncorrelated ones, and requires fewer samples to reach the distribution entropy. these findings show that with sufficient sample size, the gram matrix of natural image datasets can be well approximated by a wishart random matrix with a simple covariance structure, opening the door to rigorous studies of neural network dynamics and generalization which rely on the data gram matrix.",
        "doi": "",
        "created": "2023-06-26",
        "url": "https://arxiv.org/abs/2306.14975",
        "authors": [
            "noam levi",
            "yaron oz"
        ]
    },
    {
        "id": "2309.03842",
        "title": "early warning indicators via latent stochastic dynamical systems",
        "abstract": "detecting early warning indicators for abrupt dynamical transitions in complex systems or high-dimensional observation data is essential in many real-world applications, such as brain diseases, natural disasters, and engineering reliability. to this end, we develop a novel approach: the directed anisotropic diffusion map that captures the latent evolutionary dynamics in the low-dimensional manifold. then three effective warning signals (onsager-machlup indicator, sample entropy indicator, and transition probability indicator) are derived through the latent coordinates and the latent stochastic dynamical systems. to validate our framework, we apply this methodology to authentic electroencephalogram (eeg) data. we find that our early warning indicators are capable of detecting the tipping point during state transition. this framework not only bridges the latent dynamics with real-world data but also shows the potential ability for automatic labeling on complex high-dimensional time series.",
        "doi": "10.1063/5.0195042",
        "created": "2023-09-07",
        "url": "https://arxiv.org/abs/2309.03842",
        "authors": [
            "lingyu feng",
            "ting gao",
            "wang xiao",
            "jinqiao duan"
        ]
    },
    {
        "id": "2310.17496",
        "title": "tackling interference induced by data training loops in a/b tests: a   weighted training approach",
        "abstract": "in modern recommendation systems, the standard pipeline involves training machine learning models on historical data to predict user behaviors and improve recommendations continuously. however, these data training loops can introduce interference in a/b tests, where data generated by control and treatment algorithms, potentially with different distributions, are combined. to address these challenges, we introduce a novel approach called weighted training. this approach entails training a model to predict the probability of each data point appearing in either the treatment or control data and subsequently applying weighted losses during model training. we demonstrate that this approach achieves the least variance among all estimators that do not cause shifts in the training distributions. through simulation studies, we demonstrate the lower bias and variance of our approach compared to other methods.",
        "doi": "",
        "created": "2023-10-26",
        "url": "https://arxiv.org/abs/2310.17496",
        "authors": [
            "nian si"
        ]
    },
    {
        "id": "2311.09491",
        "title": "spatial bayesian neural networks",
        "abstract": "interpretable, and well understood models that are routinely employed even though, as is revealed through prior and posterior predictive checks, these can poorly characterise the spatial heterogeneity in the underlying process of interest. here, we propose a new, flexible class of spatial-process models, which we refer to as spatial bayesian neural networks (sbnns). an sbnn leverages the representational capacity of a bayesian neural network; it is tailored to a spatial setting by incorporating a spatial ``embedding layer'' into the network and, possibly, spatially-varying network parameters. an sbnn is calibrated by matching its finite-dimensional distribution at locations on a fine gridding of space to that of a target process of interest. that process could be easy to simulate from or we may have many realisations from it. we propose several variants of sbnns, most of which are able to match the finite-dimensional distribution of the target process at the selected grid better than conventional bnns of similar complexity. we also show that an sbnn can be used to represent a variety of spatial processes often used in practice, such as gaussian processes, lognormal processes, and max-stable processes. we briefly discuss the tools that could be used to make inference with sbnns, and we conclude with a discussion of their advantages and limitations.",
        "doi": "",
        "created": "2023-11-15",
        "url": "https://arxiv.org/abs/2311.09491",
        "authors": [
            "andrew zammit-mangion",
            "michael d. kaminski",
            "ba-hien tran",
            "maurizio filippone",
            "noel cressie"
        ]
    },
    {
        "id": "2311.12392",
        "title": "individualized dynamic model for multi-resolutional data with   application to mobile health",
        "abstract": "mobile health has emerged as a major success for tracking individual health status, due to the popularity and power of smartphones and wearable devices. this has also brought great challenges in handling heterogeneous, multi-resolution data which arise ubiquitously in mobile health due to irregular multivariate measurements collected from individuals. in this paper, we propose an individualized dynamic latent factor model for irregular multi-resolution time series data to interpolate unsampled measurements of time series with low resolution. one major advantage of the proposed method is the capability to integrate multiple irregular time series and multiple subjects by mapping the multi-resolution data to the latent space. in addition, the proposed individualized dynamic latent factor model is applicable to capturing heterogeneous longitudinal information through individualized dynamic latent factors. our theory provides a bound on the integrated interpolation error and the convergence rate for b-spline approximation methods. both the simulation studies and the application to smartwatch data demonstrate the superior performance of the proposed method compared to existing methods.",
        "doi": "",
        "created": "2023-11-21",
        "url": "https://arxiv.org/abs/2311.12392",
        "authors": [
            "jiuchen zhang",
            "fei xue",
            "qi xu",
            "jung-ah lee",
            "annie qu"
        ]
    },
    {
        "id": "2312.02033",
        "title": "asymptotic uniqueness in long-term prediction",
        "abstract": "this paper establishes the asymptotic uniqueness of long-term probability forecasts in the following form. consider two forecasters who repeatedly issue probability forecasts for the infinite future. the main result of the paper says that either at least one of the two forecasters will be discredited or their forecasts will converge in total variation. this can be regarded as a game-theoretic version of the classical blackwell-dubins result getting rid of some of its limitations. this result is further strengthened along the lines of richard jeffrey's radical probabilism.",
        "doi": "",
        "created": "2023-12-04",
        "url": "https://arxiv.org/abs/2312.02033",
        "authors": [
            "vladimir vovk"
        ]
    },
    {
        "id": "2401.11000",
        "title": "human-centric and integrative lighting asset management in public   libraries: qualitative insights and challenges from a swedish field study",
        "abstract": "traditional lighting source reliability evaluations, often covering just half of a lamp's volume, can misrepresent real-world performance. to overcome these limitations,adopting advanced asset management strategies for a more holistic evaluation is crucial. this paper investigates human-centric and integrative lighting asset management in swedish public libraries. through field observations, interviews, and gap analysis, the study highlights a disparity between current lighting conditions and stakeholder expectations, with issues like eye strain suggesting significant improvement potential. we propose a shift towards more dynamic lighting asset management and reliability evaluations, emphasizing continuous enhancement and comprehensive training in human-centric and integrative lighting principles.",
        "doi": "10.1109/access.2024.3377135",
        "created": "2024-01-19",
        "url": "https://arxiv.org/abs/2401.11000",
        "authors": [
            "jing lin",
            "per olof hedekvist",
            "nina mylly",
            "math bollen",
            "jingchun shen",
            "jiawei xiong",
            "christofer silfvenius"
        ]
    },
    {
        "id": "2402.01779",
        "title": "plug-and-play image restoration with stochastic denoising regularization",
        "abstract": "plug-and-play (pnp) algorithms are a class of iterative algorithms that address image inverse problems by combining a physical model and a deep neural network for regularization. even if they produce impressive image restoration results, these algorithms rely on a non-standard use of a denoiser on images that are less and less noisy along the iterations, which contrasts with recent algorithms based on diffusion models (dm), where the denoiser is applied only on re-noised images. we propose a new pnp framework, called stochastic denoising regularization (snore), which applies the denoiser only on images with noise of the adequate level. it is based on an explicit stochastic regularization, which leads to a stochastic gradient descent algorithm to solve ill-posed inverse problems. a convergence analysis of this algorithm and its annealing extension is provided. experimentally, we prove that snore is competitive with respect to state-of-the-art methods on deblurring and inpainting tasks, both quantitatively and qualitatively.",
        "doi": "",
        "created": "2024-02-01",
        "url": "https://arxiv.org/abs/2402.01779",
        "authors": [
            "marien renaud",
            "jean prost",
            "arthur leclaire",
            "nicolas papadakis"
        ]
    },
    {
        "id": "2402.04520",
        "title": "on computational limits of modern hopfield models: a fine-grained   complexity analysis",
        "abstract": "we investigate the computational limits of the memory retrieval dynamics of modern hopfield models from the fine-grained complexity analysis. our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern hopfield models based on the norm of patterns. specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. only below this criterion, sub-quadratic (efficient) variants of the modern hopfield model exist, assuming the strong exponential time hypothesis (seth). to showcase our theory, we provide a formal example of efficient constructions of modern hopfield models using low-rank approximation when the efficient criterion holds. this includes a derivation of a lower bound on the computational time, scaling linearly with $\\max\\{$\\# of stored memory patterns, length of input query sequence$\\}$. in addition, we prove its memory retrieval error bound and exponential memory capacity.",
        "doi": "",
        "created": "2024-02-06",
        "url": "https://arxiv.org/abs/2402.04520",
        "authors": [
            "jerry yao-chieh hu",
            "thomas lin",
            "zhao song",
            "han liu"
        ]
    },
    {
        "id": "2402.07087",
        "title": "self-correcting self-consuming loops for generative model training",
        "abstract": "as synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates \"self-consuming loops\" which may lead to training instability or even collapse, unless certain conditions are met. our paper aims to stabilize self-consuming generative model training. our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. we then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. we empirically validate the effectiveness of self-correcting self-consuming loops on the challenging human motion synthesis task, and observe that it successfully avoids model collapse, even when the ratio of synthetic data to real data is as high as 100%.",
        "doi": "",
        "created": "2024-02-10",
        "url": "https://arxiv.org/abs/2402.07087",
        "authors": [
            "nate gillman",
            "michael freeman",
            "daksh aggarwal",
            "chia-hong hsu",
            "calvin luo",
            "yonglong tian",
            "chen sun"
        ]
    },
    {
        "id": "2402.10291",
        "title": "an evaluation of real-time adaptive sampling change point detection   algorithm using kcusum",
        "abstract": "detecting abrupt changes in real-time data streams from scientific simulations presents a challenging task, demanding the deployment of accurate and efficient algorithms. identifying change points in live data stream involves continuous scrutiny of incoming observations for deviations in their statistical characteristics, particularly in high-volume data scenarios. maintaining a balance between sudden change detection and minimizing false alarms is vital. many existing algorithms for this purpose rely on known probability distributions, limiting their feasibility. in this study, we introduce the kernel-based cumulative sum (kcusum) algorithm, a non-parametric extension of the traditional cumulative sum (cusum) method, which has gained prominence for its efficacy in online change point detection under less restrictive conditions. kcusum splits itself by comparing incoming samples directly with reference samples and computes a statistic grounded in the maximum mean discrepancy (mmd) non-parametric framework. this approach extends kcusum's pertinence to scenarios where only reference samples are available, such as atomic trajectories of proteins in vacuum, facilitating the detection of deviations from the reference sample without prior knowledge of the data's underlying distribution. furthermore, by harnessing mmd's inherent random-walk structure, we can theoretically analyze kcusum's performance across various use cases, including metrics like expected delay and mean runtime to false alarms. finally, we discuss real-world use cases from scientific simulations such as nwchem codar and protein folding data, demonstrating kcusum's practical effectiveness in online change point detection.",
        "doi": "",
        "created": "2024-02-15",
        "url": "https://arxiv.org/abs/2402.10291",
        "authors": [
            "vijayalakshmi saravanan",
            "perry siehien",
            "shinjae yoo",
            "hubertus van dam",
            "thomas flynn",
            "christopher kelly",
            "khaled z ibrahim"
        ]
    },
    {
        "id": "2402.18810",
        "title": "the numeraire e-variable and reverse information projection",
        "abstract": "we consider testing a composite null hypothesis $\\mathcal{p}$ against a point alternative $\\mathsf{q}$ using e-variables, which are nonnegative random variables $x$ such that $\\mathbb{e}_\\mathsf{p}[x] \\leq 1$ for every $\\mathsf{p} \\in \\mathcal{p}$. this paper establishes a fundamental result: under no conditions whatsoever on $\\mathcal{p}$ or $\\mathsf{q}$, there exists a special e-variable $x^*$ that we call the numeraire, which is strictly positive and satisfies $\\mathbb{e}_\\mathsf{q}[x/x^*] \\leq 1$ for every other e-variable $x$. in particular, $x^*$ is log-optimal in the sense that $\\mathbb{e}_\\mathsf{q}[\\log(x/x^*)] \\leq 0$. moreover, $x^*$ identifies a particular sub-probability measure $\\mathsf{p}^*$ via the density $d \\mathsf{p}^*/d \\mathsf{q} = 1/x^*$. as a result, $x^*$ can be seen as a generalized likelihood ratio of $\\mathsf{q}$ against $\\mathcal{p}$. we show that $\\mathsf{p}^*$ coincides with the reverse information projection (ripr) when additional assumptions are made that are required for the latter to exist. thus $\\mathsf{p}^*$ is a natural definition of the ripr in the absence of any assumptions on $\\mathcal{p}$ or $\\mathsf{q}$. in addition to the abstract theory, we provide several tools for finding the numeraire and ripr in concrete cases. we discuss several nonparametric examples where we can indeed identify the numeraire and ripr, despite not having a reference measure. our results have interpretations outside of testing in that they yield the optimal kelly bet against $\\mathcal{p}$ if we believe reality follows $\\mathsf{q}$. we end with a more general optimality theory that goes beyond the ubiquitous logarithmic utility. we focus on certain power utilities, leading to reverse r\\'enyi projections in place of the ripr, which also always exist.",
        "doi": "",
        "created": "2024-02-28",
        "url": "https://arxiv.org/abs/2402.18810",
        "authors": [
            "martin larsson",
            "aaditya ramdas",
            "johannes ruf"
        ]
    },
    {
        "id": "2403.07236",
        "title": "partial identification of individual-level parameters using aggregate   data in a nonparametric binary outcome model",
        "abstract": "it is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals. this problem of aggregation becomes relevant when the researcher wants to learn individual-level relationships but only has access to data that has been aggregated. in this paper, i develop a methodology to partially identify linear combinations of conditional average outcomes from aggregate data when the outcome of interest is binary while imposing very few restrictions on the underlying data generating process. i construct identified sets using an optimization program that allows for researchers to impose additional shape and data restrictions. i also provide consistency results and construct an inference procedure that is valid with aggregate data, which only provides marginal information about each variable. i apply the methodology to simulated and real-world data sets and find that the estimated identified sets are too wide to be useful, but become narrower as more assumptions are imposed and data aggregated at a finer level is available. this suggests that to obtain useful information from aggregate data sets about individual-level relationships, researchers must impose further assumptions that are carefully justified or seek out data aggregated at the finest level possible.",
        "doi": "",
        "created": "2024-03-11",
        "url": "https://arxiv.org/abs/2403.07236",
        "authors": [
            "sarah moon"
        ]
    },
    {
        "id": "2403.11017",
        "title": "continuous-time mediation analysis for repeatedly measured mediators and   outcomes",
        "abstract": "mediation analysis aims to decipher the underlying causal mechanisms between an exposure, an outcome, and intermediate variables called mediators. initially developed for fixed-time mediator and outcome, it has been extended to the framework of longitudinal data by discretizing the assessment times of mediator and outcome. yet, processes in play in longitudinal studies are usually defined in continuous time and measured at irregular and subject-specific visits. this is the case in dementia research when cerebral and cognitive changes measured at planned visits in cohorts are of interest. we thus propose a methodology to estimate the causal mechanisms between a time-fixed exposure ($x$), a mediator process ($\\mathcal{m}_t$) and an outcome process ($\\mathcal{y}_t$) both measured repeatedly over time in the presence of a time-dependent confounding process ($\\mathcal{l}_t$). we consider three types of causal estimands, the natural effects, path-specific effects and randomized interventional analogues to natural effects, and provide identifiability assumptions. we employ a dynamic multivariate model based on differential equations for their estimation. the performance of the methods are explored in simulations, and we illustrate the method in two real-world examples motivated by the 3c cerebral aging study to assess: (1) the effect of educational level on functional dependency through depressive symptomatology and cognitive functioning, and (2) the effect of a genetic factor on cognitive functioning potentially mediated by vascular brain lesions and confounded by neurodegeneration.",
        "doi": "",
        "created": "2024-03-16",
        "url": "https://arxiv.org/abs/2403.11017",
        "authors": [
            "k. le bourdonnec",
            "l. valeri",
            "c. proust-lima"
        ]
    },
    {
        "id": "2403.15263",
        "title": "federated bayesian deep learning: the application of statistical   aggregation methods to bayesian models",
        "abstract": "federated learning (fl) is an approach to training machine learning models that takes advantage of multiple distributed datasets while maintaining data privacy and reducing communication costs associated with sharing local datasets. aggregation strategies have been developed to pool or fuse the weights and biases of distributed deterministic models; however, modern deterministic deep learning (dl) models are often poorly calibrated and lack the ability to communicate a measure of epistemic uncertainty in prediction, which is desirable for remote sensing platforms and safety-critical applications. conversely, bayesian dl models are often well calibrated and capable of quantifying and communicating a measure of epistemic uncertainty along with a competitive prediction accuracy. unfortunately, because the weights and biases in bayesian dl models are defined by a probability distribution, simple application of the aggregation methods associated with fl schemes for deterministic models is either impossible or results in sub-optimal performance. in this work, we use independent and identically distributed (iid) and non-iid partitions of the cifar-10 dataset and a fully variational resnet-20 architecture to analyze six different aggregation strategies for bayesian dl models. additionally, we analyze the traditional federated averaging approach applied to an approximate bayesian monte carlo dropout model as a lightweight alternative to more complex variational inference methods in fl. we show that aggregation strategy is a key hyperparameter in the design of a bayesian fl system with downstream effects on accuracy, calibration, uncertainty quantification, training stability, and client compute requirements.",
        "doi": "",
        "created": "2024-03-22",
        "url": "https://arxiv.org/abs/2403.15263",
        "authors": [
            "john fischer",
            "marko orescanin",
            "justin loomis",
            "patrick mcclure"
        ]
    },
    {
        "id": "2403.18578",
        "title": "steingen: generating fidelitous and diverse graph samples",
        "abstract": "generating graphs that preserve characteristic structures while promoting sample diversity can be challenging, especially when the number of graph observations is small. here, we tackle the problem of graph generation from only one observed graph. the classical approach of graph generation from parametric models relies on the estimation of parameters, which can be inconsistent or expensive to compute due to intractable normalisation constants. generative modelling based on machine learning techniques to generate high-quality graph samples avoids parameter estimation but usually requires abundant training samples. our proposed generating procedure, steingen, which is phrased in the setting of graphs as realisations of exponential random graph models, combines ideas from stein's method and mcmc by employing markovian dynamics which are based on a stein operator for the target model. steingen uses the glauber dynamics associated with an estimated stein operator to generate a sample, and re-estimates the stein operator from the sample after every sampling step. we show that on a class of exponential random graph models this novel \"estimation and re-estimation\" generation strategy yields high distributional similarity (high fidelity) to the original data, combined with high sample diversity.",
        "doi": "",
        "created": "2024-03-27",
        "url": "https://arxiv.org/abs/2403.18578",
        "authors": [
            "gesine reinert",
            "wenkai xu"
        ]
    },
    {
        "id": "2404.02283",
        "title": "integrating representative and non-representative survey data for   efficient inference",
        "abstract": "non-representative surveys are commonly used and widely available but suffer from selection bias that generally cannot be entirely eliminated using weighting techniques. instead, we propose a bayesian method to synthesize longitudinal representative unbiased surveys with non-representative biased surveys by estimating the degree of selection bias over time. we show using a simulation study that synthesizing biased and unbiased surveys together out-performs using the unbiased surveys alone, even if the selection bias may evolve in a complex manner over time. using covid-19 vaccination data, we are able to synthesize two large sample biased surveys with an unbiased survey to reduce uncertainty in now-casting and inference estimates while simultaneously retaining the empirical credible interval coverage. ultimately, we are able to conceptually obtain the properties of a large sample unbiased survey if the assumed unbiased survey, used to anchor the estimates, is unbiased for all time-points.",
        "doi": "",
        "created": "2024-04-02",
        "url": "https://arxiv.org/abs/2404.02283",
        "authors": [
            "nathaniel dyrkton",
            "paul gustafson",
            "harlan campbell"
        ]
    },
    {
        "id": "2404.03678",
        "title": "machine learning augmented diagnostic testing to identify sources of   variability in test performance",
        "abstract": "diagnostic tests which can detect pre-clinical or sub-clinical infection, are one of the most powerful tools in our armoury of weapons to control infectious diseases. considerable effort has been therefore paid to improving diagnostic testing for human, plant and animal diseases, including strategies for targeting the use of diagnostic tests towards individuals who are more likely to be infected. here, we follow other recent proposals to further refine this concept, by using machine learning to assess the situational risk under which a diagnostic test is applied to augment its interpretation . we develop this to predict the occurrence of breakdowns of cattle herds due to bovine tuberculosis, exploiting the availability of exceptionally detailed testing records. we show that, without compromising test specificity, test sensitivity can be improved so that the proportion of infected herds detected by the skin test, improves by over 16 percentage points. while many risk factors are associated with increased risk of becoming infected, of note are several factors which suggest that, in some herds there is a higher risk of infection going undetected, including effects that are correlated to the veterinary practice conducting the test, and number of livestock moved off the herd.",
        "doi": "",
        "created": "2024-03-28",
        "url": "https://arxiv.org/abs/2404.03678",
        "authors": [
            "christopher j. banks",
            "aeron sanchez",
            "vicki stewart",
            "kate bowen",
            "graham smith",
            "rowland r. kao"
        ]
    },
    {
        "id": "2404.03701",
        "title": "predictive analytics of varieties of potatoes",
        "abstract": "we explore the application of machine learning algorithms to predict the suitability of russet potato clones for advancement in breeding trials. leveraging data from manually collected trials in the state of oregon, we investigate the potential of a wide variety of state-of-the-art binary classification models. we conduct a comprehensive analysis of the dataset that includes preprocessing, feature engineering, and imputation to address missing values. we focus on several key metrics such as accuracy, f1-score, and matthews correlation coefficient (mcc) for model evaluation. the top-performing models, namely the multi-layer perceptron (mlpc), histogram-based gradient boosting classifier (hgbc), and a support vector machine (svc), demonstrate consistent and significant results. variable selection further enhances model performance and identifies influential features in predicting trial outcomes. the findings emphasize the potential of machine learning in streamlining the selection process for potato varieties, offering benefits such as increased efficiency, substantial cost savings, and judicious resource utilization. our study contributes insights into precision agriculture and showcases the relevance of advanced technologies for informed decision-making in breeding programs.",
        "doi": "",
        "created": "2024-04-03",
        "url": "https://arxiv.org/abs/2404.03701",
        "authors": [
            "fabiana ferracina",
            "bala krishnamoorthy",
            "mahantesh halappanavar",
            "shengwei hu",
            "vidyasagar sathuvalli"
        ]
    },
    {
        "id": "2404.03764",
        "title": "concert: covariate-elaborated robust local information transfer with   conditional spike-and-slab prior",
        "abstract": "the popularity of transfer learning stems from the fact that it can borrow information from useful auxiliary datasets. existing statistical transfer learning methods usually adopt a global similarity measure between the source data and the target data, which may lead to inefficiency when only local information is shared. in this paper, we propose a novel bayesian transfer learning method named \"concert\" to allow robust local information transfer for high-dimensional data analysis. a novel conditional spike-and-slab prior is introduced in the joint distribution of target and source parameters for information transfer. by incorporating covariate-specific priors, we can characterize the local similarities and make the sources work collaboratively to help improve the performance on the target. distinguished from existing work, concert is a one-step procedure, which achieves variable selection and information transfer simultaneously. variable selection consistency is established for our concert. to make our algorithm scalable, we adopt the variational bayes framework to facilitate implementation. extensive experiments and a genetic data analysis demonstrate the validity and the advantage of concert over existing cutting-edge transfer learning methods. we also extend our concert to the logistical models with numerical studies showing its superiority over other methods.",
        "doi": "",
        "created": "2024-03-30",
        "url": "https://arxiv.org/abs/2404.03764",
        "authors": [
            "ruqian zhang",
            "yijiao zhang",
            "annie qu",
            "zhongyi zhu",
            "juan shen"
        ]
    },
    {
        "id": "2404.03781",
        "title": "signal cancellation factor analysis",
        "abstract": "signal cancellation provides a radically new and efficient approach to exploratory factor analysis, without matrix decomposition nor presetting the required number of factors. its current implementation requires that each factor has at least two unique indicators. its principle is that it is always possible to combine two indicator variables exclusive to the same factor with weights that cancel their common factor information. successful combinations, consisting of nose only, are recognized by their null correlations with all remaining variables. the optimal combinations of multifactorial indicators, though, typically retain correlations with some other variables. their signal, however, can be cancelled through combinations with unifactorial indicators of their contributing factors. the loadings are estimated from the relative signal cancellation weights of the variables involved along with their observed correlations. the factor correlations are obtained from those of their unifactorial indicators, corrected by their factor loadings. the method is illustrated with synthetic data from a complex six-factor structure that even includes two doublet factors. another example using actual data documents that signal cancellation can rival confirmatory factor analysis.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03781",
        "authors": [
            "andr\u00e9 achim"
        ]
    },
    {
        "id": "2404.03786",
        "title": "non-parametric estimation of multiple periodic components in turkey's   electricity consumption",
        "abstract": "electric generation and consumption are an essential component of contemporary living, influencing diverse facets of our daily routines, convenience, and economic progress. there is a high demand for characterizing the periodic pattern of electricity consumption. vbpbb employs a bandpass filter aligned to retain the frequency of a pc component and eliminating interference from other components. this leads to a significant reduction in the size of bootstrapped confidence intervals. furthermore, other pc bootstrap methods preserve one but not multiple periodically correlated components, resulting in superior performance compared to other methods by providing a more precise estimation of the sampling distribution for the desired characteristics. the study of the periodic means of turkey electricity consumption using vbpbb is presented and compared with outcomes from alternative bootstrapping approaches. these findings offer significant evidence supporting the existence of daily, weekly, and annual pc patterns, along with information on their timing and confidence intervals for their effects. this information is valuable for enhancing predictions and preparations for future responses to electricity consumption.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03786",
        "authors": [
            "jie yao",
            "edward valachovic"
        ]
    },
    {
        "id": "2404.03804",
        "title": "transformerlsr: attentive joint model of longitudinal data, survival,   and recurrent events with concurrent latent structure",
        "abstract": "in applications such as biomedical studies, epidemiology, and social sciences, recurrent events often co-occur with longitudinal measurements and a terminal event, such as death. therefore, jointly modeling longitudinal measurements, recurrent events, and survival data while accounting for their dependencies is critical. while joint models for the three components exist in statistical literature, many of these approaches are limited by heavy parametric assumptions and scalability issues. recently, incorporating deep learning techniques into joint modeling has shown promising results. however, current methods only address joint modeling of longitudinal measurements at regularly-spaced observation times and survival events, neglecting recurrent events. in this paper, we develop transformerlsr, a flexible transformer-based deep modeling and inference framework to jointly model all three components simultaneously. transformerlsr integrates deep temporal point processes into the joint modeling framework, treating recurrent and terminal events as two competing processes dependent on past longitudinal measurements and recurrent event times. additionally, transformerlsr introduces a novel trajectory representation and model architecture to potentially incorporate a priori knowledge of known latent structures among concurrent longitudinal variables. we demonstrate the effectiveness and necessity of transformerlsr through simulation studies and analyzing a real-world medical dataset on patients after kidney transplantation.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03804",
        "authors": [
            "zhiyue zhang",
            "yao zhao",
            "yanxun xu"
        ]
    },
    {
        "id": "2404.03805",
        "title": "blessing of dimension in bayesian inference on covariance matrices",
        "abstract": "bayesian factor analysis is routinely used for dimensionality reduction in modeling of high-dimensional covariance matrices. factor analytic decompositions express the covariance as a sum of a low rank and diagonal matrix. in practice, gibbs sampling algorithms are typically used for posterior computation, alternating between updating the latent factors, loadings, and residual variances. in this article, we exploit a blessing of dimensionality to develop a provably accurate pseudo-posterior for the covariance matrix that bypasses the need for gibbs or other variants of markov chain monte carlo sampling. our proposed factor analysis with blessing of dimensionality (fable) approach relies on a first-stage singular value decomposition (svd) to estimate the latent factors, and then defines a jointly conjugate prior for the loadings and residual variances. the accuracy of the resulting pseudo-posterior for the covariance improves with increasing dimensionality. we show that fable has excellent performance in high-dimensional covariance matrix estimation, including producing well calibrated credible intervals, both theoretically and through simulation experiments. we also demonstrate the strength of our approach in terms of accurate inference and computational efficiency by applying it to a gene expression data set.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03805",
        "authors": [
            "shounak chattopadhyay",
            "anru r. zhang",
            "david b. dunson"
        ]
    },
    {
        "id": "2404.03808",
        "title": "the emergence of the normal distribution in deterministic chaotic maps",
        "abstract": "the central limit theorem states that, in the limit of a large number of terms, an appropriately scaled sum of independent random variables yields another random variable whose probability distribution tends to a stable distribution. the condition of independence, however, only holds in real systems as an approximation. to extend the theorem to more general situations, previous studies have derived a version of the central limit theorem that also holds for variables that are not independent. here, we present numerical results that characterize how convergence is attained when the variables being summed are deterministically related to one another by the recurrent application of an ergodic mapping. in all the explored cases, the convergence to the limit distribution is slower than for random sampling. yet, the speed at which convergence is attained varies substantially from system to system, and these variations imply differences in the way information about the deterministic nature of the dynamics is progressively lost as the number of summands increases. some of the identified factors in shaping the convergence process are the strength of mixing induced by the mapping and the shape of the marginal distribution of each variable, most particularly, the presence of divergences or fat tails.",
        "doi": "10.3390/e26010051",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03808",
        "authors": [
            "dami\u00e1n h. zanette",
            "in\u00e9s samengo"
        ]
    },
    {
        "id": "2404.03827",
        "title": "uniform memory retrieval with larger capacity for modern hopfield models",
        "abstract": "we propose a two-stage memory retrieval dynamics for modern hopfield models, termed $\\mathtt{u\\text{-}hop}$, with enhanced memory capacity. our key contribution is a learnable feature map $\\phi$ which transforms the hopfield energy function into a kernel space. this transformation ensures convergence between the local minima of energy and the fixed points of retrieval dynamics within the kernel space. consequently, the kernel norm induced by $\\phi$ serves as a novel similarity measure. it utilizes the stored memory patterns as learning data to enhance memory capacity across all modern hopfield models. specifically, we accomplish this by constructing a separation loss $\\mathcal{l}_\\phi$ that separates the local minima of kernelized energy by separating stored memory patterns in kernel space. methodologically, $\\mathtt{u\\text{-}hop}$ memory retrieval process consists of: \\textbf{(stage~i.)} minimizing separation loss for a more uniformed memory (local minimum) distribution, followed by \\textbf{(stage~ii.)} standard hopfield energy minimization for memory retrieval. this results in a significant reduction of possible meta-stable states in the hopfield energy function, thus enhancing memory capacity by preventing memory confusion. empirically, with real-world datasets, we demonstrate that $\\mathtt{u\\text{-}hop}$ outperforms all existing modern hopfield models and sota similarity measures, achieving substantial improvements in both associative memory retrieval and deep learning tasks.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03827",
        "authors": [
            "dennis wu",
            "jerry yao-chieh hu",
            "teng-yun hsiao",
            "han liu"
        ]
    },
    {
        "id": "2404.03828",
        "title": "outlier-efficient hopfield layers for large transformer-based models",
        "abstract": "we introduce an outlier-efficient modern hopfield model (termed $\\mathtt{outeffhop}$) and use it to address the outlier-induced challenge of quantizing gigantic transformer-based models. our main contribution is a novel associative memory model facilitating \\textit{outlier-efficient} associative memory retrievals. interestingly, this memory model manifests a model-based interpretation of an outlier-efficient attention mechanism ($\\text{softmax}_1$): it is an approximation of the memory retrieval process of $\\mathtt{outeffhop}$. methodologically, this allows us to debut novel outlier-efficient hopfield layers a powerful attention alternative with superior post-quantization performance. theoretically, the outlier-efficient modern hopfield model retains and improves the desirable properties of the standard modern hopfield models, including fixed point convergence and exponential storage capacity. empirically, we demonstrate the proposed model's efficacy across large-scale transformer-based and hopfield-based models (including bert, opt, vit and stanhop-net), benchmarking against state-of-the-art methods including $\\mathtt{clipped\\_softmax}$ and $\\mathtt{gated\\_attention}$. notably, $\\mathtt{outeffhop}$ achieves on average $\\sim$22+\\% reductions in both average kurtosis and maximum infinity norm of model outputs accross 4 models.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03828",
        "authors": [
            "jerry yao-chieh hu",
            "pei-hsuan chang",
            "robin luo",
            "hong-yu chen",
            "weijian li",
            "wei-po wang",
            "han liu"
        ]
    },
    {
        "id": "2404.03830",
        "title": "bishop: bi-directional cellular learning for tabular data with   generalized sparse modern hopfield model",
        "abstract": "we introduce the \\textbf{b}i-directional \\textbf{s}parse \\textbf{hop}field network (\\textbf{bishop}), a novel end-to-end framework for deep tabular learning. bishop handles the two major challenges of deep tabular learning: non-rotationally invariant data structure and feature sparsity in tabular data. our key motivation comes from the recent established connection between associative memory and attention mechanisms. consequently, bishop uses a dual-component approach, sequentially processing data both column-wise and row-wise through two interconnected directional learning modules. computationally, these modules house layers of generalized sparse modern hopfield layers, a sparse extension of the modern hopfield model with adaptable sparsity. methodologically, bishop facilitates multi-scale representation learning, capturing both intra-feature and inter-feature interactions, with adaptive sparsity at each scale. empirically, through experiments on diverse real-world datasets, we demonstrate that bishop surpasses current sota methods with significantly less hpo runs, marking it a robust solution for deep tabular learning.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03830",
        "authors": [
            "chenwei xu",
            "yu-chao huang",
            "jerry yao-chieh hu",
            "weijian li",
            "ammar gilani",
            "hsi-sheng goan",
            "han liu"
        ]
    },
    {
        "id": "2404.03835",
        "title": "quantile-respectful density estimation based on the harrell-davis   quantile estimator",
        "abstract": "traditional density and quantile estimators are often inconsistent with each other. their simultaneous usage may lead to inconsistent results. to address this issue, we propose a novel smooth density estimator that is naturally consistent with the harrell-davis quantile estimator. we also provide a jittering implementation to support discrete-continuous mixture distributions.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03835",
        "authors": [
            "andrey akinshin"
        ]
    },
    {
        "id": "2404.03837",
        "title": "inference for non-stationary time series quantile regression with   inequality constraints",
        "abstract": "we consider parameter inference for linear quantile regression with non-stationary predictors and errors, where the regression parameters are subject to inequality constraints. we show that the constrained quantile coefficient estimators are asymptotically equivalent to the metric projections of the unconstrained estimator onto the constrained parameter space. utilizing a geometry-invariant property of this projection operation, we propose inference procedures - the wald, likelihood ratio, and rank-based methods - that are consistent regardless of whether the true parameters lie on the boundary of the constrained parameter space. we also illustrate the advantages of considering the inequality constraints in analyses through simulations and an application to an electricity demand dataset.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03837",
        "authors": [
            "yuan sun",
            "zhou zhou"
        ]
    },
    {
        "id": "2404.03842",
        "title": "the low-degree hardness of finding large independent sets in sparse   random hypergraphs",
        "abstract": "we study the algorithmic task of finding large independent sets in erdos-renyi $r$-uniform hypergraphs on $n$ vertices having average degree $d$. krivelevich and sudakov showed that the maximum independent set has density $\\left(\\frac{r\\log d}{(r-1)d}\\right)^{1/(r-1)}$. we show that the class of low-degree polynomial algorithms can find independent sets of density $\\left(\\frac{\\log d}{(r-1)d}\\right)^{1/(r-1)}$ but no larger. this extends and generalizes earlier results of gamarnik and sudan, rahman and virag, and wein on graphs, and answers a question of bal and bennett. we conjecture that this statistical-computational gap holds for this problem.   additionally, we explore the universality of this gap by examining $r$-partite hypergraphs. a hypergraph $h=(v,e)$ is $r$-partite if there is a partition $v=v_1\\cup\\cdots\\cup v_r$ such that each edge contains exactly one vertex from each set $v_i$. we consider the problem of finding large balanced independent sets (independent sets containing the same number of vertices in each partition) in random $r$-partite hypergraphs with $n$ vertices in each partition and average degree $d$. we prove that the maximum balanced independent set has density $\\left(\\frac{r\\log d}{(r-1)d}\\right)^{1/(r-1)}$ asymptotically. furthermore, we prove an analogous low-degree computational threshold of $\\left(\\frac{\\log d}{(r-1)d}\\right)^{1/(r-1)}$. our results recover and generalize recent work of perkins and the second author on bipartite graphs.   while the graph case has been extensively studied, this work is the first to consider statistical-computational gaps of optimization problems on random hypergraphs. our results suggest that these gaps persist for larger uniformities as well as across many models. a somewhat surprising aspect of the gap for balanced independent sets is that the algorithm achieving the lower bound is a simple degree-1 polynomial.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03842",
        "authors": [
            "abhishek dhawan",
            "yuzhou wang"
        ]
    },
    {
        "id": "2404.03867",
        "title": "dimension-free relaxation times of informed mcmc samplers on discrete   spaces",
        "abstract": "convergence analysis of markov chain monte carlo methods in high-dimensional statistical applications is increasingly recognized. in this paper, we develop general mixing time bounds for metropolis-hastings algorithms on discrete spaces by building upon and refining some recent theoretical advancements in bayesian model selection problems. we establish sufficient conditions for a class of informed metropolis-hastings algorithms to attain relaxation times that are independent of the problem dimension. these conditions are grounded in high-dimensional statistical theory and allow for possibly multimodal posterior distributions. we obtain our results through two independent techniques: the multicommodity flow method and single-element drift condition analysis; we find that the latter yields a tighter mixing time bound. our results and proof techniques are readily applicable to a broad spectrum of statistical problems with discrete parameter spaces.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03867",
        "authors": [
            "hyunwoong chang",
            "quan zhou"
        ]
    },
    {
        "id": "2404.03871",
        "title": "latent space-based likelihood estimation using single observation for   bayesian updating of nonlinear hysteretic model",
        "abstract": "this study presents a novel approach to quantifying uncertainties in bayesian model updating, which is effective in sparse or single observations. conventional uncertainty quantification metrics such as the euclidean and bhattacharyya distance-based metrics are potential in scenarios with ample observations. however, their validation is limited in situations with insufficient data, particularly for nonlinear responses like post-yield behavior. our method addresses this challenge by using the latent space of a variational auto-encoder (vae), a generative model that enables nonparametric likelihood evaluation. this approach is valuable in updating model parameters based on nonlinear seismic responses of structure, wherein data scarcity is a common challenge. our numerical experiments confirm the ability of the proposed method to accurately update parameters and quantify uncertainties using limited observations. additionally, these numerical experiments reveal a tendency for increased information about nonlinear behavior to result in decreased uncertainty in terms of estimations. this study provides a robust tool for quantifying uncertainty in scenarios characterized by considerable uncertainty, thereby expanding the applicability of bayesian updating methods in data-constrained environments.",
        "doi": "",
        "created": "2024-04-04",
        "url": "https://arxiv.org/abs/2404.03871",
        "authors": [
            "sangwon lee",
            "taro yaoyama",
            "yuma matsumoto",
            "takenori hida",
            "tatsuya itoi"
        ]
    },
    {
        "id": "2404.03878",
        "title": "wasserstein f-tests for fr\\'echet regression on bures-wasserstein   manifolds",
        "abstract": "this paper considers the problem of regression analysis with random covariance matrix as outcome and euclidean covariates in the framework of fr\\'echet regression on the bures-wasserstein manifold. such regression problems have many applications in single cell genomics and neuroscience, where we have covariance matrix measured over a large set of samples. fr\\'echet regression on the bures-wasserstein manifold is formulated as estimating the conditional fr\\'echet mean given covariates $x$. a non-asymptotic $\\sqrt{n}$-rate of convergence (up to $\\log n$ factors) is obtained for our estimator $\\hat{q}_n(x)$ uniformly for $\\left\\|x\\right\\| \\lesssim \\sqrt{\\log n}$, which is crucial for deriving the asymptotic null distribution and power of our proposed statistical test for the null hypothesis of no association. in addition, a central limit theorem for the point estimate $\\hat{q}_n(x)$ is obtained, giving insights to a test for covariate effects. the null distribution of the test statistic is shown to converge to a weighted sum of independent chi-squares, which implies that the proposed test has the desired significance level asymptotically. also, the power performance of the test is demonstrated against a sequence of contiguous alternatives. simulation results show the accuracy of the asymptotic distributions. the proposed methods are applied to a single cell gene expression data set that shows the change of gene co-expression network as people age.",
        "doi": "",
        "created": "2024-04-05",
        "url": "https://arxiv.org/abs/2404.03878",
        "authors": [
            "haoshu xu",
            "hongzhe li"
        ]
    },
    {
        "id": "2404.03885",
        "title": "the esprit algorithm under high noise: optimal error scaling and noisy   super-resolution",
        "abstract": "subspace-based signal processing techniques, such as the estimation of signal parameters via rotational invariant techniques (esprit) algorithm, are popular methods for spectral estimation. these algorithms can achieve the so-called super-resolution scaling under low noise conditions, surpassing the well-known nyquist limit. however, the performance of these algorithms under high-noise conditions is not as well understood. existing state-of-the-art analysis indicates that esprit and related algorithms can be resilient even for signals where each observation is corrupted by statistically independent, mean-zero noise of size $\\mathcal{o}(1)$, but these analyses only show that the error $\\epsilon$ decays at a slow rate $\\epsilon=\\mathcal{\\tilde{o}}(n^{-1/2})$ with respect to the cutoff frequency $n$. in this work, we prove that under certain assumptions of bias and high noise, the esprit algorithm can attain a significantly improved error scaling $\\epsilon = \\mathcal{\\tilde{o}}(n^{-3/2})$, exhibiting noisy super-resolution scaling beyond the nyquist limit. we further establish a theoretical lower bound and show that this scaling is optimal. our analysis introduces novel matrix perturbation results, which could be of independent interest.",
        "doi": "",
        "created": "2024-04-05",
        "url": "https://arxiv.org/abs/2404.03885",
        "authors": [
            "zhiyan ding",
            "ethan n. epperly",
            "lin lin",
            "ruizhe zhang"
        ]
    },
    {
        "id": "2404.03900",
        "title": "nonparametric modern hopfield models",
        "abstract": "we present a nonparametric construction for deep learning compatible modern hopfield models and utilize this framework to debut an efficient variant. our key contribution stems from interpreting the memory storage and retrieval processes in modern hopfield models as a nonparametric regression problem subject to a set of query-memory pairs. crucially, our framework not only recovers the known results from the original dense modern hopfield model but also fills the void in the literature regarding efficient modern hopfield models, by introducing \\textit{sparse-structured} modern hopfield models with sub-quadratic complexity. we establish that this sparse model inherits the appealing theoretical properties of its dense analogue -- connection with transformer attention, fixed point convergence and exponential memory capacity -- even without knowing details of the hopfield energy function. additionally, we showcase the versatility of our framework by constructing a family of modern hopfield models as extensions, including linear, random masked, top-$k$ and positive random feature modern hopfield models. empirically, we validate the efficacy of our framework in both synthetic and realistic settings.",
        "doi": "",
        "created": "2024-04-05",
        "url": "https://arxiv.org/abs/2404.03900",
        "authors": [
            "jerry yao-chieh hu",
            "bo-yu chen",
            "dennis wu",
            "feng ruan",
            "han liu"
        ]
    },
    {
        "id": "2404.03916",
        "title": "estimating mixed memberships in multi-layer networks",
        "abstract": "community detection in multi-layer networks has emerged as a crucial area of modern network analysis. however, conventional approaches often assume that nodes belong exclusively to a single community, which fails to capture the complex structure of real-world networks where nodes may belong to multiple communities simultaneously. to address this limitation, we propose novel spectral methods to estimate the common mixed memberships in the multi-layer mixed membership stochastic block model. the proposed methods leverage the eigen-decomposition of three aggregate matrices: the sum of adjacency matrices, the debiased sum of squared adjacency matrices, and the sum of squared adjacency matrices. we establish rigorous theoretical guarantees for the consistency of our methods. specifically, we derive per-node error rates under mild conditions on network sparsity, demonstrating their consistency as the number of nodes and/or layers increases under the multi-layer mixed membership stochastic block model. our theoretical results reveal that the method leveraging the sum of adjacency matrices generally performs poorer than the other two methods for mixed membership estimation in multi-layer networks. we conduct extensive numerical experiments to empirically validate our theoretical findings. for real-world multi-layer networks with unknown community information, we introduce two novel modularity metrics to quantify the quality of mixed membership community detection. finally, we demonstrate the practical applications of our algorithms and modularity metrics by applying them to real-world multi-layer networks, demonstrating their effectiveness in extracting meaningful community structures.",
        "doi": "",
        "created": "2024-04-05",
        "url": "https://arxiv.org/abs/2404.03916",
        "authors": [
            "huan qing"
        ]
    },
    {
        "id": "2404.03957",
        "title": "bayesian graphs of intelligent causation",
        "abstract": "probabilistic graphical bayesian models of causation have continued to impact on strategic analyses designed to help evaluate the efficacy of different interventions on systems. however, the standard causal algebras upon which these inferences are based typically assume that the intervened population does not react intelligently to frustrate an intervention. in an adversarial setting this is rarely an appropriate assumption. in this paper, we extend an established bayesian methodology called adversarial risk analysis to apply it to settings that can legitimately be designated as causal in this graphical sense. to embed this technology we first need to generalize the concept of a causal graph. we then proceed to demonstrate how the predicable intelligent reactions of adversaries to circumvent an intervention when they hear about it can be systematically modelled within such graphical frameworks, importing these recent developments from bayesian game theory. the new methodologies and supporting protocols are illustrated through applications associated with an adversary attempting to infiltrate a friendly state.",
        "doi": "",
        "created": "2024-04-05",
        "url": "https://arxiv.org/abs/2404.03957",
        "authors": [
            "preetha ramiah",
            "james q. smith",
            "oliver bunnin",
            "silvia liverani",
            "jamie addison",
            "annabel whipp"
        ]
    },
    {
        "id": "2404.03968",
        "title": "regularization for electricity price forecasting",
        "abstract": "the most commonly used form of regularization typically involves defining the penalty function as a l1 or l2 norm. however, numerous alternative approaches remain untested in practical applications. in this study, we apply ten different penalty functions to predict electricity prices and evaluate their performance under two different model structures and in two distinct electricity markets. the study reveals that lq and elastic net consistently produce more accurate forecasts compared to other regularization types. in particular, they were the only types of penalty functions that consistently produced more accurate forecasts than the most commonly used lasso. furthermore, the results suggest that cross-validation outperforms bayesian information criteria for parameter optimization, and performs as well as models with ex-post parameter selection.",
        "doi": "",
        "created": "2024-04-05",
        "url": "https://arxiv.org/abs/2404.03968",
        "authors": [
            "bartosz uniejewski"
        ]
    },
    {
        "id": "2404.04024",
        "title": "colored gaussian dag models",
        "abstract": "we study submodels of gaussian dag models defined by partial homogeneity constraints imposed on the model error variances and structural coefficients. we represent these models with colored dags and investigate their properties for use in statistical and causal inference. local and global markov properties are provided and shown to characterize the colored dag model. additional properties relevant to causal discovery are studied, including the existence and non-existence of faithful distributions and structural identifiability. extending prior work of peters and b\\\"uhlman and wu and drton, we prove structural identifiability under the assumption of homogeneous structural coefficients, as well as for a family of models with partially homogenous structural coefficients. the latter models, termed bpec-dags, capture additional insights as they cluster the direct causes of each node into communities according to their effect on their common target. an analogue of the ges algorithm for learning bpec-dags is given and evaluated on real and synthetic data. regarding model geometry, we prove that these models are contractible, smooth, algebraic manifolds and compute their dimension. we also provide a proof of a conjecture of sullivant which generalizes to colored dag models, colored undirected graphical models and ancestral graph models.",
        "doi": "",
        "created": "2024-04-05",
        "url": "https://arxiv.org/abs/2404.04024",
        "authors": [
            "tobias boege",
            "kaie kubjas",
            "pratik misra",
            "liam solus"
        ]
    },
    {
        "id": "2404.04057",
        "title": "score identity distillation: exponentially fast distillation of   pretrained diffusion models for one-step generation",
        "abstract": "we introduce score identity distillation (sid), an innovative data-free method that distills the generative capabilities of pretrained diffusion models into a single-step generator. sid not only facilitates an exponentially fast reduction in fr\\'echet inception distance (fid) during distillation but also approaches or even exceeds the fid performance of the original teacher diffusion models. by reformulating forward diffusion processes as semi-implicit distributions, we leverage three score-related identities to create an innovative loss mechanism. this mechanism achieves rapid fid reduction by training the generator using its own synthesized images, eliminating the need for real data or reverse-diffusion-based generation, all accomplished within significantly shortened generation time. upon evaluation across four benchmark datasets, the sid algorithm demonstrates high iteration efficiency during distillation and surpasses competing distillation approaches, whether they are one-step or few-step, data-free, or dependent on training data, in terms of generation quality. this achievement not only redefines the benchmarks for efficiency and effectiveness in diffusion distillation but also in the broader field of diffusion-based generation. our pytorch implementation will be publicly accessible on github.",
        "doi": "",
        "created": "2024-04-05",
        "url": "https://arxiv.org/abs/2404.04057",
        "authors": [
            "mingyuan zhou",
            "huangjie zheng",
            "zhendong wang",
            "mingzhang yin",
            "hai huang"
        ]
    },
    {
        "id": "2404.04074",
        "title": "dgp-lvm: derivative gaussian process latent variable model",
        "abstract": "we develop a framework for derivative gaussian process latent variable models (dgp-lvm) that can handle multi-dimensional output data using modified derivative covariance functions. the modifications account for complexities in the underlying data generating process such as scaled derivatives, varying information across multiple output dimensions as well as interactions between outputs. further, our framework provides uncertainty estimates for each latent variable samples using bayesian inference. through extensive simulations, we demonstrate that latent variable estimation accuracy can be drastically increased by including derivative information due to our proposed covariance function modifications. the developments are motivated by a concrete biological research problem involving the estimation of the unobserved cellular ordering from single-cell rna (scrna) sequencing data for gene expression and its corresponding derivative information known as rna velocity. since the rna velocity is only an estimate of the exact derivative information, the derivative covariance functions need to account for potential scale differences. in a real-world case study, we illustrate the application of dgp-lvms to such scrna sequencing data. while motivated by this biological problem, our framework is generally applicable to all kinds of latent variable estimation problems involving derivative information irrespective of the field of study.",
        "doi": "",
        "created": "2024-04-05",
        "url": "https://arxiv.org/abs/2404.04074",
        "authors": [
            "soham mukherjee",
            "manfred claassen",
            "paul-christian b\u00fcrkner"
        ]
    },
    {
        "id": "2404.04122",
        "title": "hidden markov models for multivariate panel data",
        "abstract": "while advances continue to be made in model-based clustering, challenges persist in modeling various data types such as panel data. multivariate panel data present difficulties for clustering algorithms due to the unique correlation structure, a consequence of taking observations on several subjects over multiple time points. additionally, panel data are often plagued by missing data and dropouts, presenting issues for estimation algorithms. this research presents a family of hidden markov models that compensate for the unique correlation structures that arise in panel data. a modified expectation-maximization algorithm capable of handling missing not at random data and dropout is presented and used to perform model estimation.",
        "doi": "",
        "created": "2024-04-05",
        "url": "https://arxiv.org/abs/2404.04122",
        "authors": [
            "mackenzie r. neal",
            "alexa a. sochaniwsky",
            "paul d. mcnicholas"
        ]
    },
    {
        "id": "2404.04213",
        "title": "modelling handball outcomes using univariate and bivariate approaches",
        "abstract": "handball has received growing interest during the last years, including academic research for many different aspects of the sport. on the other hand modelling the outcome of the game has attracted less interest mainly because of the additional challenges that occur. data analysis has revealed that the number of goals scored by each team are under-dispersed relative to a poisson distribution and hence new models are needed for this purpose. here we propose to circumvent the problem by modelling the score difference. this removes the need for special models since typical models for integer data like the skellam distribution can provide sufficient fit and thus reveal some of the characteristics of the game. in the present paper we propose some models starting from a skellam regression model and also considering zero inflated versions as well as other discrete distributions in $\\mathbb z$. furthermore, we develop some bivariate models using copulas to model the two halves of the game and thus providing insights on the game. data from german bundesliga are used to show the potential of the new models.",
        "doi": "",
        "created": "2024-04-05",
        "url": "https://arxiv.org/abs/2404.04213",
        "authors": [
            "dimitris karlis",
            "rouven michels",
            "marius otting"
        ]
    }
]