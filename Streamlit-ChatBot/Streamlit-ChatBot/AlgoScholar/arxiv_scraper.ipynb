{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efb3ee9-ca6d-4311-8d5e-d45811d90678",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:08:39.751441Z",
     "start_time": "2024-04-17T23:08:38.651810Z"
    }
   },
   "outputs": [],
   "source": [
    "#script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4d5497-efed-406c-b194-687cc7e35d38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:08:40.052128Z",
     "start_time": "2024-04-17T23:08:39.752457Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import re\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from arxivscraper import Scraper\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff16681e-e23d-4367-98a0-d3d71e32573e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:08:40.057390Z",
     "start_time": "2024-04-17T23:08:40.053136Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_bibcode(arxiv_id, created_date, authors):\n",
    "    year = created_date[:4]\n",
    "    number = arxiv_id.replace('arXiv:', '').replace('/', '')\n",
    "    initial = authors[0].split()[-1][0].upper() if authors else ''\n",
    "    bibcode_without_dot = f\"{year}arXiv{number.replace('.', '')}{initial}\"\n",
    "    bibcode_with_dot = f\"{year}arXiv{number}{initial}\"\n",
    "    logging.info(f\"Generated bibcodes for arXiv ID {arxiv_id}: {bibcode_without_dot}, {bibcode_with_dot}\")\n",
    "    return bibcode_without_dot, bibcode_with_dot\n",
    "\n",
    "def fetch_references(bibcodes):\n",
    "    ads_api_token = \"f8XsbXMlFWBC2umi8tCdJP9VoUV0ojEGW5AxtxdE\"\n",
    "    headers = {\"Authorization\": f\"Bearer {ads_api_token}\"}\n",
    "    data = 'bibcode\\n' + '\\n'.join(bibcodes)\n",
    "    url = \"https://api.adsabs.harvard.edu/v1/search/bigquery\"\n",
    "    params = {'q': '*:*', 'fl': 'bibcode,reference', 'wt': 'json', 'fq': '{!bitset}'}\n",
    "\n",
    "    response = requests.post(url, headers=headers, params=params, data=data)\n",
    "    if response.status_code == 200:\n",
    "        return {doc['bibcode']: doc.get('reference', []) for doc in response.json().get('response', {}).get('docs', [])}\n",
    "    else:\n",
    "        logging.error(f\"Failed to fetch references with status code {response.status_code}\")\n",
    "        logging.error(f\"API response content: {response.text}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38ca79d-b2c5-462d-bfdb-133b36cee1dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:08:40.063439Z",
     "start_time": "2024-04-17T23:08:40.057390Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_ai(start_date, end_date, max_limit):\n",
    "    folder = \"ARXIV\"\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    categories = ['cs', 'stat', 'econ']\n",
    "    for category in categories:\n",
    "        scraper = Scraper(category=category, date_from=start_date, date_until=end_date, max_records=max_limit)\n",
    "        output = scraper.scrape()\n",
    "\n",
    "        cols = ('id', 'title', 'abstract', 'doi', 'created', 'url', 'authors')\n",
    "        df = pd.DataFrame(output, columns=cols)\n",
    "        json_data = df.to_json(orient='records')\n",
    "        formatted_json = json.loads(json_data)\n",
    "\n",
    "        with open(f'{folder}/arxiv_data_{category}_{start_date}_{end_date}.json', 'w') as file:\n",
    "            json.dump(formatted_json, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a20a977-7bb1-4c39-b779-b6d7409228fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:09:03.743903Z",
     "start_time": "2024-04-17T23:08:40.063439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching is completed in 4.3 seconds.\n",
      "Total number of records: 100\n",
      "Fetching is completed in 8.3 seconds.\n",
      "Total number of records: 69\n",
      "Fetching is completed in 8.0 seconds.\n",
      "Total number of records: 16\n"
     ]
    }
   ],
   "source": [
    "scrape_ai(start_date='2024-04-22', end_date='2024-04-22', max_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9b9fa9-51b1-41f6-8b76-5b6a64f14926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:09:03.752215Z",
     "start_time": "2024-04-17T23:09:03.746916Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_and_fetch_references(file_paths, output_file, references_file):\n",
    "    merged_data = []\n",
    "    arxiv_references = {}\n",
    "\n",
    "    for path in file_paths:\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            print(\"Data loaded:\", data)\n",
    "            print(\"Data type:\", type(data))\n",
    "            if not isinstance(data, list):\n",
    "                logging.error(f\"Invalid data format in file {path}: Expected a list, got {type(data)}\")\n",
    "                continue\n",
    "            for entry in data:\n",
    "                if not isinstance(entry, dict):\n",
    "                    logging.error(f\"Invalid entry format in file {path}: Expected a dictionary, got {type(entry)}\")\n",
    "                    continue\n",
    "                bibcode_without_dot, bibcode_with_dot = generate_bibcode(entry['id'], entry['created'], entry.get('authors', []))\n",
    "                merged_data.append(entry)\n",
    "                references = entry.get('references', [])\n",
    "                arxiv_refs = [ref.split(\":\")[-1] for ref in references if ref.startswith(\"arXiv:\")]\n",
    "                arxiv_references[entry['id']] = arxiv_refs\n",
    "                if arxiv_refs:\n",
    "                    logging.info(f\"References found for arXiv ID: {entry['id']}\")\n",
    "                else:\n",
    "                    logging.warning(f\"No references found for arXiv ID: {entry['id']}\")\n",
    "\n",
    "\n",
    "    all_bibcodes = []\n",
    "    for entry in merged_data:\n",
    "        bibcode_without_dot, bibcode_with_dot = generate_bibcode(entry['id'], entry['created'], entry.get('authors', []))\n",
    "        all_bibcodes.append(bibcode_without_dot)\n",
    "        all_bibcodes.append(bibcode_with_dot)\n",
    "\n",
    "    references = fetch_references(all_bibcodes)\n",
    "\n",
    "    for entry in merged_data:\n",
    "        bibcode_without_dot, bibcode_with_dot = generate_bibcode(entry['id'], entry['created'], entry.get('authors', []))\n",
    "        fetched_references = references.get(bibcode_without_dot, []) + references.get(bibcode_with_dot, [])\n",
    "        entry['references'] = fetched_references\n",
    "        arxiv_references[entry['id']] = fetched_references\n",
    "        if entry['references']:\n",
    "            logging.info(f\"References found for bibcodes: {bibcode_without_dot}, {bibcode_with_dot}\")\n",
    "        else:\n",
    "            logging.warning(f\"No references found for bibcodes: {bibcode_without_dot}, {bibcode_with_dot}\")\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(merged_data, outfile, indent=4)\n",
    "    with open(references_file, 'w') as outfile:\n",
    "        json.dump(arxiv_references, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b9d112f-4779-45a2-837e-d8fc286e55f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:09:03.757646Z",
     "start_time": "2024-04-17T23:09:03.753226Z"
    }
   },
   "outputs": [],
   "source": [
    "def fix_references_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        arxiv_references = json.load(file)\n",
    "\n",
    "    fixed_arxiv_references = {}\n",
    "    for arxiv_id, references in arxiv_references.items():\n",
    "        fixed_references = []\n",
    "        for ref in references:\n",
    "            if 'arXiv' in ref:\n",
    "                match = re.search(r'(?:arXiv)?(\\d{4,5}\\.\\d{4,5}|\\d{8,9})(?=[a-zA-Z]|$)', ref)\n",
    "                if match:\n",
    "                    fixed_arxiv_id = match.group(1)\n",
    "                    if '.' not in fixed_arxiv_id:\n",
    "                        fixed_arxiv_id = f\"{fixed_arxiv_id[:4]}.{fixed_arxiv_id[4:]}\"\n",
    "                    reference_url = f\"https://arxiv.org/abs/{fixed_arxiv_id}\"\n",
    "                    fixed_references.append([fixed_arxiv_id, reference_url])\n",
    "        fixed_arxiv_references[arxiv_id] = fixed_references\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(fixed_arxiv_references, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4002d5f9163331a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:09:03.760938Z",
     "start_time": "2024-04-17T23:09:03.758653Z"
    }
   },
   "outputs": [],
   "source": [
    "files = [os.path.join('ARXIV', file) for file in os.listdir('ARXIV') if file.endswith('.json')]\n",
    "output_file = 'ARXIV/merged.json'\n",
    "references_file = 'ARXIV/references.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c31b6ad364ae84e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T23:09:05.422368Z",
     "start_time": "2024-04-17T23:09:03.760938Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 15:46:28,569 - INFO - Generated bibcodes for arXiv ID 1711.08265: 2017arXiv171108265L, 2017arXiv1711.08265L\n",
      "2024-04-22 15:46:28,569 - WARNING - No references found for arXiv ID: 1711.08265\n",
      "2024-04-22 15:46:28,570 - INFO - Generated bibcodes for arXiv ID 1907.05325: 2019arXiv190705325M, 2019arXiv1907.05325M\n",
      "2024-04-22 15:46:28,571 - WARNING - No references found for arXiv ID: 1907.05325\n",
      "2024-04-22 15:46:28,571 - INFO - Generated bibcodes for arXiv ID 2101.01157: 2021arXiv210101157A, 2021arXiv2101.01157A\n",
      "2024-04-22 15:46:28,572 - WARNING - No references found for arXiv ID: 2101.01157\n",
      "2024-04-22 15:46:28,573 - INFO - Generated bibcodes for arXiv ID 2110.15517: 2021arXiv211015517H, 2021arXiv2110.15517H\n",
      "2024-04-22 15:46:28,574 - WARNING - No references found for arXiv ID: 2110.15517\n",
      "2024-04-22 15:46:28,575 - INFO - Generated bibcodes for arXiv ID 2111.04652: 2021arXiv211104652M, 2021arXiv2111.04652M\n",
      "2024-04-22 15:46:28,576 - WARNING - No references found for arXiv ID: 2111.04652\n",
      "2024-04-22 15:46:28,577 - INFO - Generated bibcodes for arXiv ID 2201.09648: 2022arXiv220109648P, 2022arXiv2201.09648P\n",
      "2024-04-22 15:46:28,578 - WARNING - No references found for arXiv ID: 2201.09648\n",
      "2024-04-22 15:46:28,579 - INFO - Generated bibcodes for arXiv ID 2210.02171: 2022arXiv221002171C, 2022arXiv2210.02171C\n",
      "2024-04-22 15:46:28,579 - WARNING - No references found for arXiv ID: 2210.02171\n",
      "2024-04-22 15:46:28,580 - INFO - Generated bibcodes for arXiv ID 2210.16655: 2022arXiv221016655J, 2022arXiv2210.16655J\n",
      "2024-04-22 15:46:28,581 - WARNING - No references found for arXiv ID: 2210.16655\n",
      "2024-04-22 15:46:28,582 - INFO - Generated bibcodes for arXiv ID 2212.04911: 2022arXiv221204911L, 2022arXiv2212.04911L\n",
      "2024-04-22 15:46:28,583 - WARNING - No references found for arXiv ID: 2212.04911\n",
      "2024-04-22 15:46:28,584 - INFO - Generated bibcodes for arXiv ID 2302.03558: 2023arXiv230203558G, 2023arXiv2302.03558G\n",
      "2024-04-22 15:46:28,585 - WARNING - No references found for arXiv ID: 2302.03558\n",
      "2024-04-22 15:46:28,585 - INFO - Generated bibcodes for arXiv ID 2303.06434: 2023arXiv230306434T, 2023arXiv2303.06434T\n",
      "2024-04-22 15:46:28,586 - WARNING - No references found for arXiv ID: 2303.06434\n",
      "2024-04-22 15:46:28,587 - INFO - Generated bibcodes for arXiv ID 2303.10322: 2023arXiv230310322S, 2023arXiv2303.10322S\n",
      "2024-04-22 15:46:28,588 - WARNING - No references found for arXiv ID: 2303.10322\n",
      "2024-04-22 15:46:28,589 - INFO - Generated bibcodes for arXiv ID 2304.02025: 2023arXiv230402025B, 2023arXiv2304.02025B\n",
      "2024-04-22 15:46:28,590 - WARNING - No references found for arXiv ID: 2304.02025\n",
      "2024-04-22 15:46:28,592 - INFO - Generated bibcodes for arXiv ID 2306.08553: 2023arXiv230608553J, 2023arXiv2306.08553J\n",
      "2024-04-22 15:46:28,592 - WARNING - No references found for arXiv ID: 2306.08553\n",
      "2024-04-22 15:46:28,593 - INFO - Generated bibcodes for arXiv ID 2309.00380: 2023arXiv230900380H, 2023arXiv2309.00380H\n",
      "2024-04-22 15:46:28,594 - WARNING - No references found for arXiv ID: 2309.00380\n",
      "2024-04-22 15:46:28,594 - INFO - Generated bibcodes for arXiv ID 2310.05898: 2023arXiv231005898C, 2023arXiv2310.05898C\n",
      "2024-04-22 15:46:28,595 - WARNING - No references found for arXiv ID: 2310.05898\n",
      "2024-04-22 15:46:28,596 - INFO - Generated bibcodes for arXiv ID 2310.07983: 2023arXiv231007983G, 2023arXiv2310.07983G\n",
      "2024-04-22 15:46:28,596 - WARNING - No references found for arXiv ID: 2310.07983\n",
      "2024-04-22 15:46:28,597 - INFO - Generated bibcodes for arXiv ID 2310.12079: 2023arXiv231012079L, 2023arXiv2310.12079L\n",
      "2024-04-22 15:46:28,598 - WARNING - No references found for arXiv ID: 2310.12079\n",
      "2024-04-22 15:46:28,598 - INFO - Generated bibcodes for arXiv ID 2310.18784: 2023arXiv231018784A, 2023arXiv2310.18784A\n",
      "2024-04-22 15:46:28,599 - WARNING - No references found for arXiv ID: 2310.18784\n",
      "2024-04-22 15:46:28,600 - INFO - Generated bibcodes for arXiv ID 2310.19454: 2023arXiv231019454K, 2023arXiv2310.19454K\n",
      "2024-04-22 15:46:28,600 - WARNING - No references found for arXiv ID: 2310.19454\n",
      "2024-04-22 15:46:28,601 - INFO - Generated bibcodes for arXiv ID 2311.00944: 2023arXiv231100944S, 2023arXiv2311.00944S\n",
      "2024-04-22 15:46:28,602 - WARNING - No references found for arXiv ID: 2311.00944\n",
      "2024-04-22 15:46:28,603 - INFO - Generated bibcodes for arXiv ID 2311.05883: 2023arXiv231105883C, 2023arXiv2311.05883C\n",
      "2024-04-22 15:46:28,604 - WARNING - No references found for arXiv ID: 2311.05883\n",
      "2024-04-22 15:46:28,605 - INFO - Generated bibcodes for arXiv ID 2311.14653: 2023arXiv231114653H, 2023arXiv2311.14653H\n",
      "2024-04-22 15:46:28,605 - WARNING - No references found for arXiv ID: 2311.14653\n",
      "2024-04-22 15:46:28,606 - INFO - Generated bibcodes for arXiv ID 2312.00417: 2023arXiv231200417D, 2023arXiv2312.00417D\n",
      "2024-04-22 15:46:28,607 - WARNING - No references found for arXiv ID: 2312.00417\n",
      "2024-04-22 15:46:28,607 - INFO - Generated bibcodes for arXiv ID 2402.10043: 2024arXiv240210043P, 2024arXiv2402.10043P\n",
      "2024-04-22 15:46:28,608 - WARNING - No references found for arXiv ID: 2402.10043\n",
      "2024-04-22 15:46:28,609 - INFO - Generated bibcodes for arXiv ID 2402.11394: 2024arXiv240211394P, 2024arXiv2402.11394P\n",
      "2024-04-22 15:46:28,610 - WARNING - No references found for arXiv ID: 2402.11394\n",
      "2024-04-22 15:46:28,610 - INFO - Generated bibcodes for arXiv ID 2402.15984: 2024arXiv240215984S, 2024arXiv2402.15984S\n",
      "2024-04-22 15:46:28,611 - WARNING - No references found for arXiv ID: 2402.15984\n",
      "2024-04-22 15:46:28,617 - INFO - Generated bibcodes for arXiv ID 2403.12459: 2024arXiv240312459W, 2024arXiv2403.12459W\n",
      "2024-04-22 15:46:28,619 - WARNING - No references found for arXiv ID: 2403.12459\n",
      "2024-04-22 15:46:28,619 - INFO - Generated bibcodes for arXiv ID 2404.03701: 2024arXiv240403701F, 2024arXiv2404.03701F\n",
      "2024-04-22 15:46:28,622 - WARNING - No references found for arXiv ID: 2404.03701\n",
      "2024-04-22 15:46:28,623 - INFO - Generated bibcodes for arXiv ID 2404.12215: 2024arXiv240412215H, 2024arXiv2404.12215H\n",
      "2024-04-22 15:46:28,624 - WARNING - No references found for arXiv ID: 2404.12215\n",
      "2024-04-22 15:46:28,624 - INFO - Generated bibcodes for arXiv ID 2404.12219: 2024arXiv240412219A, 2024arXiv2404.12219A\n",
      "2024-04-22 15:46:28,625 - WARNING - No references found for arXiv ID: 2404.12219\n",
      "2024-04-22 15:46:28,626 - INFO - Generated bibcodes for arXiv ID 2404.12396: 2024arXiv240412396V, 2024arXiv2404.12396V\n",
      "2024-04-22 15:46:28,627 - WARNING - No references found for arXiv ID: 2404.12396\n",
      "2024-04-22 15:46:28,628 - INFO - Generated bibcodes for arXiv ID 2404.12408: 2024arXiv240412408C, 2024arXiv2404.12408C\n",
      "2024-04-22 15:46:28,628 - WARNING - No references found for arXiv ID: 2404.12408\n",
      "2024-04-22 15:46:28,629 - INFO - Generated bibcodes for arXiv ID 2404.12418: 2024arXiv240412418G, 2024arXiv2404.12418G\n",
      "2024-04-22 15:46:28,629 - WARNING - No references found for arXiv ID: 2404.12418\n",
      "2024-04-22 15:46:28,630 - INFO - Generated bibcodes for arXiv ID 2404.12463: 2024arXiv240412463K, 2024arXiv2404.12463K\n",
      "2024-04-22 15:46:28,630 - WARNING - No references found for arXiv ID: 2404.12463\n",
      "2024-04-22 15:46:28,631 - INFO - Generated bibcodes for arXiv ID 2404.12478: 2024arXiv240412478R, 2024arXiv2404.12478R\n",
      "2024-04-22 15:46:28,632 - WARNING - No references found for arXiv ID: 2404.12478\n",
      "2024-04-22 15:46:28,632 - INFO - Generated bibcodes for arXiv ID 2404.12481: 2024arXiv240412481L, 2024arXiv2404.12481L\n",
      "2024-04-22 15:46:28,633 - WARNING - No references found for arXiv ID: 2404.12481\n",
      "2024-04-22 15:46:28,633 - INFO - Generated bibcodes for arXiv ID 2404.12483: 2024arXiv240412483X, 2024arXiv2404.12483X\n",
      "2024-04-22 15:46:28,634 - WARNING - No references found for arXiv ID: 2404.12483\n",
      "2024-04-22 15:46:28,634 - INFO - Generated bibcodes for arXiv ID 2404.12484: 2024arXiv240412484Z, 2024arXiv2404.12484Z\n",
      "2024-04-22 15:46:28,635 - WARNING - No references found for arXiv ID: 2404.12484\n",
      "2024-04-22 15:46:28,635 - INFO - Generated bibcodes for arXiv ID 2404.12499: 2024arXiv240412499D, 2024arXiv2404.12499D\n",
      "2024-04-22 15:46:28,637 - WARNING - No references found for arXiv ID: 2404.12499\n",
      "2024-04-22 15:46:28,638 - INFO - Generated bibcodes for arXiv ID 2404.12534: 2024arXiv240412534S, 2024arXiv2404.12534S\n",
      "2024-04-22 15:46:28,638 - WARNING - No references found for arXiv ID: 2404.12534\n",
      "2024-04-22 15:46:28,639 - INFO - Generated bibcodes for arXiv ID 2404.12544: 2024arXiv240412544E, 2024arXiv2404.12544E\n",
      "2024-04-22 15:46:28,639 - WARNING - No references found for arXiv ID: 2404.12544\n",
      "2024-04-22 15:46:28,640 - INFO - Generated bibcodes for arXiv ID 2404.12553: 2024arXiv240412553J, 2024arXiv2404.12553J\n",
      "2024-04-22 15:46:28,646 - WARNING - No references found for arXiv ID: 2404.12553\n",
      "2024-04-22 15:46:28,647 - INFO - Generated bibcodes for arXiv ID 2404.12556: 2024arXiv240412556B, 2024arXiv2404.12556B\n",
      "2024-04-22 15:46:28,648 - WARNING - No references found for arXiv ID: 2404.12556\n",
      "2024-04-22 15:46:28,649 - INFO - Generated bibcodes for arXiv ID 2404.12583: 2024arXiv240412583K, 2024arXiv2404.12583K\n",
      "2024-04-22 15:46:28,650 - WARNING - No references found for arXiv ID: 2404.12583\n",
      "2024-04-22 15:46:28,651 - INFO - Generated bibcodes for arXiv ID 2404.12586: 2024arXiv240412586C, 2024arXiv2404.12586C\n",
      "2024-04-22 15:46:28,653 - WARNING - No references found for arXiv ID: 2404.12586\n",
      "2024-04-22 15:46:28,654 - INFO - Generated bibcodes for arXiv ID 2404.12589: 2024arXiv240412589C, 2024arXiv2404.12589C\n",
      "2024-04-22 15:46:28,654 - WARNING - No references found for arXiv ID: 2404.12589\n",
      "2024-04-22 15:46:28,656 - INFO - Generated bibcodes for arXiv ID 2404.12592: 2024arXiv240412592X, 2024arXiv2404.12592X\n",
      "2024-04-22 15:46:28,656 - WARNING - No references found for arXiv ID: 2404.12592\n",
      "2024-04-22 15:46:28,657 - INFO - Generated bibcodes for arXiv ID 2404.12597: 2024arXiv240412597Z, 2024arXiv2404.12597Z\n",
      "2024-04-22 15:46:28,658 - WARNING - No references found for arXiv ID: 2404.12597\n",
      "2024-04-22 15:46:28,658 - INFO - Generated bibcodes for arXiv ID 2404.12610: 2024arXiv240412610D, 2024arXiv2404.12610D\n",
      "2024-04-22 15:46:28,659 - WARNING - No references found for arXiv ID: 2404.12610\n",
      "2024-04-22 15:46:28,660 - INFO - Generated bibcodes for arXiv ID 2404.12613: 2024arXiv240412613L, 2024arXiv2404.12613L\n",
      "2024-04-22 15:46:28,661 - WARNING - No references found for arXiv ID: 2404.12613\n",
      "2024-04-22 15:46:28,661 - INFO - Generated bibcodes for arXiv ID 2404.12648: 2024arXiv240412648H, 2024arXiv2404.12648H\n",
      "2024-04-22 15:46:28,662 - WARNING - No references found for arXiv ID: 2404.12648\n",
      "2024-04-22 15:46:28,663 - INFO - Generated bibcodes for arXiv ID 2404.12657: 2024arXiv240412657J, 2024arXiv2404.12657J\n",
      "2024-04-22 15:46:28,663 - WARNING - No references found for arXiv ID: 2404.12657\n",
      "2024-04-22 15:46:28,664 - INFO - Generated bibcodes for arXiv ID 2404.12684: 2024arXiv240412684M, 2024arXiv2404.12684M\n",
      "2024-04-22 15:46:28,665 - WARNING - No references found for arXiv ID: 2404.12684\n",
      "2024-04-22 15:46:28,665 - INFO - Generated bibcodes for arXiv ID 2404.12685: 2024arXiv240412685M, 2024arXiv2404.12685M\n",
      "2024-04-22 15:46:28,666 - WARNING - No references found for arXiv ID: 2404.12685\n",
      "2024-04-22 15:46:28,667 - INFO - Generated bibcodes for arXiv ID 2404.12692: 2024arXiv240412692M, 2024arXiv2404.12692M\n",
      "2024-04-22 15:46:28,670 - WARNING - No references found for arXiv ID: 2404.12692\n",
      "2024-04-22 15:46:28,670 - INFO - Generated bibcodes for arXiv ID 2404.12696: 2024arXiv240412696W, 2024arXiv2404.12696W\n",
      "2024-04-22 15:46:28,673 - WARNING - No references found for arXiv ID: 2404.12696\n",
      "2024-04-22 15:46:28,674 - INFO - Generated bibcodes for arXiv ID 2404.12756: 2024arXiv240412756C, 2024arXiv2404.12756C\n",
      "2024-04-22 15:46:28,675 - WARNING - No references found for arXiv ID: 2404.12756\n",
      "2024-04-22 15:46:28,677 - INFO - Generated bibcodes for arXiv ID 2404.12812: 2024arXiv240412812C, 2024arXiv2404.12812C\n",
      "2024-04-22 15:46:28,679 - WARNING - No references found for arXiv ID: 2404.12812\n",
      "2024-04-22 15:46:28,679 - INFO - Generated bibcodes for arXiv ID 2404.12828: 2024arXiv240412828M, 2024arXiv2404.12828M\n",
      "2024-04-22 15:46:28,680 - WARNING - No references found for arXiv ID: 2404.12828\n",
      "2024-04-22 15:46:28,681 - INFO - Generated bibcodes for arXiv ID 2404.12862: 2024arXiv240412862E, 2024arXiv2404.12862E\n",
      "2024-04-22 15:46:28,682 - WARNING - No references found for arXiv ID: 2404.12862\n",
      "2024-04-22 15:46:28,682 - INFO - Generated bibcodes for arXiv ID 2404.12889: 2024arXiv240412889K, 2024arXiv2404.12889K\n",
      "2024-04-22 15:46:28,683 - WARNING - No references found for arXiv ID: 2404.12889\n",
      "2024-04-22 15:46:28,685 - INFO - Generated bibcodes for arXiv ID 2404.12923: 2024arXiv240412923L, 2024arXiv2404.12923L\n",
      "2024-04-22 15:46:28,686 - WARNING - No references found for arXiv ID: 2404.12923\n",
      "2024-04-22 15:46:28,686 - INFO - Generated bibcodes for arXiv ID 2404.12940: 2024arXiv240412940B, 2024arXiv2404.12940B\n",
      "2024-04-22 15:46:28,688 - WARNING - No references found for arXiv ID: 2404.12940\n",
      "2024-04-22 15:46:28,689 - INFO - Generated bibcodes for arXiv ID 2404.12943: 2024arXiv240412943C, 2024arXiv2404.12943C\n",
      "2024-04-22 15:46:28,689 - WARNING - No references found for arXiv ID: 2404.12943\n",
      "2024-04-22 15:46:28,690 - INFO - Generated bibcodes for arXiv ID 2404.12949: 2024arXiv240412949G, 2024arXiv2404.12949G\n",
      "2024-04-22 15:46:28,693 - WARNING - No references found for arXiv ID: 2404.12949\n",
      "2024-04-22 15:46:28,696 - INFO - Generated bibcodes for arXiv ID 2404.12967: 2024arXiv240412967J, 2024arXiv2404.12967J\n",
      "2024-04-22 15:46:28,697 - WARNING - No references found for arXiv ID: 2404.12967\n",
      "2024-04-22 15:46:28,698 - INFO - Generated bibcodes for arXiv ID 2404.12968: 2024arXiv240412968K, 2024arXiv2404.12968K\n",
      "2024-04-22 15:46:28,698 - WARNING - No references found for arXiv ID: 2404.12968\n",
      "2024-04-22 15:46:28,699 - INFO - Generated bibcodes for arXiv ID 2404.13016: 2024arXiv240413016L, 2024arXiv2404.13016L\n",
      "2024-04-22 15:46:28,700 - WARNING - No references found for arXiv ID: 2404.13016\n",
      "2024-04-22 15:46:28,707 - INFO - Generated bibcodes for arXiv ID 1906.02358: 2019arXiv190602358S, 2019arXiv1906.02358S\n",
      "2024-04-22 15:46:28,708 - WARNING - No references found for arXiv ID: 1906.02358\n",
      "2024-04-22 15:46:28,708 - INFO - Generated bibcodes for arXiv ID 1907.05325: 2019arXiv190705325M, 2019arXiv1907.05325M\n",
      "2024-04-22 15:46:28,709 - WARNING - No references found for arXiv ID: 1907.05325\n",
      "2024-04-22 15:46:28,710 - INFO - Generated bibcodes for arXiv ID 1912.10642: 2019arXiv191210642P, 2019arXiv1912.10642P\n",
      "2024-04-22 15:46:28,710 - WARNING - No references found for arXiv ID: 1912.10642\n",
      "2024-04-22 15:46:28,711 - INFO - Generated bibcodes for arXiv ID 2101.07223: 2021arXiv210107223A, 2021arXiv2101.07223A\n",
      "2024-04-22 15:46:28,712 - WARNING - No references found for arXiv ID: 2101.07223\n",
      "2024-04-22 15:46:28,713 - INFO - Generated bibcodes for arXiv ID 2101.09180: 2021arXiv210109180Z, 2021arXiv2101.09180Z\n",
      "2024-04-22 15:46:28,713 - WARNING - No references found for arXiv ID: 2101.09180\n",
      "2024-04-22 15:46:28,714 - INFO - Generated bibcodes for arXiv ID 2105.05716: 2021arXiv210505716R, 2021arXiv2105.05716R\n",
      "2024-04-22 15:46:28,715 - WARNING - No references found for arXiv ID: 2105.05716\n",
      "2024-04-22 15:46:28,715 - INFO - Generated bibcodes for arXiv ID 2111.01993: 2021arXiv211101993U, 2021arXiv2111.01993U\n",
      "2024-04-22 15:46:28,717 - WARNING - No references found for arXiv ID: 2111.01993\n",
      "2024-04-22 15:46:28,717 - INFO - Generated bibcodes for arXiv ID 2202.07595: 2022arXiv220207595H, 2022arXiv2202.07595H\n",
      "2024-04-22 15:46:28,718 - WARNING - No references found for arXiv ID: 2202.07595\n",
      "2024-04-22 15:46:28,719 - INFO - Generated bibcodes for arXiv ID 2203.07976: 2022arXiv220307976R, 2022arXiv2203.07976R\n",
      "2024-04-22 15:46:28,719 - WARNING - No references found for arXiv ID: 2203.07976\n",
      "2024-04-22 15:46:28,720 - INFO - Generated bibcodes for arXiv ID 2203.11593: 2022arXiv220311593J, 2022arXiv2203.11593J\n",
      "2024-04-22 15:46:28,721 - WARNING - No references found for arXiv ID: 2203.11593\n",
      "2024-04-22 15:46:28,722 - INFO - Generated bibcodes for arXiv ID 2204.10325: 2022arXiv220410325D, 2022arXiv2204.10325D\n",
      "2024-04-22 15:46:28,723 - WARNING - No references found for arXiv ID: 2204.10325\n",
      "2024-04-22 15:46:28,723 - INFO - Generated bibcodes for arXiv ID 2205.01438: 2022arXiv220501438Z, 2022arXiv2205.01438Z\n",
      "2024-04-22 15:46:28,724 - WARNING - No references found for arXiv ID: 2205.01438\n",
      "2024-04-22 15:46:28,725 - INFO - Generated bibcodes for arXiv ID 2205.02533: 2022arXiv220502533X, 2022arXiv2205.02533X\n",
      "2024-04-22 15:46:28,725 - WARNING - No references found for arXiv ID: 2205.02533\n",
      "2024-04-22 15:46:28,726 - INFO - Generated bibcodes for arXiv ID 2205.14223: 2022arXiv220514223Z, 2022arXiv2205.14223Z\n",
      "2024-04-22 15:46:28,731 - WARNING - No references found for arXiv ID: 2205.14223\n",
      "2024-04-22 15:46:28,732 - INFO - Generated bibcodes for arXiv ID 2206.09325: 2022arXiv220609325Z, 2022arXiv2206.09325Z\n",
      "2024-04-22 15:46:28,733 - WARNING - No references found for arXiv ID: 2206.09325\n",
      "2024-04-22 15:46:28,733 - INFO - Generated bibcodes for arXiv ID 2206.15101: 2022arXiv220615101R, 2022arXiv2206.15101R\n",
      "2024-04-22 15:46:28,734 - WARNING - No references found for arXiv ID: 2206.15101\n",
      "2024-04-22 15:46:28,735 - INFO - Generated bibcodes for arXiv ID 2209.01621: 2022arXiv220901621B, 2022arXiv2209.01621B\n",
      "2024-04-22 15:46:28,735 - WARNING - No references found for arXiv ID: 2209.01621\n",
      "2024-04-22 15:46:28,736 - INFO - Generated bibcodes for arXiv ID 2209.10192: 2022arXiv220910192J, 2022arXiv2209.10192J\n",
      "2024-04-22 15:46:28,737 - WARNING - No references found for arXiv ID: 2209.10192\n",
      "2024-04-22 15:46:28,737 - INFO - Generated bibcodes for arXiv ID 2210.01988: 2022arXiv221001988Z, 2022arXiv2210.01988Z\n",
      "2024-04-22 15:46:28,738 - WARNING - No references found for arXiv ID: 2210.01988\n",
      "2024-04-22 15:46:28,741 - INFO - Generated bibcodes for arXiv ID 2210.08298: 2022arXiv221008298F, 2022arXiv2210.08298F\n",
      "2024-04-22 15:46:28,741 - WARNING - No references found for arXiv ID: 2210.08298\n",
      "2024-04-22 15:46:28,742 - INFO - Generated bibcodes for arXiv ID 2211.02866: 2022arXiv221102866B, 2022arXiv2211.02866B\n",
      "2024-04-22 15:46:28,743 - WARNING - No references found for arXiv ID: 2211.02866\n",
      "2024-04-22 15:46:28,743 - INFO - Generated bibcodes for arXiv ID 2211.07440: 2022arXiv221107440R, 2022arXiv2211.07440R\n",
      "2024-04-22 15:46:28,745 - WARNING - No references found for arXiv ID: 2211.07440\n",
      "2024-04-22 15:46:28,746 - INFO - Generated bibcodes for arXiv ID 2211.11424: 2022arXiv221111424X, 2022arXiv2211.11424X\n",
      "2024-04-22 15:46:28,747 - WARNING - No references found for arXiv ID: 2211.11424\n",
      "2024-04-22 15:46:28,749 - INFO - Generated bibcodes for arXiv ID 2212.03218: 2022arXiv221203218K, 2022arXiv2212.03218K\n",
      "2024-04-22 15:46:28,750 - WARNING - No references found for arXiv ID: 2212.03218\n",
      "2024-04-22 15:46:28,750 - INFO - Generated bibcodes for arXiv ID 2301.00185: 2022arXiv230100185S, 2022arXiv2301.00185S\n",
      "2024-04-22 15:46:28,751 - WARNING - No references found for arXiv ID: 2301.00185\n",
      "2024-04-22 15:46:28,753 - INFO - Generated bibcodes for arXiv ID 2301.00812: 2022arXiv230100812Y, 2022arXiv2301.00812Y\n",
      "2024-04-22 15:46:28,754 - WARNING - No references found for arXiv ID: 2301.00812\n",
      "2024-04-22 15:46:28,755 - INFO - Generated bibcodes for arXiv ID 2301.06280: 2023arXiv230106280J, 2023arXiv2301.06280J\n",
      "2024-04-22 15:46:28,755 - WARNING - No references found for arXiv ID: 2301.06280\n",
      "2024-04-22 15:46:28,756 - INFO - Generated bibcodes for arXiv ID 2301.06520: 2023arXiv230106520M, 2023arXiv2301.06520M\n",
      "2024-04-22 15:46:28,757 - WARNING - No references found for arXiv ID: 2301.06520\n",
      "2024-04-22 15:46:28,757 - INFO - Generated bibcodes for arXiv ID 2303.04467: 2023arXiv230304467S, 2023arXiv2303.04467S\n",
      "2024-04-22 15:46:28,758 - WARNING - No references found for arXiv ID: 2303.04467\n",
      "2024-04-22 15:46:28,760 - INFO - Generated bibcodes for arXiv ID 2303.10216: 2023arXiv230310216K, 2023arXiv2303.10216K\n",
      "2024-04-22 15:46:28,761 - WARNING - No references found for arXiv ID: 2303.10216\n",
      "2024-04-22 15:46:28,762 - INFO - Generated bibcodes for arXiv ID 2303.10322: 2023arXiv230310322S, 2023arXiv2303.10322S\n",
      "2024-04-22 15:46:28,762 - WARNING - No references found for arXiv ID: 2303.10322\n",
      "2024-04-22 15:46:28,763 - INFO - Generated bibcodes for arXiv ID 2303.12342: 2023arXiv230312342L, 2023arXiv2303.12342L\n",
      "2024-04-22 15:46:28,763 - WARNING - No references found for arXiv ID: 2303.12342\n",
      "2024-04-22 15:46:28,764 - INFO - Generated bibcodes for arXiv ID 2303.16421: 2023arXiv230316421B, 2023arXiv2303.16421B\n",
      "2024-04-22 15:46:28,765 - WARNING - No references found for arXiv ID: 2303.16421\n",
      "2024-04-22 15:46:28,765 - INFO - Generated bibcodes for arXiv ID 2304.00372: 2023arXiv230400372L, 2023arXiv2304.00372L\n",
      "2024-04-22 15:46:28,766 - WARNING - No references found for arXiv ID: 2304.00372\n",
      "2024-04-22 15:46:28,767 - INFO - Generated bibcodes for arXiv ID 2304.05166: 2023arXiv230405166M, 2023arXiv2304.05166M\n",
      "2024-04-22 15:46:28,767 - WARNING - No references found for arXiv ID: 2304.05166\n",
      "2024-04-22 15:46:28,768 - INFO - Generated bibcodes for arXiv ID 2304.07468: 2023arXiv230407468K, 2023arXiv2304.07468K\n",
      "2024-04-22 15:46:28,769 - WARNING - No references found for arXiv ID: 2304.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: [{'id': '1711.08265', 'title': 'sparse variable selection on high dimensional heterogeneous data with   tree structured responses', 'abstract': 'we consider the problem of sparse variable selection on high dimension heterogeneous data sets, which has been taking on renewed interest recently due to the growth of biological and medical data sets with complex, non-i.i.d. structures and huge quantities of response variables. the heterogeneity is likely to confound the association between explanatory variables and responses, resulting in enormous false discoveries when lasso or its variants are na\\\\\"ively applied. therefore, developing effective confounder correction methods is a growing heat point among researchers. however, ordinarily employing recent confounder correction methods will result in undesirable performance due to the ignorance of the convoluted interdependency among response variables. to fully improve current variable selection methods, we introduce a model, the tree-guided sparse linear mixed model, that can utilize the dependency information from multiple responses to explore how specifically clusters are and select the active variables from heterogeneous data. through extensive experiments on synthetic and real data sets, we show that our proposed model outperforms the existing methods and achieves the highest roc area.', 'doi': '10.1109/access.2024.3384309', 'created': '2017-11-22', 'url': 'https://arxiv.org/abs/1711.08265', 'authors': ['hui liu', 'xiang liu', 'jing diao', 'wenting ye', 'xueling liu', 'dehui wei']}, {'id': '1907.05325', 'title': 'low-rank matrix completion and denoising under poisson noise', 'abstract': 'this paper considers the problem of estimating a low-rank matrix from the observation of all or a subset of its entries in the presence of poisson noise. when we observe all entries, this is a problem of matrix denoising; when we observe only a subset of the entries, this is a problem of matrix completion. in both cases, we exploit an assumption that the underlying matrix is low-rank. specifically, we analyze several estimators, including a constrained nuclear-norm minimization program, nuclear-norm regularized least squares, and a nonconvex constrained low-rank optimization problem. we show that for all three estimators, with high probability, we have an upper error bound (in the frobenius norm error metric) that depends on the matrix rank, the fraction of the elements observed, and maximal row and column sums of the true matrix. we furthermore show that the above results are minimax optimal (within a universal constant) in classes of matrices with low rank and bounded row and column sums. we also extend these results to handle the case of matrix multinomial denoising and completion.', 'doi': '10.1093/imaiai/iaaa020', 'created': '2019-07-11', 'url': 'https://arxiv.org/abs/1907.05325', 'authors': ['andrew d. mcrae', 'mark a. davenport']}, {'id': '2101.01157', 'title': 'a tutorial on spatiotemporal partially observed markov process models   via the r package spatpomp', 'abstract': 'we describe a computational framework for modeling and statistical inference on high-dimensional stochastic dynamic systems. our primary motivation is the investigation of metapopulation dynamics arising from a collection of spatially distributed, interacting biological populations. to make progress on this goal, we embed it in a more general problem: inference for a collection of interacting partially observed nonlinear non-gaussian stochastic processes. each process in the collection is called a unit; in the case of spatiotemporal models, the units correspond to distinct spatial locations. the dynamic state for each unit may be discrete or continuous, scalar or vector valued. in metapopulation applications, the state can represent a structured population or the abundances of a collection of species at a single location. we consider models where the collection of states has a markov property. a sequence of noisy measurements is made on each unit, resulting in a collection of time series. a model of this form is called a spatiotemporal partially observed markov process (spatpomp). the r package spatpomp provides an environment for implementing spatpomp models, analyzing data using existing methods, and developing new inference approaches. our presentation of spatpomp reviews various methodologies in a unifying notational framework. we demonstrate the package on a simple gaussian system and on a nontrivial epidemiological model for measles transmission within and between cities. we show how to construct user-specified spatpomp models within spatpomp.', 'doi': '', 'created': '2021-01-04', 'url': 'https://arxiv.org/abs/2101.01157', 'authors': ['kidus asfaw', 'joonha park', 'aaron a. king', 'edward l. ionides']}, {'id': '2110.15517', 'title': 'cp factor model for dynamic tensors', 'abstract': 'observations in various applications are frequently represented as a time series of multidimensional arrays, called tensor time series, preserving the inherent multidimensional structure. in this paper, we present a factor model approach, in a form similar to tensor cp decomposition, to the analysis of high-dimensional dynamic tensor time series. as the loading vectors are uniquely defined but not necessarily orthogonal, it is significantly different from the existing tensor factor models based on tucker-type tensor decomposition. the model structure allows for a set of uncorrelated one-dimensional latent dynamic factor processes, making it much more convenient to study the underlying dynamics of the time series. a new high order projection estimator is proposed for such a factor model, utilizing the special structure and the idea of the higher order orthogonal iteration procedures commonly used in tucker-type tensor factor model and general tensor cp decomposition procedures. theoretical investigation provides statistical error bounds for the proposed methods, which shows the significant advantage of utilizing the special model structure. simulation study is conducted to further demonstrate the finite sample properties of the estimators. real data application is used to illustrate the model and its interpretations.', 'doi': '', 'created': '2021-10-28', 'url': 'https://arxiv.org/abs/2110.15517', 'authors': ['yuefeng han', 'dan yang', 'cun-hui zhang', 'rong chen']}, {'id': '2111.04652', 'title': 'optimal convex lifted sparse phase retrieval and pca with an atomic   matrix norm regularizer', 'abstract': 'we present novel analysis and algorithms for solving sparse phase retrieval and sparse principal component analysis (pca) with convex lifted matrix formulations. the key innovation is a new mixed atomic matrix norm that, when used as regularization, promotes low-rank matrices with sparse factors. we show that convex programs with this atomic norm as a regularizer provide near-optimal sample complexity and error rate guarantees for sparse phase retrieval and sparse pca. while we do not know how to solve the convex programs exactly with an efficient algorithm, for the phase retrieval case we carefully analyze the program and its dual and thereby derive a practical heuristic algorithm. we show empirically that this practical algorithm performs similarly to existing state-of-the-art algorithms.', 'doi': '10.1109/tit.2022.3228508', 'created': '2021-11-08', 'url': 'https://arxiv.org/abs/2111.04652', 'authors': ['andrew d. mcrae', 'justin romberg', 'mark a. davenport']}, {'id': '2201.09648', 'title': 'differentially private estimation in a class of directed network models', 'abstract': 'although the theoretical properties in the $p_0$ model based on a differentially private bi-degree sequence have been derived, it is still lack of a unified theory for a general class of directed network models with the $p_{0}$ model as a special case. we use the popular laplace data releasing method to output the bi-degree sequence of directed networks, which satisfies the private standard--differential privacy. the method of moment is used to estimate unknown parameters. we prove that the differentially private estimator is uniformly consistent and asymptotically normal under some conditions. our results are illustrated by the probit model. we carry out simulation studies to illustrate theoretical results and provide a real data analysis.', 'doi': '', 'created': '2022-01-24', 'url': 'https://arxiv.org/abs/2201.09648', 'authors': ['lu pan', 'jianwei hu', 'peiyan li']}, {'id': '2210.02171', 'title': 'a uniform kernel trick for high-dimensional two-sample problems', 'abstract': 'we use a suitable version of the so-called \"kernel trick\" to devise two-sample (homogeneity) tests, especially focussed on high-dimensional and functional data. our proposal entails a simplification related to the important practical problem of selecting an appropriate kernel function. specifically, we apply a uniform variant of the kernel trick which involves the supremum within a class of kernel-based distances. we obtain the asymptotic distribution (under the null and alternative hypotheses) of the test statistic. the proofs rely on empirical processes theory, combined with the delta method and hadamard (directional) differentiability techniques, and functional karhunen-lo\\\\`eve-type expansions of the underlying processes. this methodology has some advantages over other standard approaches in the literature. we also give some experimental insight into the performance of our proposal compared to the original kernel-based approach \\\\cite{gretton2007} and the test based on energy distances \\\\cite{szekely-rizzo-2017}.', 'doi': '', 'created': '2022-10-05', 'url': 'https://arxiv.org/abs/2210.02171', 'authors': ['javier cárcamo', 'antonio cuevas', 'luis-alberto rodríguez']}, {'id': '2210.16655', 'title': 'a note on the equivalence between the conditional uncorrelation and the   independence of random variables', 'abstract': 'it is well known that while the independence of random variables implies zero correlation, the opposite is not true. namely, uncorrelated random variables are not necessarily independent. in this note we show that the implication could be reversed if we consider the localised version of the correlation coefficient. more specifically, we show that if random variables are conditionally (locally) uncorrelated for any quantile conditioning sets, then they are independent. for simplicity, we focus on the absolutely continuous case. also, we illustrate potential usefulness of the stated result using two simple examples.', 'doi': '10.1214/24-ejs2212', 'created': '2022-10-29', 'url': 'https://arxiv.org/abs/2210.16655', 'authors': ['piotr jaworski', 'damian jelito', 'marcin pitera']}, {'id': '2212.04911', 'title': 'a design and analytic strategy for monitoring disease positivity and   case characteristics in accessible closed populations', 'abstract': 'we propose a monitoring strategy for efficient and robust estimation of disease prevalence and case numbers within closed and enumerated populations such as schools, workplaces, or retirement communities. the proposed design relies largely on voluntary testing, notoriously biased (e.g., in the case of covid-19) due to non-representative sampling. the approach yields unbiased and comparatively precise estimates with no assumptions about factors underlying selection of individuals for voluntary testing, building on the strength of what can be a small random sampling component. this component unlocks a previously proposed \"anchor stream\" estimator, a well-calibrated alternative to classical capture-recapture (crc) estimators based on two data streams. we show here that this estimator is equivalent to a direct standardization based on \"capture\", i.e., selection (or not) by the voluntary testing program, made possible by means of a key parameter identified by design. this equivalency simultaneously allows for novel two-stream crc-like estimation of general means (e.g., of continuous variables such as antibody levels or biomarkers). for inference, we propose adaptations of a bayesian credible interval when estimating case counts and bootstrapping when estimating means of continuous variables. we use simulations to demonstrate significant precision benefits relative to random sampling alone.', 'doi': '10.1093/aje/kwad177', 'created': '2022-12-09', 'url': 'https://arxiv.org/abs/2212.04911', 'authors': ['robert h. lyles', 'yuzi zhang', 'lin ge', 'lance a. waller']}, {'id': '2302.03558', 'title': 'enhanced inference for finite population sampling-based prevalence   estimation with misclassification errors', 'abstract': 'epidemiologic screening programs often make use of tests with small, but non-zero probabilities of misdiagnosis. in this article, we assume the target population is finite with a fixed number of true cases, and that we apply an imperfect test with known sensitivity and specificity to a sample of individuals from the population. in this setting, we propose an enhanced inferential approach for use in conjunction with sampling-based bias-corrected prevalence estimation. while ignoring the finite nature of the population can yield markedly conservative estimates, direct application of a standard finite population correction (fpc) conversely leads to underestimation of variance. we uncover a way to leverage the typical fpc indirectly toward valid statistical inference. in particular, we derive a readily estimable extra variance component induced by misclassification in this specific but arguably common diagnostic testing scenario. our approach yields a standard error estimate that properly captures the sampling variability of the usual bias-corrected maximum likelihood estimator of disease prevalence. finally, we develop an adapted bayesian credible interval for the true prevalence that offers improved frequentist properties (i.e., coverage and width) relative to a wald-type confidence interval. we report the simulation results to demonstrate the enhanced performance of the proposed inferential methods.', 'doi': '10.1080/00031305.2023.2250401', 'created': '2023-02-07', 'url': 'https://arxiv.org/abs/2302.03558', 'authors': ['lin ge', 'yuzi zhang', 'lance a. waller', 'robert h. lyles']}, {'id': '2303.06434', 'title': 'direct bayesian regression for distribution-valued covariates', 'abstract': 'in this manuscript, we study the problem of scalar-on-distribution regression; that is, instances where subject-specific distributions or densities, or in practice, repeated measures from those distributions, are the covariates related to a scalar outcome via a regression model. we propose a direct regression for such distribution-valued covariates that circumvents estimating subject-specific densities and directly uses the observed repeated measures as covariates. the model is invariant to any transformation or ordering of the repeated measures. endowing the regression function with a gaussian process prior, we obtain closed form or conjugate bayesian inference. our method subsumes the standard bayesian non-parametric regression using gaussian processes as a special case. theoretically, we show that the method can achieve an optimal estimation error bound. to our knowledge, this is the first theoretical study on bayesian regression using distribution-valued covariates. through simulation studies and analysis of activity count dataset, we demonstrate that our method performs better than approaches that require an intermediate density estimation step.', 'doi': '', 'created': '2023-03-11', 'url': 'https://arxiv.org/abs/2303.06434', 'authors': ['bohao tang', 'sandipan pramanik', 'yi zhao', 'brian caffo', 'abhirup datta']}, {'id': '2303.10322', 'title': 'inverse cubature and quadrature kalman filters', 'abstract': \"recent research in inverse cognition with cognitive radar has led to the development of inverse stochastic filters that are employed by the target to infer the information the cognitive radar may have learned. prior works addressed this inverse cognition problem by proposing inverse kalman filter (i-kf) and inverse extended kf (i-ekf), respectively, for linear and non-linear gaussian state-space models. however, in practice, many counter-adversarial settings involve highly non-linear system models, wherein ekf's linearization often fails. in this paper, we consider the efficient numerical integration techniques to address such non-linearities and, to this end, develop inverse cubature kf (i-ckf), inverse quadrature kf (i-qkf), and inverse cubature-quadrature kf (i-cqkf). for the unknown system model case, we develop reproducing kernel hilbert space (rkhs)-based ckf. we derive the stochastic stability conditions for the proposed filters in the exponential-mean-squared-boundedness sense and prove the filters' consistency. numerical experiments demonstrate the estimation accuracy of our i-ckf, i-qkf, and i-cqkf with the recursive cram\\\\'{e}r-rao lower bound as a benchmark.\", 'doi': '', 'created': '2023-03-17', 'url': 'https://arxiv.org/abs/2303.10322', 'authors': ['himali singh', 'kumar vijay mishra', 'arpan chattopadhyay']}, {'id': '2304.02025', 'title': 'estimating global identifiability using conditional mutual information   in a bayesian framework', 'abstract': 'a novel information-theoretic approach is proposed to assess the global practical identifiability of bayesian statistical models. based on the concept of conditional mutual information, an estimate of information gained for each model parameter is used to quantify the identifiability with practical considerations. no assumptions are made about the structure of the statistical model or the prior distribution while constructing the estimator. the estimator has the following notable advantages: first, no controlled experiment or data is required to conduct the practical identifiability analysis; second, unlike popular variance-based global sensitivity analysis methods, different forms of uncertainties, such as model-form, parameter, or measurement can be taken into account; third, the identifiability analysis is global, and therefore independent of a realization of the parameters. if an individual parameter has low identifiability, it can belong to an identifiable subset such that parameters within the subset have a functional relationship and thus have a combined effect on the statistical model. the practical identifiability framework is extended to highlight the dependencies between parameter pairs that emerge a posteriori to find identifiable parameter subsets. the applicability of the proposed approach is demonstrated using a linear gaussian model and a non-linear methane-air reduced kinetics model. it is shown that by examining the information gained for each model parameter along with its dependencies with other parameters, a subset of parameters that can be estimated with high posterior certainty can be found.', 'doi': '10.1038/s41598-023-44589-3', 'created': '2023-04-04', 'url': 'https://arxiv.org/abs/2304.02025', 'authors': ['sahil bhola', 'karthik duraisamy']}, {'id': '2306.08553', 'title': 'noise stability optimization for flat minima with tight rates', 'abstract': \"we consider minimizing a perturbed function $f(w) = \\\\mathbb{e}_{u}[f(w + u)]$, given a function $f: \\\\mathbb{r}^d \\\\rightarrow \\\\mathbb{r}$ and a random sample $u$ from a distribution $\\\\mathcal{p}$ with mean zero. when $\\\\mathcal{p}$ is the isotropic gaussian, $f(w)$ is roughly equal to $f(w)$ plus a penalty on the trace of $\\\\nabla^2 f(w)$, scaled by the variance of $\\\\mathcal{p}$. this penalty on the hessian has the benefit of improving generalization, through pac-bayes analysis. it is useful in low-sample regimes, for instance, when a (large) pre-trained model is fine-tuned on a small data set. one way to minimize $f$ is by adding $u$ to $w$, and then run sgd. we observe, empirically, that this noise injection does not provide significant gains over sgd, in our experiments of conducting fine-tuning on three image classification data sets. we design a simple, practical algorithm that adds noise along both $u$ and $-u$, with the option of adding several perturbations and taking their average. we analyze the convergence of this algorithm, showing tight rates on the norm of the output's gradient.   we provide a comprehensive empirical analysis of our algorithm, by first showing that in an over-parameterized matrix sensing problem, it can find solutions with lower test loss than naive noise injection. then, we compare our algorithm with four sharpness-reducing training methods (such as the sharpness-aware minimization (foret et al., 2021)). we find that our algorithm can outperform them by up to 1.8% test accuracy, for fine-tuning resnet on six image classification data sets. it leads to a 17.7% (and 12.8%) reduction in the trace (and largest eigenvalue) of the hessian matrix of the loss surface. this form of regularization on the hessian is compatible with $\\\\ell_2$ weight decay (and data augmentation), in the sense that combining both can lead to improved empirical performance.\", 'doi': '', 'created': '2023-06-14', 'url': 'https://arxiv.org/abs/2306.08553', 'authors': ['haotian ju', 'dongyue li', 'hongyang r. zhang']}, {'id': '2309.00380', 'title': 'learning multi-modal generative models with permutation-invariant   encoders and tighter variational bounds', 'abstract': 'devising deep latent variable models for multi-modal data has been a long-standing theme in machine learning research. multi-modal variational autoencoders (vaes) have been a popular generative model class that learns latent representations that jointly explain multiple modalities. various objective functions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic considerations. to encode latent variables from different modality subsets, product-of-experts (poe) or mixture-of-experts (moe) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regarding their generative quality or consistency across multiple modalities. in this work, we consider a variational bound that can tightly approximate the data log-likelihood. we develop more flexible aggregation schemes that generalize poe or moe approaches by combining encoded features from different modalities based on permutation-invariant neural networks. our numerical experiments illustrate trade-offs for multi-modal variational bounds and various aggregation schemes. we show that tighter variational bounds and more flexible aggregation models can become beneficial when one wants to approximate the true joint distribution over observed modalities and latent variables in identifiable models.', 'doi': '', 'created': '2023-09-01', 'url': 'https://arxiv.org/abs/2309.00380', 'authors': ['marcel hirt', 'domenico campolo', 'victoria leong', 'juan-pablo ortega']}, {'id': '2310.05898', 'title': 'lion secretly solves constrained optimization: as lyapunov predicts', 'abstract': \"lion (evolved sign momentum), a new optimizer discovered through program search, has shown promising results in training large ai models. it performs comparably or favorably to adamw but with greater memory efficiency. as we can expect from the results of a random search program, lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, polak, and nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. thus, even though lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. this lack of theoretical clarity limits opportunities to further enhance and expand lion's efficacy.   this work aims to demystify lion. based on both continuous-time and discrete-time analysis, we demonstrate that lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint $\\\\|x\\\\|_\\\\infty \\\\leq 1/\\\\lambda$. lion achieves this through the incorporation of decoupled weight decay, where $\\\\lambda$ represents the weight decay coefficient. our analysis is made possible by the development of a new lyapunov function for the lion updates. it applies to a broader family of lion-$\\\\kappa$ algorithms, where the $\\\\text{sign}(\\\\cdot)$ operator in lion is replaced by the subgradient of a convex function $\\\\kappa$, leading to the solution of a general composite optimization problem of $\\\\min_x f(x) + \\\\kappa^*(x)$. our findings provide valuable insights into the dynamics of lion and pave the way for further improvements and extensions of lion-related algorithms.\", 'doi': '', 'created': '2023-10-09', 'url': 'https://arxiv.org/abs/2310.05898', 'authors': ['lizhang chen', 'bo liu', 'kaizhao liang', 'qiang liu']}, {'id': '2310.07983', 'title': 'revisiting decentralized proxskip: achieving linear speedup', 'abstract': 'the proxskip algorithm for decentralized and federated learning is gaining increasing attention due to its proven benefits in accelerating communication complexity while maintaining robustness against data heterogeneity. however, existing analyses of proxskip are limited to the strongly convex setting and do not achieve linear speedup, where convergence performance increases linearly with respect to the number of nodes. so far, questions remain open about how proxskip behaves in the non-convex setting and whether linear speedup is achievable.   in this paper, we revisit decentralized proxskip and address both questions. we demonstrate that the leading communication complexity of proxskip is $\\\\mathcal{o}\\\\left(\\\\frac{p\\\\sigma^2}{n\\\\epsilon^2}\\\\right)$ for non-convex and convex settings, and $\\\\mathcal{o}\\\\left(\\\\frac{p\\\\sigma^2}{n\\\\epsilon}\\\\right)$ for the strongly convex setting, where $n$ represents the number of nodes, $p$ denotes the probability of communication, $\\\\sigma^2$ signifies the level of stochastic noise, and $\\\\epsilon$ denotes the desired accuracy level. this result illustrates that proxskip achieves linear speedup and can asymptotically reduce communication overhead proportional to the probability of communication. additionally, for the strongly convex setting, we further prove that proxskip can achieve linear speedup with network-independent stepsizes.', 'doi': '', 'created': '2023-10-11', 'url': 'https://arxiv.org/abs/2310.07983', 'authors': ['luyao guo', 'sulaiman a. alghunaim', 'kun yuan', 'laurent condat', 'jinde cao']}, {'id': '2310.12079', 'title': 'differential equation scaling limits of shaped and unshaped neural   networks', 'abstract': 'recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. however, these results do not a priori tell us anything about \"ordinary\" unshaped networks, where the activation is unchanged as the network size grows. in this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks.   firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected resnet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (mlp) with depth $d \\\\ll$ width $n$ and shaped relu activation at rate $d^{-1/2}$.   secondly, for an unshaped mlp at initialization, we derive the first order asymptotic correction to the layerwise correlation. in particular, if $\\\\rho_\\\\ell$ is the correlation at layer $\\\\ell$, then $q_t = \\\\ell^2 (1 - \\\\rho_\\\\ell)$ with $t = \\\\frac{\\\\ell}{n}$ converges to an sde with a singularity at $t=0$.   these results together provide a connection between shaped and unshaped network architectures, and opens up the possibility of studying the effect of normalization methods and how it connects with shaping activation functions.', 'doi': '', 'created': '2023-10-18', 'url': 'https://arxiv.org/abs/2310.12079', 'authors': ['mufan bill li', 'mihai nica']}, {'id': '2310.18784', 'title': 'high-probability convergence bounds for nonlinear stochastic gradient   descent under heavy-tailed noise', 'abstract': 'we study high-probability convergence guarantees of learning on streaming data in the presence of heavy-tailed noise. in the proposed scenario, the model is updated in an online fashion, as new information is observed, without storing any additional data. to combat the heavy-tailed noise, we consider a general framework of nonlinear stochastic gradient descent (sgd), providing several strong results. first, for non-convex costs and component-wise nonlinearities, we establish a convergence rate arbitrarily close to $\\\\mathcal{o}\\\\left(t^{-\\\\frac{1}{4}}\\\\right)$, whose exponent is independent of noise and problem parameters. second, for strongly convex costs and a broader class of nonlinearities, we establish convergence of the last iterate to the optimum, with a rate $\\\\mathcal{o}\\\\left(t^{-\\\\zeta} \\\\right)$, where $\\\\zeta \\\\in (0,1)$ depends on problem parameters, noise and nonlinearity. as we show analytically and numerically, $\\\\zeta$ can be used to inform the preferred choice of nonlinearity for given problem settings. compared to state-of-the-art, who only consider clipping, require bounded noise moments of order $\\\\eta \\\\in (1,2]$, and establish convergence rates whose exponents go to zero as $\\\\eta \\\\rightarrow 1$, we provide high-probability guarantees for a much broader class of nonlinearities and symmetric density noise, with convergence rates whose exponents are bounded away from zero, even when the noise has finite first moment only. moreover, in the case of strongly convex functions, we demonstrate analytically and numerically that clipping is not always the optimal nonlinearity, further underlining the value of our general framework.', 'doi': '', 'created': '2023-10-28', 'url': 'https://arxiv.org/abs/2310.18784', 'authors': ['aleksandar armacki', 'pranay sharma', 'gauri joshi', 'dragana bajovic', 'dusan jakovetic', 'soummya kar']}, {'id': '2310.19454', 'title': 'mmm and mmmsynth: clustering of heterogeneous tabular data, and   synthetic data generation', 'abstract': \"we provide new algorithms for two tasks relating to heterogeneous tabular datasets: clustering, and synthetic data generation. tabular datasets typically consist of heterogeneous data types (numerical, ordinal, categorical) in columns, but may also have hidden cluster structure in their rows: for example, they may be drawn from heterogeneous (geographical, socioeconomic, methodological) sources, such that the outcome variable they describe (such as the presence of a disease) may depend not only on the other variables but on the cluster context. moreover, sharing of biomedical data is often hindered by patient confidentiality laws, and there is current interest in algorithms to generate synthetic tabular data from real data, for example via deep learning.   we demonstrate a novel em-based clustering algorithm, mmm (``madras mixture model''), that outperforms standard algorithms in determining clusters in synthetic heterogeneous data, and recovers structure in real data. based on this, we demonstrate a synthetic tabular data generation algorithm, mmmsynth, that pre-clusters the input data, and generates cluster-wise synthetic data assuming cluster-specific data distributions for the input columns. we benchmark this algorithm by testing the performance of standard ml algorithms when they are trained on synthetic data and tested on real published datasets. our synthetic data generation algorithm outperforms other literature tabular-data generators, and approaches the performance of training purely with real data.\", 'doi': '10.1371/journal.pone.0302271', 'created': '2023-10-30', 'url': 'https://arxiv.org/abs/2310.19454', 'authors': ['chandrani kumari', 'rahul siddharthan']}, {'id': '2311.00944', 'title': 'stochastic smoothed gradient descent ascent for federated minimax   optimization', 'abstract': 'in recent years, federated minimax optimization has attracted growing interest due to its extensive applications in various machine learning tasks. while smoothed alternative gradient descent ascent (smoothed-agda) has proved its success in centralized nonconvex minimax optimization, how and whether smoothing technique could be helpful in federated setting remains unexplored. in this paper, we propose a new algorithm termed federated stochastic smoothed gradient descent ascent (fess-gda), which utilizes the smoothing technique for federated minimax optimization. we prove that fess-gda can be uniformly used to solve several classes of federated minimax problems and prove new or better analytical convergence results for these settings. we showcase the practical efficiency of fess-gda in practical federated learning tasks of training generative adversarial networks (gans) and fair classification.', 'doi': '', 'created': '2023-11-01', 'url': 'https://arxiv.org/abs/2311.00944', 'authors': ['wei shen', 'minhui huang', 'jiawei zhang', 'cong shen']}, {'id': '2311.05883', 'title': 'time-varying identification of monetary policy shocks', 'abstract': \"we propose a new bayesian heteroskedastic markov-switching structural vector autoregression with data-driven time-varying identification. the model selects alternative exclusion restrictions over time and, as a condition for the search, allows to verify identification through heteroskedasticity within each regime. based on four alternative monetary policy rules, we show that a monthly six-variable system supports time variation in us monetary policy shock identification. in the sample-dominating first regime, systematic monetary policy follows a taylor rule extended by the term spread, effectively curbing inflation. in the second regime, occurring after 2000 and gaining more persistence after the global financial and covid crises, it is characterized by a money-augmented taylor rule. this regime's unconventional monetary policy provides economic stimulus, features the liquidity effect, and is complemented by a pure term spread shock. absent the specific monetary policy of the second regime, inflation would be over one percentage point higher on average after 2008.\", 'doi': '', 'created': '2023-11-10', 'url': 'https://arxiv.org/abs/2311.05883', 'authors': ['annika camehl', 'tomasz woźniak']}, {'id': '2311.14653', 'title': 'data-driven prior learning for bayesian optimisation', 'abstract': 'transfer learning for bayesian optimisation has generally assumed a strong similarity between optimisation tasks, with at least a subset having similar optimal inputs. this assumption can reduce computational costs, but it is violated in a wide range of optimisation problems where transfer learning may nonetheless be useful. we replace this assumption with a weaker one only requiring the shape of the optimisation landscape to be similar, and analyse the recent method prior learning for bayesian optimisation - plebo - in this setting. by learning priors for the hyperparameters of the gaussian process surrogate model we can better approximate the underlying function, especially for few function evaluations. we validate the learned priors and compare to a breadth of transfer learning approaches, using synthetic data and a recent air pollution optimisation problem as benchmarks. we show that plebo and prior transfer find good inputs in fewer evaluations.', 'doi': '', 'created': '2023-11-24', 'url': 'https://arxiv.org/abs/2311.14653', 'authors': ['sigrid passano hellan', 'christopher g. lucas', 'nigel h. goddard']}, {'id': '2312.00417', 'title': 'geodesic slice sampling on riemannian manifolds', 'abstract': \"we propose a theoretically justified and practically applicable slice sampling based markov chain monte carlo (mcmc) method for approximate sampling from probability measures on riemannian manifolds. the latter naturally arise as posterior distributions in bayesian inference of matrix-valued parameters, for example belonging to either the stiefel or the grassmann manifold. our method, called geodesic slice sampling, is reversible with respect to the distribution of interest, and generalizes hit-and-run slice sampling on $\\\\mathbb{r}^{d}$ to riemannian manifolds by using geodesics instead of straight lines. we demonstrate the robustness of our sampler's performance compared to other mcmc methods dealing with manifold valued distributions through extensive numerical experiments, on both synthetic and real data. in particular, we illustrate its remarkable ability to cope with anisotropic target densities, without using gradient information and preconditioning.\", 'doi': '', 'created': '2023-12-01', 'url': 'https://arxiv.org/abs/2312.00417', 'authors': ['alain durmus', 'samuel gruffaz', 'mareike hasenpflug', 'daniel rudolf']}, {'id': '2402.10043', 'title': 'negative impact of heavy-tailed uncertainty and error distributions on   the reliability of calibration statistics for machine learning regression   tasks', 'abstract': 'average calibration of the prediction uncertainties of machine learning regression tasks can be tested in two ways: one is to estimate the calibration error (ce) as the difference between the mean absolute error (mse) and the mean variance (mv) or mean squared uncertainty; the alternative is to compare the mean squared z-scores (zms) or scaled errors to 1. the problem is that both approaches might lead to different conclusions, as illustrated in this study for an ensemble of datasets from the recent machine learning uncertainty quantification (ml-uq) literature. it is shown that the estimation of mv, mse and their confidence intervals can become unreliable for heavy-tailed uncertainty and error distributions, which seems to be a common issue for ml-uq datasets. by contrast, the zms statistic is less sensitive and offers the most reliable approach in this context. unfortunately, the same problem affects also conditional calibrations statistics, such as the popular ence, and very likely post-hoc calibration methods based on similar statistics. as not much can be done to relieve this issue, except for a change of paradigm to intervals- or distribution-based uq metrics, robust tailedness metrics are proposed to detect the potentially problematic datasets.', 'doi': '', 'created': '2024-02-15', 'url': 'https://arxiv.org/abs/2402.10043', 'authors': ['pascal pernot']}, {'id': '2402.11394', 'title': 'maximal inequalities for empirical processes under general mixing   conditions with an application to strong approximations', 'abstract': 'this paper provides a bound for the supremum of sample averages over a class of functions for a general class of mixing stochastic processes with arbitrary mixing rates. regardless of the speed of mixing, the bound is comprised of a concentration rate and a novel measure of complexity. the speed of mixing, however, affects the former quantity implying a phase transition. fast mixing leads to the standard root-n concentration rate, while slow mixing leads to a slower concentration rate, its speed depends on the mixing structure. our findings are applied to derive strong approximation results for a general class of mixing processes with arbitrary mixing rates.', 'doi': '', 'created': '2024-02-17', 'url': 'https://arxiv.org/abs/2402.11394', 'authors': ['demian pouzo']}, {'id': '2402.15984', 'title': 'a unified fourier slice method to derive ridgelet transform for a   variety of depth-2 neural networks', 'abstract': 'to investigate neural network parameters, it is easier to study the distribution of parameters than to study the parameters in each neuron. the ridgelet transform is a pseudo-inverse operator that maps a given function $f$ to the parameter distribution $\\\\gamma$ so that a network $\\\\mathtt{nn}[\\\\gamma]$ reproduces $f$, i.e. $\\\\mathtt{nn}[\\\\gamma]=f$. for depth-2 fully-connected networks on a euclidean space, the ridgelet transform has been discovered up to the closed-form expression, thus we could describe how the parameters are distributed. however, for a variety of modern neural network architectures, the closed-form expression has not been known. in this paper, we explain a systematic method using fourier expressions to derive ridgelet transforms for a variety of modern networks such as networks on finite fields $\\\\mathbb{f}_p$, group convolutional networks on abstract hilbert space $\\\\mathcal{h}$, fully-connected networks on noncompact symmetric spaces $g/k$, and pooling layers, or the $d$-plane ridgelet transform.', 'doi': '10.1016/j.jspi.2024.106184', 'created': '2024-02-24', 'url': 'https://arxiv.org/abs/2402.15984', 'authors': ['sho sonoda', 'isao ishikawa', 'masahiro ikeda']}, {'id': '2403.12459', 'title': 'non-negative contrastive learning', 'abstract': \"deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. in this paper, we propose non-negative contrastive learning (ncl), a renaissance of non-negative matrix factorization (nmf) aimed at deriving interpretable features. the power of ncl lies in its enforcement of non-negativity constraints on features, reminiscent of nmf's capability to extract features that align closely with sample clusters. ncl not only aligns mathematically well with an nmf objective but also preserves nmf's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (cl). theoretically, we establish guarantees on the identifiability and downstream generalization of ncl. empirically, we show that these advantages enable ncl to outperform cl significantly on feature disentanglement, feature selection, as well as downstream classification tasks. at last, we show that ncl can be easily extended to other learning scenarios and benefit supervised learning as well. code is available at https://github.com/pku-ml/non_neg.\", 'doi': '', 'created': '2024-03-19', 'url': 'https://arxiv.org/abs/2403.12459', 'authors': ['yifei wang', 'qi zhang', 'yaoyu guo', 'yisen wang']}, {'id': '2404.03701', 'title': 'predictive analytics of varieties of potatoes', 'abstract': 'we explore the application of machine learning algorithms to predict the suitability of russet potato clones for advancement in breeding trials. leveraging data from manually collected trials in the state of oregon, we investigate the potential of a wide variety of state-of-the-art binary classification models. we conduct a comprehensive analysis of the dataset that includes preprocessing, feature engineering, and imputation to address missing values. we focus on several key metrics such as accuracy, f1-score, and matthews correlation coefficient (mcc) for model evaluation. the top-performing models, namely the multi-layer perceptron classifier (mlpc), histogram-based gradient boosting classifier (hgbc), and a support vector machine classifier (svc), demonstrate consistent and significant results. variable selection further enhances model performance and identifies influential features in predicting trial outcomes. the findings emphasize the potential of machine learning in streamlining the selection process for potato varieties, offering benefits such as increased efficiency, substantial cost savings, and judicious resource utilization. our study contributes insights into precision agriculture and showcases the relevance of advanced technologies for informed decision-making in breeding programs.', 'doi': '', 'created': '2024-04-03', 'url': 'https://arxiv.org/abs/2404.03701', 'authors': ['fabiana ferracina', 'bala krishnamoorthy', 'mahantesh halappanavar', 'shengwei hu', 'vidyasagar sathuvalli']}, {'id': '2404.12215', 'title': 'quantifying aleatoric and epistemic uncertainty with proper scoring   rules', 'abstract': 'uncertainty representation and quantification are paramount in machine learning and constitute an important prerequisite for safety-critical applications. in this paper, we propose novel measures for the quantification of aleatoric and epistemic uncertainty based on proper scoring rules, which are loss functions with the meaningful property that they incentivize the learner to predict ground-truth (conditional) probabilities. we assume two common representations of (epistemic) uncertainty, namely, in terms of a credal set, i.e. a set of probability distributions, or a second-order distribution, i.e., a distribution over probability distributions. our framework establishes a natural bridge between these representations. we provide a formal justification of our approach and introduce new measures of epistemic and aleatoric uncertainty as concrete instantiations.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12215', 'authors': ['paul hofman', 'yusuf sale', 'eyke hüllermeier']}, {'id': '2404.12219', 'title': 'a quadrature approach for general-purpose batch bayesian optimization   via probabilistic lifting', 'abstract': 'parallelisation in bayesian optimisation is a common strategy but faces several challenges: the need for flexibility in acquisition functions and kernel choices, flexibility dealing with discrete and continuous variables simultaneously, model misspecification, and lastly fast massive parallelisation. to address these challenges, we introduce a versatile and modular framework for batch bayesian optimisation via probabilistic lifting with kernel quadrature, called sober, which we present as a python library based on gpytorch/botorch. our framework offers the following unique benefits: (1) versatility in downstream tasks under a unified approach. (2) a gradient-free sampler, which does not require the gradient of acquisition functions, offering domain-agnostic sampling (e.g., discrete and mixed variables, non-euclidean space). (3) flexibility in domain prior distribution. (4) adaptive batch size (autonomous determination of the optimal batch size). (5) robustness against a misspecified reproducing kernel hilbert space. (6) natural stopping criterion.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12219', 'authors': ['masaki adachi', 'satoshi hayakawa', 'martin jørgensen', 'saad hamid', 'harald oberhauser', 'michael a. osborne']}, {'id': '2404.12396', 'title': 'optimized dynamic mode decomposition for reconstruction and forecasting   of atmospheric chemistry data', 'abstract': 'we introduce the optimized dynamic mode decomposition algorithm for constructing an adaptive and computationally efficient reduced order model and forecasting tool for global atmospheric chemistry dynamics. by exploiting a low-dimensional set of global spatio-temporal modes, interpretable characterizations of the underlying spatial and temporal scales can be computed. forecasting is also achieved with a linear model that uses a linear superposition of the dominant spatio-temporal features. the dmd method is demonstrated on three months of global chemistry dynamics data, showing its significant performance in computational speed and interpretability. we show that the presented decomposition method successfully extracts known major features of atmospheric chemistry, such as summertime surface pollution and biomass burning activities. moreover, the dmd algorithm allows for rapid reconstruction of the underlying linear model, which can then easily accommodate non-stationary data and changes in the dynamics.', 'doi': '', 'created': '2024-04-13', 'url': 'https://arxiv.org/abs/2404.12396', 'authors': ['meghana velegar', 'christoph keller', 'j. nathan kutz']}, {'id': '2404.12408', 'title': 'benchmarking changepoint detection algorithms on cardiac time series', 'abstract': \"the pattern of state changes in a biomedical time series can be related to health or disease. this work presents a principled approach for selecting a changepoint detection algorithm for a specific task, such as disease classification. eight key algorithms were compared, and the performance of each algorithm was evaluated as a function of temporal tolerance, noise, and abnormal conduction (ectopy) on realistic artificial cardiovascular time series data. all algorithms were applied to real data (cardiac time series of 22 patients with rem-behavior disorder (rbd) and 15 healthy controls) using the parameters selected on artificial data. finally, features were derived from the detected changepoints to classify rbd patients from healthy controls using a k-nearest neighbors approach. on artificial data, modified bayesian changepoint detection algorithm provided superior positive predictive value for state change identification while recursive mean difference maximization (rmdm) achieved the highest true positive rate. for the classification task, features derived from the rmdm algorithm provided the highest leave one out cross validated accuracy of 0.89 and true positive rate of 0.87. automatically detected changepoints provide useful information about subject's physiological state which cannot be directly observed. however, the choice of change point detection algorithm depends on the nature of the underlying data and the downstream application, such as a classification task. this work represents the first time change point detection algorithms have been compared in a meaningful way and utilized in a classification task, which demonstrates the effect of changepoint algorithm choice on application performance.\", 'doi': '', 'created': '2024-04-16', 'url': 'https://arxiv.org/abs/2404.12408', 'authors': ['ayse cakmak', 'erik reinertsen', 'shamim nemati', 'gari d. clifford']}, {'id': '2404.12418', 'title': 'the graph alignment problem: fundamental limits and efficient algorithms', 'abstract': 'this thesis studies the graph alignment problem, the noisy version of the graph isomorphism problem, which aims to find a matching between the nodes of two graphs which preserves most of the edges. focusing on the planted version where the graphs are random, we are interested in understanding the fundamental information-theoretical limits for this problem, as well as designing and analyzing algorithms that are able to recover the underlying alignment in the data. for these algorithms, we give some high probability guarantees on the regime in which they succeed or fail.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12418', 'authors': ['luca ganassali']}, {'id': '2404.12463', 'title': 'spatially selected and dependent random effects for small area   estimation with application to rent burden', 'abstract': 'area-level models for small area estimation typically rely on areal random effects to shrink design-based direct estimates towards a model-based predictor. incorporating the spatial dependence of the random effects into these models can further improve the estimates when there are not enough covariates to fully account for spatial dependence of the areal means. a number of recent works have investigated models that include random effects for only a subset of areas, in order to improve the precision of estimates. however, such models do not readily handle spatial dependence. in this paper, we introduce a model that accounts for spatial dependence in both the random effects as well as the latent process that selects the effects. we show how this model can significantly improve predictive accuracy via an empirical simulation study based on data from the american community survey, and illustrate its properties via an application to estimate county-level median rent burden.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12463', 'authors': ['sho kawano', 'paul a. parker', 'zehang richard li']}, {'id': '2404.12478', 'title': 'a new reliable & parsimonious learning strategy comprising two layers of   gaussian processes, to address inhomogeneous empirical correlation structures', 'abstract': 'we present a new strategy for learning the functional relation between a pair of variables, while addressing inhomogeneities in the correlation structure of the available data, by modelling the sought function as a sample function of a non-stationary gaussian process (gp), that nests within itself multiple other gps, each of which we prove can be stationary, thereby establishing sufficiency of two gp layers. in fact, a non-stationary kernel is envisaged, with each hyperparameter set as dependent on the sample function drawn from the outer non-stationary gp, such that a new sample function is drawn at every pair of input values at which the kernel is computed. however, such a model cannot be implemented, and we substitute this by recalling that the average effect of drawing different sample functions from a given gp is equivalent to that of drawing a sample function from each of a set of gps that are rendered different, as updated during the equilibrium stage of the undertaken inference (via mcmc). the kernel is fully non-parametric, and it suffices to learn one hyperparameter per layer of gp, for each dimension of the input variable. we illustrate this new learning strategy on a real dataset.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12478', 'authors': ['gargi roy', 'dalia chakrabarty']}, {'id': '2404.12481', 'title': 'understanding optimal feature transfer via a fine-grained bias-variance   analysis', 'abstract': 'in the transfer learning paradigm models learn useful representations (or features) during a data-rich pretraining stage, and then use the pretrained representation to improve model performance on data-scarce downstream tasks. in this work, we explore transfer learning with the goal of optimizing downstream performance. we introduce a simple linear model that takes as input an arbitrary pretrained feature transform. we derive exact asymptotics of the downstream risk and its fine-grained bias-variance decomposition. our finding suggests that using the ground-truth featurization can result in \"double-divergence\" of the asymptotic risk, indicating that it is not necessarily optimal for downstream performance. we then identify the optimal pretrained representation by minimizing the asymptotic downstream risk averaged over an ensemble of downstream tasks. our analysis reveals the relative importance of learning the task-relevant features and structures in the data covariates and characterizes how each contributes to controlling the downstream risk from a bias-variance perspective. moreover, we uncover a phase transition phenomenon where the optimal pretrained representation transitions from hard to soft selection of relevant features and discuss its connection to principal component regression.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12481', 'authors': ['yufan li', 'subhabrata sen', 'ben adlam']}, {'id': '2404.12483', 'title': 'a studentized permutation test in group sequential designs', 'abstract': 'in group sequential designs, where several data looks are conducted for early stopping, we generally assume the vector of test statistics from the sequential analyses follows (at least approximately or asymptotially) a multivariate normal distribution. however, it is well-known that test statistics for which an asymptotic distribution is derived may suffer from poor small sample approximation. this might become even worse with an increasing number of data looks. the aim of this paper is to improve the small sample behaviour of group sequential designs while maintaining the same asymptotic properties as classical group sequential designs. this improvement is achieved through the application of a modified permutation test. in particular, this paper shows that the permutation distribution approximates the distribution of the test statistics not only under the null hypothesis but also under the alternative hypothesis, resulting in an asymptotically valid permutation test. an extensive simulation study shows that the proposed permutation test better controls the type i error rate than its competitors in the case of small sample sizes.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12483', 'authors': ['long-hao xu', 'tobias mütze', 'frank konietschke', 'tim friede']}, {'id': '2404.12484', 'title': 'neural methods for amortised parameter inference', 'abstract': 'simulation-based methods for making statistical inference have evolved dramatically over the past 50 years, keeping pace with technological advancements. the field is undergoing a new revolution as it embraces the representational capacity of neural networks, optimisation libraries, and graphics processing units for learning complex mappings between data and inferential targets. the resulting tools are amortised, in the sense that they allow inference to be made quickly through fast feedforward operations. in this article we review recent progress made in the context of point estimation, approximate bayesian inference, the automatic construction of summary statistics, and likelihood approximation. the review also covers available software, and includes a simple illustration to showcase the wide array of tools available for amortised inference and the benefits they offer over state-of-the-art markov chain monte carlo methods. the article concludes with an overview of relevant topics and an outlook on future research directions.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12484', 'authors': ['andrew zammit-mangion', 'matthew sainsbury-dale', 'raphaël huser']}, {'id': '2404.12499', 'title': 'a multivariate copula-based bayesian framework for doping detection', 'abstract': \"doping control is an essential component of anti-doping organizations for protecting clean sports competitions. since 2009, this mission has been complemented worldwide by the athlete biological passport (abp), used to monitor athletes' individual profiles over time. the practical implementation of the abp is based on a bayesian framework, called adaptive, intended to identify individual reference ranges outside of which an observation may indicate doping abuse. currently, this method follows a univariate approach, relying on simultaneous univariate analysis of different markers. this work extends the adaptive method to a multivariate testing framework, making use of copula models to couple the marginal distribution of biomarkers with their dependency structure. after introducing the proposed copula-based hierarchical model, we discuss our approach to inference, grounded in a bayesian spirit, and present an extension to multidimensional predictive reference regions. focusing on the hematological module of the abp, we evaluate the proposed framework in both data-driven simulations and real data.\", 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12499', 'authors': ['nina deliu', 'brunero liseo']}, {'id': '2404.12534', 'title': 'towards large language models as copilots for theorem proving in lean', 'abstract': 'theorem proving is an important challenge for large language models (llms), as formal proofs can be checked rigorously by proof assistants such as lean, leaving no room for hallucination. existing llm-based provers try to prove theorems in a fully autonomous mode without human intervention. in this mode, they struggle with novel and challenging theorems, for which human insights may be critical. in this paper, we explore llms as copilots that assist humans in proving theorems. we introduce lean copilot, a framework for running llm inference in lean. it enables programmers to build various llm-based proof automation tools that integrate seamlessly into the workflow of lean users. using lean copilot, we build tools for suggesting proof steps (tactic suggestion), completing intermediate proof goals (proof search), and selecting relevant premises (premise selection) using llms. users can use our pretrained models or bring their own ones that run either locally (with or without gpus) or on the cloud. experimental results demonstrate the effectiveness of our method in assisting humans and automating theorem proving process compared to existing rule-based proof automation in lean. we open source all codes under a permissive mit license to facilitate further research.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12534', 'authors': ['peiyang song', 'kaiyu yang', 'anima anandkumar']}, {'id': '2404.12544', 'title': 'beyond development: challenges in deploying machine learning models for   structural engineering applications', 'abstract': 'machine learning (ml)-based solutions are rapidly changing the landscape of many fields, including structural engineering. despite their promising performance, these approaches are usually only demonstrated as proof-of-concept in structural engineering, and are rarely deployed for real-world applications. this paper aims to illustrate the challenges of developing ml models suitable for deployment through two illustrative examples. among various pitfalls, the presented discussion focuses on model overfitting and underspecification, training data representativeness, variable omission bias, and cross-validation. the results highlight the importance of implementing rigorous model validation techniques through adaptive sampling, careful physics-informed feature selection, and considerations of both model complexity and generalizability.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12544', 'authors': ['mohsen zaker esteghamati', 'brennan bean', 'henry v. burton', 'm. z. naser']}, {'id': '2404.12553', 'title': \"assessing the longitudinal impact of environmental chemical mixtures on   children's neurodevelopment: a bayesian approach\", 'abstract': \"this manuscript presents a novel bayesian varying coefficient quantile regression (bvcqr) model designed to assess the longitudinal effects of chemical exposure mixtures on children's neurodevelopment. recognizing the complexity and high-dimensionality of environmental exposures, the proposed approach addresses critical gaps in existing research by offering a method that can manage the sparsity of data and provide interpretable results. the proposed bvcqr model estimates the effects of mixtures on neurodevelopmental outcomes at specific ages, leveraging a horseshoe prior for sparsity and utilizing a bayesian method for uncertainty quantification. our simulations demonstrate the model's robustness and effectiveness in handling high-dimensional data, offering significant improvements over traditional models. the model's application to the health outcomes and measures of the environment (home) study further illustrates its utility in identifying significant chemical exposures affecting children's growth and development. the findings underscore the potential of bvcqr in environmental health research, providing a sophisticated tool for analyzing the longitudinal impact of complex chemical mixtures, with implications for future studies aimed at understanding and mitigating environmental risks to child health.\", 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12553', 'authors': ['wei jia', 'roman jandarov']}, {'id': '2404.12556', 'title': 'variance-informed rounding uncertainty analysis for floating-point   statistical models', 'abstract': 'advancements in computer hardware have made it possible to utilize low- and mixed-precision arithmetic for enhanced computational efficiency. in practical predictive modeling, however, it is vital to quantify uncertainty due to rounding along other sources like measurement, sampling, and numerical discretization. traditional deterministic rounding uncertainty analysis (dbea) assumes that the rounding errors equal the unit roundoff $u$. however, despite providing strong guarantees, dbea severely overestimates rounding uncertainty. this work presents a novel probabilistic rounding uncertainty analysis called vibea. by treating rounding errors as i.i.d. random variables and leveraging concentration inequalities, vibea provides high-confidence estimates for rounding uncertainty using higher-order rounding error statistics. the presented framework is valid for all problem sizes $n$, unlike dbea, which necessitates $nu<1$. further, it can account for the potential cancellation of rounding errors, resulting in rounding uncertainty estimates that grow slowly with $n$. we show that for $n>n_c(u)$, vibea produces tighter estimates for rounding uncertainty than dbea. we also show that vibea improves existing probabilistic rounding uncertainty analysis techniques for $n\\\\ge3$ by using higher-order rounding error statistics. we conduct numerical experiments on random vector dot products, a linear system solution, and a stochastic boundary value problem. we show that quantifying rounding uncertainty along with traditional sources (numerical discretization, sampling, parameters) enables a more efficient allocation of computational resources, thereby balancing computational efficiency with predictive accuracy. this study is a step towards a comprehensive mixed-precision approach that improves model reliability and enables budgeting of computational resources in predictive modeling and decision-making.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12556', 'authors': ['sahil bhola', 'karthik duraisamy']}, {'id': '2404.12583', 'title': 'analyzing whale calling through hawkes process modeling', 'abstract': 'sound is assumed to be the primary modality of communication among marine mammal species. analyzing acoustic recordings helps to understand the function of the acoustic signals as well as the possible impact of anthropogenic noise on acoustic behavior. motivated by a dataset from a network of hydrophones in cape cod bay, massachusetts, utilizing automatically detected calls in recordings, we study the communication process of the endangered north atlantic right whale. for right whales an \"up-call\" is known as a contact call, and ensuing counter-calling between individuals is presumed to facilitate group cohesion. we present novel spatiotemporal excitement modeling consisting of a background process and a counter-call process. the background process intensity incorporates the influences of diel patterns and ambient noise on occurrence. the counter-call intensity captures potential excitement, that calling elicits calling behavior. call incidence is found to be clustered in space and time; a call seems to excite more calls nearer to it in time and space. we find evidence that whales make more calls during twilight hours, respond to other whales nearby, and are likely to remain quiet in the presence of increased ambient noise.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12583', 'authors': ['bokgyeong kang', 'erin m. schliep', 'alan e. gelfand', 'tina m. yack', 'christopher w. clark', 'robert s. schick']}, {'id': '2404.12586', 'title': 'risk bounds for mixture density estimation on compact domains via the   $h$-lifted kullback--leibler divergence', 'abstract': 'we consider the problem of estimating probability density functions based on sample data, using a finite mixture of densities from some component class. to this end, we introduce the $h$-lifted kullback--leibler (kl) divergence as a generalization of the standard kl divergence and a criterion for conducting risk minimization. under a compact support assumption, we prove an $\\\\mc{o}(1/{\\\\sqrt{n}})$ bound on the expected estimation error when using the $h$-lifted kl divergence, which extends the results of rakhlin et al. (2005, esaim: probability and statistics, vol. 9) and li and barron (1999, advances in neural information processingsystems, vol. 12) to permit the risk bounding of density functions that are not strictly positive. we develop a procedure for the computation of the corresponding maximum $h$-lifted likelihood estimators ($h$-mlles) using the majorization-maximization framework and provide experimental results in support of our theoretical bounds.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12586', 'authors': ['mark chiu chong', 'hien duy nguyen', 'trungtin nguyen']}, {'id': '2404.12589', 'title': 'a rate-distortion framework for mcmc algorithms: geometry and   factorization of multivariate markov chains', 'abstract': 'we introduce a framework rooted in a rate distortion problem for markov chains, and show how a suite of commonly used markov chain monte carlo (mcmc) algorithms are specific instances within it, where the target stationary distribution is controlled by the distortion function. our approach offers a unified variational view on the optimality of algorithms such as metropolis-hastings, glauber dynamics, the swapping algorithm and feynman-kac path models. along the way, we analyze factorizability and geometry of multivariate markov chains. specifically, we demonstrate that induced chains on factors of a product space can be regarded as information projections with respect to a particular divergence. this perspective yields han--shearer type inequalities for markov chains as well as applications in the context of large deviations and mixing time comparison.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12589', 'authors': ['michael c. h. choi', 'youjia wang', 'geoffrey wolfer']}, {'id': '2404.12592', 'title': 'integer programming for learning directed acyclic graphs from   non-identifiable gaussian models', 'abstract': 'we study the problem of learning directed acyclic graphs from continuous observational data, generated according to a linear gaussian structural equation model. state-of-the-art structure learning methods for this setting have at least one of the following shortcomings: i) they cannot provide optimality guarantees and can suffer from learning sub-optimal models; ii) they rely on the stringent assumption that the noise is homoscedastic, and hence the underlying model is fully identifiable. we overcome these shortcomings and develop a computationally efficient mixed-integer programming framework for learning medium-sized problems that accounts for arbitrary heteroscedastic noise. we present an early stopping criterion under which we can terminate the branch-and-bound procedure to achieve an asymptotically optimal solution and establish the consistency of this approximate solution. in addition, we show via numerical experiments that our method outperforms three state-of-the-art algorithms and is robust to noise heteroscedasticity, whereas the performance of the competing methods deteriorates under strong violations of the identifiability assumption. the software implementation of our method is available as the python package \\\\emph{micodag}.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12592', 'authors': ['tong xu', 'armeen taeb', 'simge küçükyavuz', 'ali shojaie']}, {'id': '2404.12597', 'title': 'the phase diagram of kernel interpolation in large dimensions', 'abstract': \"the generalization ability of kernel interpolation in large dimensions (i.e., $n \\\\asymp d^{\\\\gamma}$ for some $\\\\gamma>0$) might be one of the most interesting problems in the recent renaissance of kernel regression, since it may help us understand the 'benign overfitting phenomenon' reported in the neural networks literature. focusing on the inner product kernel on the sphere, we fully characterized the exact order of both the variance and bias of large-dimensional kernel interpolation under various source conditions $s\\\\geq 0$. consequently, we obtained the $(s,\\\\gamma)$-phase diagram of large-dimensional kernel interpolation, i.e., we determined the regions in $(s,\\\\gamma)$-plane where the kernel interpolation is minimax optimal, sub-optimal and inconsistent.\", 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12597', 'authors': ['haobo zhang', 'weihao lu', 'qian lin']}, {'id': '2404.12610', 'title': 'corporate financial distress prediction: based on multi-source data and   feature selection', 'abstract': \"the advent of the era of big data provides new ideas for financial distress prediction. in order to evaluate the financial status of listed companies more accurately, this study establishes a financial distress prediction indicator system based on multi-source data by integrating three data sources: the company's internal management, the external market and online public opinion. this study addresses the redundancy and dimensional explosion problems of multi-source data integration, feature selection of the fused data, and a financial distress prediction model based on maximum relevance and minimum redundancy and support vector machine recursive feature elimination (mrmr-svm-rfe). to verify the effectiveness of the model, we used back propagation (bp), support vector machine (svm), and gradient boosted decision tree (gbdt) classification algorithms, and conducted an empirical study on china's listed companies based on different financial distress prediction indicator systems. mrmr-svm-rfe feature selection can effectively extract information from multi-source fused data. the new feature dataset obtained by selection has higher prediction accuracy than the original data, and the bp classification model is better than linear regression (lr), decision tree (dt), and random forest (rf).\", 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12610', 'authors': ['yi ding', 'chun yan']}, {'id': '2404.12613', 'title': 'a fourier approach to the parameter estimation problem for   one-dimensional gaussian mixture models', 'abstract': 'the purpose of this paper is twofold. first, we propose a novel algorithm for estimating parameters in one-dimensional gaussian mixture models (gmms). the algorithm takes advantage of the hankel structure inherent in the fourier data obtained from independent and identically distributed (i.i.d) samples of the mixture. for gmms with a unified variance, a singular value ratio functional using the fourier data is introduced and used to resolve the variance and component number simultaneously. the consistency of the estimator is derived. compared to classic algorithms such as the method of moments and the maximum likelihood method, the proposed algorithm does not require prior knowledge of the number of gaussian components or good initial guesses. numerical experiments demonstrate its superior performance in estimation accuracy and computational cost. second, we reveal that there exists a fundamental limit to the problem of estimating the number of gaussian components or model order in the mixture model if the number of i.i.d samples is finite. for the case of a single variance, we show that the model order can be successfully estimated only if the minimum separation distance between the component means exceeds a certain threshold value and can fail if below. we derive a lower bound for this threshold value, referred to as the computational resolution limit, in terms of the number of i.i.d samples, the variance, and the number of gaussian components. numerical experiments confirm this phase transition phenomenon in estimating the model order. moreover, we demonstrate that our algorithm achieves better scores in likelihood, aic, and bic when compared to the em algorithm.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12613', 'authors': ['xinyu liu', 'hai zhang']}, {'id': '2404.12648', 'title': 'sample-efficient learning of infinite-horizon average-reward mdps with   general function approximation', 'abstract': 'we study infinite-horizon average-reward markov decision processes (amdps) in the context of general function approximation. specifically, we propose a novel algorithmic framework named local-fitted optimization with optimism (loop), which incorporates both model-based and value-based incarnations. in particular, loop features a novel construction of confidence sets and a low-switching policy updating scheme, which are tailored to the average-reward and function approximation setting. moreover, for amdps, we propose a novel complexity measure -- average-reward generalized eluder coefficient (agec) -- which captures the challenge of exploration in amdps with general function approximation. such a complexity measure encompasses almost all previously known tractable amdp models, such as linear amdps and linear mixture amdps, and also includes newly identified cases such as kernel amdps and amdps with bellman eluder dimensions. using agec, we prove that loop achieves a sublinear $\\\\tilde{\\\\mathcal{o}}(\\\\mathrm{poly}(d, \\\\mathrm{sp}(v^*)) \\\\sqrt{t\\\\beta} )$ regret, where $d$ and $\\\\beta$ correspond to agec and log-covering number of the hypothesis class respectively, $\\\\mathrm{sp}(v^*)$ is the span of the optimal state bias function, $t$ denotes the number of steps, and $\\\\tilde{\\\\mathcal{o}} (\\\\cdot) $ omits logarithmic factors. when specialized to concrete amdp models, our regret bounds are comparable to those established by the existing algorithms designed specifically for these special cases. to the best of our knowledge, this paper presents the first comprehensive theoretical framework capable of handling nearly all amdps.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12648', 'authors': ['jianliang he', 'han zhong', 'zhuoran yang']}, {'id': '2404.12657', 'title': 'proposer selection in eip-7251', 'abstract': 'immediate settlement, or single-slot finality (ssf), is a long-term goal for ethereum. the growing active validator set size is placing an increasing computational burden on the network, making ssf more challenging. eip-7251 aims to reduce the number of validators by giving stakers the option to merge existing validators. key to the success of this proposal therefore is whether stakers choose to merge their validators once eip-7251 is implemented. it is natural to assume stakers participate only if they anticipate greater expected utility (risk-adjusted returns) as a single large validator. in this paper, we focus on one of the duties that a validator performs, viz. being the proposer for the next block. this duty can be quite lucrative, but happens infrequently. based on previous analysis, we may assume that eip-7251 implies no change to the security of the protocol. we confirm that the probability of a validator being selected as block proposer is equivalent under each consolidation regime. this result ensures that the decision of one staker to merge has no impact on the opportunity of another to propose the next block, in turn ensuring there is no major systemic change to the economics of the protocol with respect to proposer selection.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12657', 'authors': ['sandra johnson', 'kerrie mengersen', \"patrick o'callaghan\", 'anders l. madsen']}, {'id': '2404.12684', 'title': 'estimating weak periodic vector autoregressive time series', 'abstract': 'this article develops the asymptotic distribution of the least squares estimator of the model parameters in periodicvector autoregressive time series models (hereafter pvar) with uncorrelated but dependent innovations. when theinnovations are dependent, this asymptotic distributions can be quite different from that of pvar models with in-dependent and identically distributed (iid for short) innovations developed in ursu and duchesne (2009). modifiedversions of the wald tests are proposed for testing linear restrictions on the parameters. these asymptotic results are illustrated by monte carlo experiments. an application to a bivariate real financial data is also proposed', 'doi': '10.1007/s11749-023-00859-w', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12684', 'authors': ['yacouba boubacar maïnassara', 'eugen ursu']}, {'id': '2404.12685', 'title': 'portmanteau test for a class of multivariate asymmetric power garch   model', 'abstract': 'we establish the asymptotic behaviour of the sum of squared residuals autocovariances and autocorrelations for the class of multi-variate power transformed asymmetric models. we then derive a portmanteau test. we establish the asymptotic distribution of the proposed statistics. these asymptotic results are illustrated by monte carlo experiments. an application to a bivariate real financial data is also proposed.', 'doi': '10.1111/jtsa.12646', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12685', 'authors': ['yacouba boubacar maïnassara', 'othman kadmiri', 'bruno saussereau']}, {'id': '2404.12692', 'title': 'diagnostic checking in multivariate arma models with dependent errors   using normalized residual autocorrelations', 'abstract': 'in this paper we derive the asymptotic distribution of normalized residual empirical autocovariances and autocorrelations under weak assumptions on the noise. we propose new portmanteau statistics for vector autoregressive moving-average (varma) models with uncorrelated but non-independent innovations by using a self-normalization approach. we establish the asymptotic distribution of the proposed statistics. this asymptotic distribution is quite different from the usual chi-squared approximation used under the independent and identically distributed assumption on the noise, or the weighted sum of independent chi-squared random variables obtained under nonindependent innovations. a set of monte carlo experiments and an application to the daily returns of the cac40 is presented.', 'doi': '10.1080/01621459.2017.1380030', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12692', 'authors': ['yacouba boubacar maïnassara', 'bruno saussereau']}, {'id': '2404.12696', 'title': 'gaussian dependence structure pairwise goodness-of-fit testing based on   conditional covariance and the 20/60/20 rule', 'abstract': 'we present a novel data-oriented statistical framework that assesses the presumed gaussian dependence structure in a pairwise setting. this refers to both multivariate normality and normal copula goodness-of-fit testing. the proposed test clusters the data according to the 20/60/20 rule and confronts conditional covariance (or correlation) estimates on the obtained subsets. the corresponding test statistic has a natural practical interpretation, desirable statistical properties, and asymptotic pivotal distribution under the multivariate normality assumption. we illustrate the usefulness of the introduced framework using extensive power simulation studies and show that our approach outperforms popular benchmark alternatives. also, we apply the proposed methodology to commodities market data.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12696', 'authors': ['jakub woźny', 'piotr jaworski', 'damian jelitoa', 'marcin pitera', 'agnieszka wyłomańska']}, {'id': '2404.12756', 'title': 'why not a thin plate spline for spatial models? a comparative study   using bayesian inference', 'abstract': 'spatial modelling often uses gaussian random fields to capture the stochastic nature of studied phenomena. however, this approach incurs significant computational burdens (o(n3)), primarily due to covariance matrix computations. in this study, we propose to use a low-rank approximation of a thin plate spline as a spatial random effect in bayesian spatial models. we compare its statistical performance and computational efficiency with the approximated gaussian random field (by the spde method). in this case, the dense matrix of the thin plate spline is approximated using a truncated spectral decomposition, resulting in computational complexity of o(kn2) operations, where k is the number of knots. bayesian inference is conducted via the hamiltonian monte carlo algorithm of the probabilistic software stan, which allows us to evaluate performance and diagnostics for the proposed models. a simulation study reveals that both models accurately recover the parameters used to simulate data. however, models using a thin plate spline demonstrate superior execution time to achieve the convergence of chains compared to the models utilizing an approximated gaussian random field. furthermore, thin plate spline models exhibited better computational efficiency for simulated data coming from different spatial locations. in a real application, models using a thin plate spline as spatial random effect produced similar results in estimating a relative index of abundance for a benthic marine species when compared to models incorporating an approximated gaussian random field. although they were not the more computational efficient models, their simplicity in parametrization, execution time and predictive performance make them a valid alternative for spatial modelling under bayesian inference.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12756', 'authors': ['joaquin cavieres', 'paula moraga', 'cole c. monnahan']}, {'id': '2404.12812', 'title': 'algorithmic changes are not enough: evaluating the removal of race   adjustment from the egfr equation', 'abstract': 'changing clinical algorithms to remove race adjustment has been proposed and implemented for multiple health conditions. removing race adjustment from estimated glomerular filtration rate (egfr) equations may reduce disparities in chronic kidney disease (ckd), but has not been studied in clinical practice after implementation. here, we assessed whether implementing an egfr equation (ckd-epi 2021) without adjustment for black or african american race modified quarterly rates of nephrology referrals and visits within a single healthcare system, stanford health care (shc). our cohort study analyzed 547,194 adult patients aged 21 and older who had at least one recorded serum creatinine or serum cystatin c between january 1, 2019 and september 1, 2023. during the study period, implementation of ckd-epi 2021 did not modify rates of quarterly nephrology referrals in those documented as black or african american or in the overall cohort. after adjusting for capacity at shc nephrology clinics, estimated rates of nephrology referrals and visits with ckd-epi 2021 were 34 (95% ci 29, 39) and 188 (175, 201) per 10,000 patients documented as black or african american. if race adjustment had not been removed, estimated rates were nearly identical: 38 (95% ci: 28, 53) and 189 (165, 218) per 10,000 patients. changes to the egfr equation are likely insufficient to achieve health equity in ckd care decision-making as many other structural inequities remain.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12812', 'authors': ['marika m. cusick', 'glenn m. chertow', 'douglas k. owens', 'michelle y. williams', 'sherri rose']}, {'id': '2404.12828', 'title': 'low solution rank of the matrix lasso under rip with consequences for   rank-constrained algorithms', 'abstract': 'we show that solutions to the popular convex matrix lasso problem (nuclear-norm--penalized linear least-squares) have low rank under similar assumptions as required by classical low-rank matrix sensing error bounds. although the purpose of the nuclear norm penalty is to promote low solution rank, a proof has not yet (to our knowledge) been provided outside very specific circumstances. furthermore, we show that this result has significant theoretical consequences for nonconvex rank-constrained optimization approaches. specifically, we show that if (a) the ground truth matrix has low rank, (b) the (linear) measurement operator has the matrix restricted isometry property (rip), and (c) the measurement error is small enough relative to the nuclear norm penalty, then the (unique) lasso solution has rank (approximately) bounded by that of the ground truth. from this, we show (a) that a low-rank--projected proximal gradient descent algorithm will converge linearly to the lasso solution from any initialization, and (b) that the nonconvex landscape of the low-rank burer-monteiro--factored problem formulation is benign in the sense that all second-order critical points are globally optimal and yield the lasso solution.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12828', 'authors': ['andrew d. mcrae']}, {'id': '2404.12862', 'title': 'a guide to feature importance methods for scientific inference', 'abstract': 'while machine learning (ml) models are increasingly used due to their high predictive power, their use in understanding the data-generating process (dgp) is limited. understanding the dgp requires insights into feature-target associations, which many ml models cannot directly provide, due to their opaque internal mechanisms. feature importance (fi) methods provide useful insights into the dgp under certain conditions. since the results of different fi methods have different interpretations, selecting the correct fi method for a concrete use case is crucial and still requires expert knowledge. this paper serves as a comprehensive guide to help understand the different interpretations of fi methods. through an extensive review of fi methods and providing new proofs regarding their interpretation, we facilitate a thorough understanding of these methods and formulate concrete recommendations for scientific inference. we conclude by discussing options for fi uncertainty estimation and point to directions for future research aiming at full statistical inference from black-box ml models.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12862', 'authors': ['fiona katharina ewald', 'ludwig bothmann', 'marvin n. wright', 'bernd bischl', 'giuseppe casalicchio', 'gunnar könig']}, {'id': '2404.12889', 'title': 'on the probability of linear separability through intrinsic volumes', 'abstract': \"a dataset with two labels is linearly separable if it can be split into its two classes with a hyperplane. this inflicts a curse on some statistical tools (such as logistic regression) but forms a blessing for others (e.g. support vector machines). recently, the following question has regained interest: what is the probability that the data are linearly separable?   we provide a formula for the probability of linear separability for gaussian features and labels depending only on one marginal of the features (as in generalized linear models). in this setting, we derive an upper bound that complements the recent result by hayakawa, lyons, and oberhauser [2023], and a sharp upper bound for sign-flip noise.   to prove our results, we exploit that this probability can be expressed as a sum of the intrinsic volumes of a polyhedral cone of the form $\\\\text{span}\\\\{v\\\\}\\\\oplus[0,\\\\infty)^n$, as shown in cand\\\\`es and sur [2020]. after providing the inequality description for this cone, and an algorithm to project onto it, we calculate its intrinsic volumes. in doing so, we encounter youden's demon problem, for which we provide a formula following kabluchko and zaporozhets [2020]. the key insight of this work is the following: the number of correctly labeled observations in the data affects the structure of this polyhedral cone, allowing the translation of insights from geometry into statistics.\", 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12889', 'authors': ['felix kuchelmeister']}, {'id': '2404.12923', 'title': 'probabilistic-numeric smc sampling for bayesian nonlinear system   identification in continuous time', 'abstract': \"in engineering, accurately modeling nonlinear dynamic systems from data contaminated by noise is both essential and complex. established sequential monte carlo (smc) methods, used for the bayesian identification of these systems, facilitate the quantification of uncertainty in the parameter identification process. a significant challenge in this context is the numerical integration of continuous-time ordinary differential equations (odes), crucial for aligning theoretical models with discretely sampled data. this integration introduces additional numerical uncertainty, a factor that is often over looked. to address this issue, the field of probabilistic numerics combines numerical methods, such as numerical integration, with probabilistic modeling to offer a more comprehensive analysis of total uncertainty. by retaining the accuracy of classical deterministic methods, these probabilistic approaches offer a deeper understanding of the uncertainty inherent in the inference process. this paper demonstrates the application of a probabilistic numerical method for solving odes in the joint parameter-state identification of nonlinear dynamic systems. the presented approach efficiently identifies latent states and system parameters from noisy measurements. simultaneously incorporating probabilistic solutions to the ode in the identification challenge. the methodology's primary advantage lies in its capability to produce posterior distributions over system parameters, thereby representing the inherent uncertainties in both the data and the identification process.\", 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12923', 'authors': ['joe d. longbottom', 'max d. champneys', 'timothy j. rogers']}, {'id': '2404.12940', 'title': 'neural flow diffusion models: learnable forward process for improved   diffusion modelling', 'abstract': \"conventional diffusion models typically relies on a fixed forward process, which implicitly defines complex marginal distributions over latent variables. this can often complicate the reverse process' task in learning generative trajectories, and results in costly inference for diffusion models. to address these limitations, we introduce neural flow diffusion models (nfdm), a novel framework that enhances diffusion models by supporting a broader range of forward processes beyond the fixed linear gaussian. we also propose a novel parameterization technique for learning the forward process. our framework provides an end-to-end, simulation-free optimization objective, effectively minimizing a variational upper bound on the negative log-likelihood. experimental results demonstrate nfdm's strong performance, evidenced by state-of-the-art likelihood estimation. furthermore, we investigate nfdm's capacity for learning generative dynamics with specific characteristics, such as deterministic straight lines trajectories. this exploration underscores nfdm's versatility and its potential for a wide range of applications.\", 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12940', 'authors': ['grigory bartosh', 'dmitry vetrov', 'christian a. naesseth']}, {'id': '2404.12943', 'title': 'symmetry: a general structure in nonparametric regression', 'abstract': 'in this paper we present the framework of symmetry in nonparametric regression. this generalises the framework of covariate sparsity, where the regression function depends only on at most $s < d$ of the covariates, which is a special case of translation symmetry with linear orbits. in general this extends to other types of functions that capture lower dimensional behavior even when these structures are non-linear. we show both that known symmetries of regression functions can be exploited to give similarly faster rates, and that unknown symmetries with lipschitz actions can be estimated sufficiently quickly to obtain the same rates. this is done by explicit constructions of partial symmetrisation operators that are then applied to usual estimators, and with a two step m-estimator of the maximal symmetry of the regression function. we also demonstrate the finite sample performance of these estimators on synthetic data.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12943', 'authors': ['louis g. christie', 'john a. d. aston']}, {'id': '2404.12949', 'title': 'optimal single threshold stopping rules and sharp prophet inequalities', 'abstract': 'this paper considers a finite horizon optimal stopping problem for a sequence of independent and identically distributed random variables. the objective is to design stopping rules that attempt to select the random variable with the highest value in the sequence. the performance of any stopping rule may be benchmarked relative to the selection of a \"prophet\" that has perfect foreknowledge of the largest value. such comparisons are typically stated in the form of \"prophet inequalities.\" in this paper we characterize sharp prophet inequalities for single threshold stopping rules as solutions to infinite two person zero sum games on the unit square with special payoff kernels. the proposed game theoretic characterization allows one to derive sharp non-asymptotic prophet inequalities for different classes of distributions. this, in turn, gives rise to a simple and computationally tractable algorithmic paradigm for deriving optimal single threshold stopping rules. our results also indicate that several classical observations in the literature are either incorrect or incomplete in treating this problem.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12949', 'authors': ['alexander goldenshluger', 'yaakov malinovsky', 'assaf zeevi']}, {'id': '2404.12967', 'title': 'bootstrap confidence intervals: a comparative simulation study', 'abstract': 'bootstrap is a widely used technique that allows estimating the properties of a given estimator, such as its bias and standard error. in this paper, we evaluate and compare five bootstrap-based methods for making confidence intervals: two of them (normal and studentized) based on the bootstrap estimate of the standard error; another two (quantile and better) based on the estimated distribution of the parameter estimator; and finally, considering an interval constructed based on bayesian bootstrap, relying on the notion of credible interval. the methods are compared through monte carlo simulations in different scenarios, including samples with autocorrelation induced by a copula model. the results are also compared with respect to the coverage rate, the median interval length and a novel indicator, proposed in this paper, combining both of them. the results show that the studentized method has the best coverage rate, although the smallest intervals are attained by the bayesian method. in general, all methods are appropriate and demonstrated good performance even in the scenarios violating the independence assumption.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12967', 'authors': ['vinícius litvinoff justus', 'vitor batista rodrigues', 'alex rodrigo dos santos sousa']}, {'id': '2404.12968', 'title': 'scalable data assimilation with message passing', 'abstract': 'data assimilation is a core component of numerical weather prediction systems. the large quantity of data processed during assimilation requires the computation to be distributed across increasingly many compute nodes, yet existing approaches suffer from synchronisation overhead in this setting. in this paper, we exploit the formulation of data assimilation as a bayesian inference problem and apply a message-passing algorithm to solve the spatial inference problem. since message passing is inherently based on local computations, this approach lends itself to parallel and distributed computation. in combination with a gpu-accelerated implementation, we can scale the algorithm to very large grid sizes while retaining good accuracy and compute and memory requirements.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12968', 'authors': ['oscar key', 'so takao', 'daniel giles', 'marc peter deisenroth']}, {'id': '2404.13016', 'title': 'optimizing calibration by gaining aware of prediction correctness', 'abstract': 'model calibration aims to align confidence with prediction correctness. the cross-entropy ce) loss is widely used for calibrator training, which enforces the model to increase confidence on the ground truth class. however, we find the ce loss has intrinsic limitations. for example, for a narrow misclassification, a calibrator trained by the ce loss often produces high confidence on the wrongly predicted class (e.g., a test sample is wrongly classified and its softmax score on the ground truth class is around 0.4), which is undesirable. in this paper, we propose a new post-hoc calibration objective derived from the aim of calibration. intuitively, the proposed objective function asks that the calibrator decrease model confidence on wrongly predicted samples and increase confidence on correctly predicted samples. because a sample itself has insufficient ability to indicate correctness, we use its transformed versions (e.g., rotated, greyscaled and color-jittered) during calibrator training. trained on an in-distribution validation set and tested with isolated, individual test samples, our method achieves competitive calibration performance on both in-distribution and out-of-distribution test sets compared with the state of the art. further, our analysis points out the difference between our method and commonly used objectives such as ce loss and mean square error loss, where the latters sometimes deviates from the calibration aim.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.13016', 'authors': ['yuchi liu', 'lei wang', 'yuli zou', 'james zou', 'liang zheng']}]\n",
      "Data type: <class 'list'>\n",
      "Data loaded: [{'id': '1906.02358', 'title': 'survey on publicly available sinhala natural language processing tools   and research', 'abstract': 'sinhala is the native language of the sinhalese people who make up the largest ethnic group of sri lanka. the language belongs to the globe-spanning language tree, indo-european. however, due to poverty in both linguistic and economic capital, sinhala, in the perspective of natural language processing tools and research, remains a resource-poor language which has neither the economic drive its cousin english has nor the sheer push of the law of numbers a language such as chinese has. a number of research groups from sri lanka have noticed this dearth and the resultant dire need for proper tools and research for sinhala natural language processing. however, due to various reasons, these attempts seem to lack coordination and awareness of each other. the objective of this paper is to fill that gap of a comprehensive literature survey of the publicly available sinhala natural language tools and research so that the researchers working in this field can better utilize contributions of their peers. as such, we shall be uploading this paper to arxiv and perpetually update it periodically to reflect the advances made in the field.', 'doi': '', 'created': '2019-06-05', 'url': 'https://arxiv.org/abs/1906.02358', 'authors': ['nisansa de silva']}, {'id': '1907.05325', 'title': 'low-rank matrix completion and denoising under poisson noise', 'abstract': 'this paper considers the problem of estimating a low-rank matrix from the observation of all or a subset of its entries in the presence of poisson noise. when we observe all entries, this is a problem of matrix denoising; when we observe only a subset of the entries, this is a problem of matrix completion. in both cases, we exploit an assumption that the underlying matrix is low-rank. specifically, we analyze several estimators, including a constrained nuclear-norm minimization program, nuclear-norm regularized least squares, and a nonconvex constrained low-rank optimization problem. we show that for all three estimators, with high probability, we have an upper error bound (in the frobenius norm error metric) that depends on the matrix rank, the fraction of the elements observed, and maximal row and column sums of the true matrix. we furthermore show that the above results are minimax optimal (within a universal constant) in classes of matrices with low rank and bounded row and column sums. we also extend these results to handle the case of matrix multinomial denoising and completion.', 'doi': '10.1093/imaiai/iaaa020', 'created': '2019-07-11', 'url': 'https://arxiv.org/abs/1907.05325', 'authors': ['andrew d. mcrae', 'mark a. davenport']}, {'id': '1912.10642', 'title': 'notes on category theory with examples from basic mathematics', 'abstract': 'these notes were originally developed as lecture notes for a category theory course. they should be well-suited to anyone that wants to learn category theory from scratch and has a scientific mind. there is no need to know advanced mathematics, nor any of the disciplines where category theory is traditionally applied, such as algebraic geometry or theoretical computer science. the only knowledge that is assumed from the reader is linear algebra. all concepts are explained by giving concrete examples from different, non-specialized areas of mathematics (such as basic group theory, graph theory, and probability). not every example is helpful for every reader, but hopefully every reader can find at least one helpful example per concept. the reader is encouraged to read all the examples, this way they may even learn something new about a different field.   particular emphasis is given to the yoneda lemma and its significance, with both intuitive explanations, detailed proofs, and specific examples. another common theme in these notes is the relationship between categories and directed multigraphs, which is treated in detail. from the applied point of view, this shows why categorical thinking can help whenever some process is taking place on a graph. from the pure math point of view, this can be seen as the 1-dimensional first step into the theory of simplicial sets. finally, monads and comonads are treated on an equal footing, differently to most literature in which comonads are often overlooked as \"just the dual to monads\". theorems, interpretations and concrete examples are given for monads as well as for comonads.   this work, thoroughly revised and expanded, is now a book, with an extra section on monoidal categories.', 'doi': '10.1142/13670', 'created': '2019-12-23', 'url': 'https://arxiv.org/abs/1912.10642', 'authors': ['paolo perrone']}, {'id': '2101.07223', 'title': 'leveraging ai to optimize website structure discovery during penetration   testing', 'abstract': 'dirbusting is a technique used to brute force directories and file names on web servers while monitoring http responses, in order to enumerate server contents. such a technique uses lists of common words to discover the hidden structure of the target website. dirbusting typically relies on response codes as discovery conditions to find new pages. it is widely used in web application penetration testing, an activity that allows companies to detect websites vulnerabilities. dirbusting techniques are both time and resource consuming and innovative approaches have never been explored in this field. we hence propose an advanced technique to optimize the dirbusting process by leveraging artificial intelligence. more specifically, we use semantic clustering techniques in order to organize wordlist items in different groups according to their semantic meaning. the created clusters are used in an ad-hoc implemented next-word intelligent strategy. this paper demonstrates that the usage of clustering techniques outperforms the commonly used brute force methods. performance is evaluated by testing eight different web applications. results show a performance increase that is up to 50% for each of the conducted experiments.', 'doi': '', 'created': '2021-01-18', 'url': 'https://arxiv.org/abs/2101.07223', 'authors': ['diego antonelli', 'roberta cascella', 'gaetano perrone', 'simon pietro romano', 'antonio schiano']}, {'id': '2101.09180', 'title': \"a newton's iteration converges quadratically to nonisolated solutions   too\", 'abstract': \"the textbook newton's iteration is practically inapplicable on solutions of nonlinear systems with singular jacobians. by a simple modification, a novel extension of newton's iteration regains its local quadratic convergence toward nonisolated solutions that are semiregular as properly defined regardless of whether the system is square, underdetermined or overdetermined while jacobians can be rank-deficient. furthermore, the iteration serves as a regularization mechanism for computing singular solutions from empirical data. when a system is perturbed, its nonisolated solutions can be altered substantially or even disappear. the iteration still locally converges to a stationary point that approximates a singular solution of the underlying system with an error bound in the same order of the data accuracy. geometrically, the iteration approximately approaches the nearest point on the solution manifold. the method simplifies the modeling of nonlinear systems by permitting nonisolated solutions and enables a wide range of applications in algebraic computation.\", 'doi': '10.1190/mcom/3657', 'created': '2021-01-22', 'url': 'https://arxiv.org/abs/2101.09180', 'authors': ['zhonggang zeng']}, {'id': '2105.05716', 'title': 'acting upon imagination: when to trust imagined trajectories in model   based reinforcement learning', 'abstract': 'model-based reinforcement learning (mbrl) aims to learn model(s) of the environment dynamics that can predict the outcome of its actions. forward application of the model yields so called imagined trajectories (sequences of action, predicted state-reward) used to optimize the set of candidate actions that maximize expected reward. the outcome, an ideal imagined trajectory or plan, is imperfect and typically mbrl relies on model predictive control (mpc) to overcome this by continuously re-planning from scratch, incurring thus major computational cost and increasing complexity in tasks with longer receding horizon. we propose uncertainty estimation methods for online evaluation of imagined trajectories to assess whether further planned actions can be trusted to deliver acceptable reward. these methods include comparing the error after performing the last action with the standard expected error and using model uncertainty to assess the deviation from expected outcomes. additionally, we introduce methods that exploit the forward propagation of the dynamics model to evaluate if the remainder of the plan aligns with expected results and assess the remainder of the plan in terms of the expected reward. our experiments demonstrate the effectiveness of the proposed uncertainty estimation methods by applying them to avoid unnecessary trajectory replanning in a shooting mbrl setting. results highlight significant reduction on computational costs without sacrificing performance.', 'doi': '', 'created': '2021-05-12', 'url': 'https://arxiv.org/abs/2105.05716', 'authors': ['adrian remonda', 'eduardo veas', 'granit luzhnica']}, {'id': '2111.01993', 'title': \"estimaci\\\\'on y an\\\\'alisis de sensibilidad para el coeficiente de   difusividad en un problema de conducci\\\\'on de calor\", 'abstract': 'the aim of this article is to discuss the estimation of the diffusivity coefficient of a homogeneous metal rod from temperature values at a fixed point in the bar for different time instants. the time-dependent problem of heat conduction is analyzed in an insulated conductor wire of length l considering constant boundary conditions. the problem is modeled by a parabolic partial differential equation, imposing dirichlet boundary conditions. we consider simulated temperature values at a point of the bar for different time instants and estimate the coefficient of diffusivity using usual techniques for solving inverse problems. for the discretization of the equation we consider a finite difference centered scheme. we include an analytical and numerical study of the sensitivity of the temperature function with respect to the coefficient of diffusivity. numerical experiments show very good accuracy in the estimates.', 'doi': '10.54789/rince.12.3', 'created': '2021-11-02', 'url': 'https://arxiv.org/abs/2111.01993', 'authors': ['guillermo federico umbricht', 'diana rubio']}, {'id': '2202.07595', 'title': 'bayesian optimisation for active monitoring of air pollution', 'abstract': 'air pollution is one of the leading causes of mortality globally, resulting in millions of deaths each year. efficient monitoring is important to measure exposure and enforce legal limits. new low-cost sensors can be deployed in greater numbers and in more varied locations, motivating the problem of efficient automated placement. previous work suggests bayesian optimisation is an appropriate method, but only considered a satellite data set, with data aggregated over all altitudes. it is ground-level pollution, that humans breathe, which matters most. we improve on those results using hierarchical models and evaluate our models on urban pollution data in london to show that bayesian optimisation can be successfully applied to the problem.', 'doi': '10.1609/aaai.v36i11.21448', 'created': '2022-02-15', 'url': 'https://arxiv.org/abs/2202.07595', 'authors': ['sigrid passano hellan', 'christopher g. lucas', 'nigel h. goddard']}, {'id': '2203.07976', 'title': 'on the pitfalls of batch normalization for end-to-end video learning: a   study on surgical workflow analysis', 'abstract': \"batch normalization's (bn) unique property of depending on other samples in a batch is known to cause problems in several tasks, including sequence modeling. yet, bn-related issues are hardly studied for long video understanding, despite the ubiquitous use of bn in cnns (convolutional neural networks) for feature extraction. especially in surgical workflow analysis, where the lack of pretrained feature extractors has led to complex, multi-stage training pipelines, limited awareness of bn issues may have hidden the benefits of training cnns and temporal models end to end. in this paper, we analyze pitfalls of bn in video learning, including issues specific to online tasks such as a 'cheating' effect in anticipation. we observe that bn's properties create major obstacles for end-to-end learning. however, using bn-free backbones, even simple cnn-lstms beat the state of the art {\\\\color{\\\\colorrevtwo}on three surgical workflow benchmarks} by utilizing adequate end-to-end training strategies which maximize temporal context. we conclude that awareness of bn's pitfalls is crucial for effective end-to-end learning in surgical tasks. by reproducing results on natural-video datasets, we hope our insights will benefit other areas of video learning as well. code is available at: \\\\url{https://gitlab.com/nct_tso_public/pitfalls_bn}\", 'doi': '10.1016/j.media.2024.103126', 'created': '2022-03-15', 'url': 'https://arxiv.org/abs/2203.07976', 'authors': ['dominik rivoir', 'isabel funke', 'stefanie speidel']}, {'id': '2203.11593', 'title': 'unified negative pair generation toward well-discriminative feature   space for face recognition', 'abstract': 'the goal of face recognition (fr) can be viewed as a pair similarity optimization problem, maximizing a similarity set $\\\\mathcal{s}^p$ over positive pairs, while minimizing similarity set $\\\\mathcal{s}^n$ over negative pairs. ideally, it is expected that fr models form a well-discriminative feature space (wdfs) that satisfies $\\\\inf{\\\\mathcal{s}^p} > \\\\sup{\\\\mathcal{s}^n}$. with regard to wdfs, the existing deep feature learning paradigms (i.e., metric and classification losses) can be expressed as a unified perspective on different pair generation (pg) strategies. unfortunately, in the metric loss (ml), it is infeasible to generate negative pairs taking all classes into account in each iteration because of the limited mini-batch size. in contrast, in classification loss (cl), it is difficult to generate extremely hard negative pairs owing to the convergence of the class weight vectors to their center. this leads to a mismatch between the two similarity distributions of the sampled pairs and all negative pairs. thus, this paper proposes a unified negative pair generation (unpg) by combining two pg strategies (i.e., mlpg and clpg) from a unified perspective to alleviate the mismatch. unpg introduces useful information about negative pairs using mlpg to overcome the clpg deficiency. moreover, it includes filtering the similarities of noisy negative pairs to guarantee reliable convergence and improved performance. exhaustive experiments show the superiority of unpg by achieving state-of-the-art performance across recent loss functions on public benchmark datasets. our code and pretrained models are publicly available.', 'doi': '', 'created': '2022-03-22', 'url': 'https://arxiv.org/abs/2203.11593', 'authors': ['junuk jung', 'seonhoon lee', 'heung-seon oh', 'yongjun park', 'joochan park', 'sungbin son']}, {'id': '2204.10325', 'title': 'ai-based automated speech therapy tools for persons with speech sound   disorders: a systematic literature review', 'abstract': 'this paper presents a systematic literature review of published studies on ai-based automated speech therapy tools for persons with speech sound disorders (ssd). the covid-19 pandemic has initiated the requirement for automated speech therapy tools for persons with ssd making speech therapy accessible and affordable. however, there are no guidelines for designing such automated tools and their required degree of automation compared to human experts. in this systematic review, we followed the prisma framework to address four research questions: 1) what types of ssd do ai-based automated speech therapy tools address, 2) what is the level of autonomy achieved by such tools, 3) what are the different modes of intervention, and 4) how effective are such tools in comparison with human experts. an extensive search was conducted on digital libraries to find research papers relevant to our study from 2007 to 2022. the results show that ai-based automated speech therapy tools for persons with ssd are increasingly gaining attention among researchers. articulation disorders were the most frequently addressed ssd based on the reviewed papers. further, our analysis shows that most researchers proposed fully automated tools without considering the role of other stakeholders. our review indicates that mobile-based and gamified applications were the most frequent mode of intervention. the results further show that only a few studies compared the effectiveness of such tools compared to expert speech-language pathologists (slp). our paper presents the state-of-the-art in the field, contributes significant insights based on the research questions, and provides suggestions for future research directions.', 'doi': '10.21203/rs.3.rs-1517404/v1', 'created': '2022-04-21', 'url': 'https://arxiv.org/abs/2204.10325', 'authors': ['chinmoy deka', 'abhishek shrivastava', 'ajish k. abraham', 'saurabh nautiyal', 'praveen chauhan']}, {'id': '2205.01438', 'title': 'fedgia: an efficient hybrid algorithm for federated learning', 'abstract': 'federated learning has shown its advances recently but is still facing many challenges, such as how algorithms save communication resources and reduce computational costs, and whether they converge. to address these critical issues, we propose a hybrid federated learning algorithm (fedgia) that combines the gradient descent and the inexact alternating direction method of multipliers. the proposed algorithm is more communication- and computation-efficient than several state-of-the-art algorithms theoretically and numerically. moreover, it also converges globally under mild conditions.', 'doi': '10.1109/tsp.2023.3268845', 'created': '2022-05-03', 'url': 'https://arxiv.org/abs/2205.01438', 'authors': ['shenglong zhou', 'geoffrey ye li']}, {'id': '2205.02533', 'title': 'near-field wideband extremely large-scale mimo transmission with   holographic metasurface antennas', 'abstract': 'extremely large-scale multiple-input multiple-output (xl-mimo) is the development trend of future wireless communications. however, the extremely large-scale antenna array could bring inevitable nearfield and dual-wideband effects that seriously reduce the transmission performance. this paper proposes an algorithmic framework to design the beam combining for the near-field wideband xl-mimo uplink transmissions assisted by holographic metasurface antennas (hmas). firstly, we introduce a spherical-wave-based channel model that simultaneously takes into account both the near-field and dual-wideband effects. based on such a model, we then formulate the hma-based beam combining problem for the proposed xl-mimo communications, which is challenging due to the nonlinear coupling of high dimensional hma weights and baseband combiners. we further present a sum-mean-square-error-minimization-based algorithmic framework. numerical results showcase that the proposed scheme can effectively alleviate the sum-rate loss caused by the near-field and dual-wideband effects in hma-assisted xl-mimo systems. meanwhile, the proposed hma-based scheme can achieve a higher sum rate than the conventional phase-shifter-based hybrid analog/digital one with the same array aperture.', 'doi': '10.1109/twc.2024.3387709', 'created': '2022-05-05', 'url': 'https://arxiv.org/abs/2205.02533', 'authors': ['jie xu', 'li you', 'george c. alexandropoulos', 'xinping yi', 'wenjin wang', 'xiqi gao']}, {'id': '2205.14223', 'title': 'a short note on inf-sup conditions for the taylor-hood family   $q_k$-$q_{k-1}$', 'abstract': 'we discuss two types of discrete inf-sup conditions for the taylor-hood family $q_k$-$q_{k-1}$ for all $k\\\\in \\\\mathbb{n}$ with $k\\\\ge 2$ in 2d and 3d. while in 2d all results hold for a general class of hexahedral meshes, the results in 3d are restricted to meshes of parallelepipeds. the analysis is based on an element-wise technique as opposed to the widely used macroelement technique. this leads to inf-sup conditions on each element of the subdivision as well as to inf-sup conditions on the whole computational domain.', 'doi': '', 'created': '2022-05-27', 'url': 'https://arxiv.org/abs/2205.14223', 'authors': ['walter zulehner']}, {'id': '2206.09325', 'title': 'eatformer: improving vision transformer inspired by evolutionary   algorithm', 'abstract': 'motivated by biological evolution, this paper explains the rationality of vision transformer by analogy with the proven practical evolutionary algorithm (ea) and derives that both have consistent mathematical formulation. then inspired by effective ea variants, we propose a novel pyramid eatformer backbone that only contains the proposed \\\\emph{ea-based transformer} (eat) block, which consists of three residual parts, i.e., \\\\emph{multi-scale region aggregation} (msra), \\\\emph{global and local interaction} (gli), and \\\\emph{feed-forward network} (ffn) modules, to model multi-scale, interactive, and individual information separately. moreover, we design a \\\\emph{task-related head} (trh) docked with transformer backbone to complete final information fusion more flexibly and \\\\emph{improve} a \\\\emph{modulated deformable msa} (md-msa) to dynamically model irregular locations. massive quantitative and quantitative experiments on image classification, downstream tasks, and explanatory experiments demonstrate the effectiveness and superiority of our approach over state-of-the-art (sota) methods. \\\\eg, our mobile (1.8m), tiny (6.1m), small (24.3m), and base (49.0m) models achieve 69.4, 78.4, 83.1, and 83.9 top-1 only trained on imagenet-1k with naive training recipe; eatformer-tiny/small/base armed mask-r-cnn obtain 45.4/47.4/49.0 box ap and 41.4/42.9/44.2 mask ap on coco detection, surpassing contemporary mpvit-t, swin-t, and swin-s by 0.6/1.4/0.5 box ap and 0.4/1.3/0.9 mask ap separately with less flops; our eatformer-small/base achieve 47.3/49.3 miou on ade20k by upernet that exceeds swin-t/s by 2.8/1.7. code is available at \\\\url{https://github.com/zhangzjn/eatformer}.', 'doi': '', 'created': '2022-06-19', 'url': 'https://arxiv.org/abs/2206.09325', 'authors': ['jiangning zhang', 'xiangtai li', 'yabiao wang', 'chengjie wang', 'yibo yang', 'yong liu', 'dacheng tao']}, {'id': '2206.15101', 'title': 'the maximum capability of a topological feature in link prediction', 'abstract': \"networks offer a powerful approach to modeling complex systems by representing the underlying set of pairwise interactions. link prediction is the task that predicts links of a network that are not directly visible, with profound applications in biological, social, and other complex systems. despite intensive utilization of the topological feature in this task, it is unclear to what extent a feature can be leveraged to infer missing links. here, we aim to unveil the capability of a topological feature in link prediction by identifying its prediction performance upper bound. we introduce a theoretical framework that is compatible with different indexes to gauge the feature, different prediction approaches to utilize the feature, and different metrics to quantify the prediction performance. the maximum capability of a topological feature follows a simple yet theoretically validated expression, which only depends on the extent to which the feature is held in missing and nonexistent links. because a family of indexes based on the same feature shares the same upper bound, the potential of all others can be estimated from one single index. furthermore, a feature's capability is lifted in the supervised prediction, which can be mathematically quantified, allowing us to estimate the benefit of applying machine learning algorithms. the universality of the pattern uncovered is empirically verified by 550 structurally diverse networks. the findings have applications in feature and method selection, and shed light on network characteristics that make a topological feature effective in link prediction.\", 'doi': '10.1093/pnasnexus/pgae113', 'created': '2022-06-30', 'url': 'https://arxiv.org/abs/2206.15101', 'authors': ['yijun ran', 'xiao-ke xu', 'tao jia']}, {'id': '2209.01621', 'title': 'interactive question answering systems: literature review', 'abstract': 'question answering systems are recognized as popular and frequently effective means of information seeking on the web. in such systems, information seekers can receive a concise response to their query by presenting their questions in natural language. interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. on the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. by permitting the user to ask more questions, interactive question answering enables users to dynamically interact with the system and receive more precise results. this survey offers a detailed overview of the interactive question-answering methods that are prevalent in current literature. it begins by explaining the foundational principles of question-answering systems, hence defining new notations and taxonomies to combine all identified works inside a unified framework. the reviewed published work on interactive question-answering systems is then presented and examined in terms of its proposed methodology, evaluation approaches, and dataset/application domain. we also describe trends surrounding specific tasks and issues raised by the community, so shedding light on the future interests of scholars. our work is further supported by a github page with a synthesis of all the major topics covered in this literature study. https://sisinflab.github.io/interactive-question-answering-systems-survey/', 'doi': '10.1145/3657631', 'created': '2022-09-04', 'url': 'https://arxiv.org/abs/2209.01621', 'authors': ['giovanni maria biancofiore', 'yashar deldjoo', 'tommaso di noia', 'eugenio di sciascio', 'fedelucio narducci']}, {'id': '2209.10192', 'title': 'multi-field de-interlacing using deformable convolution residual blocks   and self-attention', 'abstract': 'although deep learning has made significant impact on image/video restoration and super-resolution, learned deinterlacing has so far received less attention in academia or industry. this is despite deinterlacing is well-suited for supervised learning from synthetic data since the degradation model is known and fixed. in this paper, we propose a novel multi-field full frame-rate deinterlacing network, which adapts the state-of-the-art superresolution approaches to the deinterlacing task. our model aligns features from adjacent fields to a reference field (to be deinterlaced) using both deformable convolution residual blocks and self attention. our extensive experimental results demonstrate that the proposed method provides state-of-the-art deinterlacing results in terms of both numerical and perceptual performance. at the time of writing, our model ranks first in the full framerate leaderboard at https://videoprocessing.ai/benchmarks/deinterlacer.html', 'doi': '10.1109/icip46576.2022.9897353', 'created': '2022-09-21', 'url': 'https://arxiv.org/abs/2209.10192', 'authors': ['ronglei ji', 'a. murat tekalp']}, {'id': '2210.01988', 'title': 'bicoptor: two-round secure three-party non-linear computation without   preprocessing for privacy-preserving machine learning', 'abstract': 'the overhead of non-linear functions dominates the performance of the secure multiparty computation (mpc) based privacy-preserving machine learning (ppml). this work introduces a family of novel secure three-party computation (3pc) protocols, bicoptor, which improve the efficiency of evaluating non-linear functions. the basis of bicoptor is a new sign determination protocol, which relies on a clever use of the truncation protocol proposed in secureml (s\\\\&p 2017). our 3pc sign determination protocol only requires two communication rounds, and does not involve any preprocessing. such sign determination protocol is well-suited for computing non-linear functions in ppml, e.g. the activation function relu, maxpool, and their variants. we develop suitable protocols for these non-linear functions, which form a family of gpu-friendly protocols, bicoptor. all bicoptor protocols only require two communication rounds without preprocessing. we evaluate bicoptor under a 3-party lan network over a public cloud, and achieve more than 370,000 drelu/relu or 41,000 maxpool (find the maximum value of nine inputs) operations per second. under the same settings and environment, our relu protocol has a one or even two orders of magnitude improvement to the state-of-the-art works, falcon (pets 2021) or edabits (crypto 2020), respectively without batch processing.', 'doi': '10.1109/sp46215.2023.00074', 'created': '2022-10-04', 'url': 'https://arxiv.org/abs/2210.01988', 'authors': ['lijing zhou', 'ziyu wang', 'hongrui cui', 'qingrui song', 'yu yu']}, {'id': '2210.08298', 'title': 'myhill-nerode theorem for higher-dimensional automata', 'abstract': 'we establish a myhill-nerode type theorem for higher-dimensional automata (hdas), stating that a language is regular if and only if it has finite prefix quotient. hdas extend standard automata with additional structure, making it possible to distinguish between interleavings and concurrency. we also introduce deterministic hdas and show that not all hdas are determinizable, that is, there exist regular languages that cannot be recognised by a deterministic hda. using our theorem, we develop an internal characterisation of deterministic languages. lastly, we develop analogues of the myhill-nerode construction and of determinacy for hdas with interfaces.', 'doi': '', 'created': '2022-10-15', 'url': 'https://arxiv.org/abs/2210.08298', 'authors': ['uli fahrenberg', 'krzysztof ziemiański']}, {'id': '2211.02866', 'title': 'multiband linear cellular automata and endomorphisms of algebraic vector   groups', 'abstract': \"we propose a correspondence between certain multiband linear cellular automata - models of computation widely used in the description of physical phenomena - and endomorphisms of certain algebraic unipotent groups over finite fields. the correspondence is based on the construction of a universal element specialising to a normal generator for any finite field. we use this correspondence to deduce new results concerning the temporal dynamics of such automata, using our prior, purely algebraic, study of the endomorphism ring of vector groups. these produce 'for free' a formula for the number of fixed points of the $n$-iterate in terms of the $p$-adic valuation of $n$, a dichotomy for the artin-mazur dynamical zeta function, and an asymptotic formula for the number of periodic orbits. since multiband linear cellular automata simulate higher order linear automata (in which states depend on finitely many prior temporal states, not just the direct predecessor), the results apply equally well to that class.\", 'doi': '', 'created': '2022-11-05', 'url': 'https://arxiv.org/abs/2211.02866', 'authors': ['jakub byszewski', 'gunther cornelissen']}, {'id': '2211.07440', 'title': 'leveraging automatic personalised nutrition: food image recognition   benchmark and dataset based on nutrition taxonomy', 'abstract': 'maintaining a healthy lifestyle has become increasingly challenging in today\\'s sedentary society marked by poor eating habits. to address this issue, both national and international organisations have made numerous efforts to promote healthier diets and increased physical activity. however, implementing these recommendations in daily life can be difficult, as they are often generic and not tailored to individuals. this study presents the ai4food-nutritiondb database, the first nutrition database that incorporates food images and a nutrition taxonomy based on recommendations by national and international health authorities. the database offers a multi-level categorisation, comprising 6 nutritional levels, 19 main categories (e.g., \"meat\"), 73 subcategories (e.g., \"white meat\"), and 893 specific food products (e.g., \"chicken\"). the ai4food-nutritiondb opens the doors to new food computing approaches in terms of food intake frequency, quality, and categorisation. also, we present a standardised experimental protocol and benchmark including three tasks based on the nutrition taxonomy (i.e., category, subcategory, and final product recognition). these resources are available to the research community, including our deep learning models trained on ai4food-nutritiondb, which can serve as pre-trained models, achieving accurate recognition results for challenging food image databases.', 'doi': '10.1007/s11042-024-19161-4', 'created': '2022-11-14', 'url': 'https://arxiv.org/abs/2211.07440', 'authors': ['sergio romero-tapiador', 'ruben tolosana', 'aythami morales', 'julian fierrez', 'ruben vera-rodriguez', 'isabel espinosa-salinas', 'gala freixer', 'enrique carrillo de santa pau', 'ana ramírez de molina', 'javier ortega-garcia']}, {'id': '2211.11424', 'title': 'modeling hierarchical structural distance for unsupervised domain   adaptation', 'abstract': 'unsupervised domain adaptation (uda) aims to estimate a transferable model for unlabeled target domains by exploiting labeled source data. optimal transport (ot) based methods have recently been proven to be a promising solution for uda with a solid theoretical foundation and competitive performance. however, most of these methods solely focus on domain-level ot alignment by leveraging the geometry of domains for domain-invariant features based on the global embeddings of images. however, global representations of images may destroy image structure, leading to the loss of local details that offer category-discriminative information. this study proposes an end-to-end deep hierarchical optimal transport method (deephot), which aims to learn both domain-invariant and category-discriminative representations by mining hierarchical structural relations among domains. the main idea is to incorporate a domain-level ot and image-level ot into a unified ot framework, hierarchical optimal transport, to model the underlying geometry in both domain space and image space. in deephot framework, an image-level ot serves as the ground distance metric for the domain-level ot, leading to the hierarchical structural distance. compared with the ground distance of the conventional domain-level ot, the image-level ot captures structural associations among local regions of images that are beneficial to classification. in this way, deephot, a unified ot framework, not only aligns domains by domain-level ot, but also enhances the discriminative power through image-level ot. moreover, to overcome the limitation of high computational complexity, we propose a robust and efficient implementation of deephot by approximating origin ot with sliced wasserstein distance in image-level ot and accomplishing the mini-batch unbalanced domain-level ot.', 'doi': '', 'created': '2022-11-21', 'url': 'https://arxiv.org/abs/2211.11424', 'authors': ['yingxue xu', 'guihua wen', 'yang hu', 'pei yang']}, {'id': '2212.03218', 'title': 'transforming eu governance: the digital integration through ebsi and   glass', 'abstract': \"traditionally, government systems managed citizen identities through disconnected data systems, using simple identifiers and paper-based processes, limiting digital trust and requiring citizens to request identity verification documents. the digital era offers a shift towards unique digital identifiers for each citizen, enabling a 'citizen wallet' for easier access to personal documents like academic records and licences, with enhanced security through digital signatures. the european commission's initiative for a digital wallet for every eu citizen aims to improve mobility and integration, leveraging the european blockchain services infrastructure (ebsi) for harmonised citizen integration. this paper discusses how ebsi and the glass project can advance governance and streamline access to identity documents.\", 'doi': '', 'created': '2022-12-06', 'url': 'https://arxiv.org/abs/2212.03218', 'authors': ['dimitrios kasimatis', 'william j buchanan', 'mwarwan abubakar', 'owen lo', 'christos chrysoulas', 'nikolaos pitropakis', 'pavlos papadopoulos', 'sarwar sayeed', 'marc sel']}, {'id': '2301.00185', 'title': 'dimensions of exactly divergence-free finite element spaces in 3d', 'abstract': 'we examine the dimensions of various inf-sup stable mixed finite element spaces on tetrahedral meshes in 3d with exact divergence constraints. more precisely, we compare the standard scott-vogelius elements of higher polynomial degree and low order methods on split meshes, the alfeld and the worsey-farin split. the main tool is a counting strategy to express the degrees of freedom for given polynomial degree and given split in terms of few mesh quantities, for which bounds and asymptotic behavior under mesh refinement is investigated. furthermore, this is used to obtain insights on potential precursor spaces in full de rham complexes for finite element methods on the worsey-farin split.', 'doi': '10.1137/22m1544579', 'created': '2022-12-31', 'url': 'https://arxiv.org/abs/2301.00185', 'authors': ['l. ridgway scott', 'tabea tscherpel']}, {'id': '2301.00812', 'title': 'one-shot skill assessment in high-stakes domains with limited data via   meta learning', 'abstract': 'deep learning (dl) has achieved robust competency assessment in various high-stakes fields. however, the applicability of dl models is often hampered by their substantial data requirements and confinement to specific training domains. this prevents them from transitioning to new tasks where data is scarce. therefore, domain adaptation emerges as a critical element for the practical implementation of dl in real-world scenarios. herein, we introduce a-vbanet, a novel meta-learning model capable of delivering domain-agnostic skill assessment via one-shot learning. our methodology has been tested by assessing surgical skills on five laparoscopic and robotic simulators and real-life laparoscopic cholecystectomy. our model successfully adapted with accuracies up to 99.5% in one-shot and 99.9% in few-shot settings for simulated tasks and 89.7% for laparoscopic cholecystectomy. this study marks the first instance of a domain-agnostic methodology for skill assessment in critical fields setting a precedent for the broad application of dl across diverse real-life domains with limited data.', 'doi': '10.1016/j.compbiomed.2024.108470', 'created': '2022-12-15', 'url': 'https://arxiv.org/abs/2301.00812', 'authors': ['erim yanik', 'steven schwaitzberg', 'gene yang', 'xavier intes', 'jack norfleet', 'matthew hackett', 'suvranu de']}, {'id': '2301.06280', 'title': 'an augmented matrix-based cj-feast svdsolver for computing a partial   singular value decomposition with the singular values in a given interval', 'abstract': 'the cross-product matrix-based cj-feast svdsolver proposed previously by the authors is shown to compute the left singular vector possibly much less accurately than the right singular vector and may be numerically backward unstable when a desired singular value is small. in this paper, an alternative augmented matrix-based cj-feast svdsolver is considered to compute the singular triplets of a large matrix $a$ with the singular values in an interval $[a,b]$ contained in the singular spectrum. the new cj-feast svdsolver is a subspace iteration applied to an approximate spectral projector of the augmented matrix $[0, a^t; a, 0]$ associated with the eigenvalues in $[a,b]$, and constructs approximate left and right singular subspaces with the desired singular values independently, onto which $a$ is projected to obtain the ritz approximations to the desired singular triplets. compact estimates are given for the accuracy of the approximate spectral projector, and a number of convergence results are established. the new solver is proved to be always numerically backward stable. a convergence comparison of the cross-product and augmented matrix-based cj-feast svdsolvers is made, and a general-purpose choice strategy between the two solvers is proposed for the robustness and overall efficiency. numerical experiments confirm all the results.', 'doi': '10.1137/23m1547500', 'created': '2023-01-16', 'url': 'https://arxiv.org/abs/2301.06280', 'authors': ['zhongxiao jia', 'kailiang zhang']}, {'id': '2301.06520', 'title': 'ul-dl duality for cell-free massive mimo with per-ap power and   information constraints', 'abstract': 'we derive a novel uplink-downlink duality principle for optimal joint precoding design under per-transmitter power and information constraints in fading channels. the information constraints model limited sharing of channel state information and data bearing signals across the transmitters. the main application is to cell-free networks, where each access point (ap) must typically satisfy an individual power constraint and form its transmit signal using limited cooperation capabilities. our duality principle applies to ergodic achievable rates given by the popular hardening bound, and it can be interpreted as a nontrivial generalization of a previous result by yu and lan for deterministic channels. this generalization allows us to study involved information constraints going beyond the simple case of cluster-wise centralized precoding covered by previous techniques. specifically, we show that the optimal joint precoders are, in general, given by an extension of the recently developed team minimum mean-square error method. as a particular yet practical example, we then solve the problem of optimal local precoding design in user-centric cell-free massive mimo networks subject to per-ap power constraints.', 'doi': '', 'created': '2023-01-16', 'url': 'https://arxiv.org/abs/2301.06520', 'authors': ['lorenzo miretti', 'renato l. g. cavalcante', 'emil björnson', 'sławomir stańczak']}, {'id': '2303.04467', 'title': 'the evolution of cooperation and diversity by integrated indirect   reciprocity', 'abstract': \"indirect reciprocity is one of the major mechanisms for the evolution of cooperation in human societies. there are two types of indirect reciprocity: upstream and downstream. cooperation in downstream reciprocity follows the pattern, 'you helped someone, and i will help you'. the direction of cooperation is reversed in upstream reciprocity, which instead follows the pattern, 'you helped me, and i will help someone else'. in reality, these two types of indirect reciprocity often occur in combination. however, upstream and downstream reciprocity have mostly been studied theoretically in isolation. here, we propose a new model that integrates both types. we apply the standard giving-game framework of indirect reciprocity and analyze the model by means of evolutionary game theory. we show that the model can result in the stable coexistence of altruistic reciprocators and free riders in well-mixed populations. we also found that considering inattention in the assessment rule can strengthen the stability of this mixed equilibrium, even resulting in a global attractor. our results indicate that the cycles of forwarding help and rewarding help need to be established for creating and maintaining diversity and inclusion in a society.\", 'doi': '10.3390/g15020015', 'created': '2023-03-08', 'url': 'https://arxiv.org/abs/2303.04467', 'authors': ['tatsuya sasaki', 'satoshi uchida', 'isamu okada', 'hitoshi yamamoto']}, {'id': '2303.10216', 'title': 'approximation of group explainers with coalition structure using monte   carlo sampling on the product space of coalitions and features', 'abstract': 'in recent years, many machine learning (ml) explanation techniques have been designed using ideas from cooperative game theory. these game-theoretic explainers suffer from high complexity, hindering their exact computation in practical settings. in our work, we focus on a wide class of linear game values, as well as coalitional values, for the marginal game based on a given ml model and predictor vector. by viewing these explainers as expectations over appropriate sample spaces, we design a novel monte carlo sampling algorithm that estimates them at a reduced complexity that depends linearly on the size of the background dataset. we set up a rigorous framework for the statistical analysis and obtain error bounds for our sampling methods. the advantage of this approach is that it is fast, easily implementable, and model-agnostic. furthermore, it has similar statistical accuracy as other known estimation techniques that are more complex and model-specific. we provide rigorous proofs of statistical convergence, as well as numerical experiments whose results agree with our theoretical findings.', 'doi': '', 'created': '2023-03-17', 'url': 'https://arxiv.org/abs/2303.10216', 'authors': ['konstandinos kotsiopoulos', 'alexey miroshnikov', 'khashayar filom', 'arjun ravi kannan']}, {'id': '2303.10322', 'title': 'inverse cubature and quadrature kalman filters', 'abstract': \"recent research in inverse cognition with cognitive radar has led to the development of inverse stochastic filters that are employed by the target to infer the information the cognitive radar may have learned. prior works addressed this inverse cognition problem by proposing inverse kalman filter (i-kf) and inverse extended kf (i-ekf), respectively, for linear and non-linear gaussian state-space models. however, in practice, many counter-adversarial settings involve highly non-linear system models, wherein ekf's linearization often fails. in this paper, we consider the efficient numerical integration techniques to address such non-linearities and, to this end, develop inverse cubature kf (i-ckf), inverse quadrature kf (i-qkf), and inverse cubature-quadrature kf (i-cqkf). for the unknown system model case, we develop reproducing kernel hilbert space (rkhs)-based ckf. we derive the stochastic stability conditions for the proposed filters in the exponential-mean-squared-boundedness sense and prove the filters' consistency. numerical experiments demonstrate the estimation accuracy of our i-ckf, i-qkf, and i-cqkf with the recursive cram\\\\'{e}r-rao lower bound as a benchmark.\", 'doi': '', 'created': '2023-03-17', 'url': 'https://arxiv.org/abs/2303.10322', 'authors': ['himali singh', 'kumar vijay mishra', 'arpan chattopadhyay']}, {'id': '2303.12342', 'title': 'one-step detection paradigm for hyperspectral anomaly detection via   spectral deviation relationship learning', 'abstract': 'hyperspectral anomaly detection (had) involves identifying the targets that deviate spectrally from their surroundings, without prior knowledge. recently, deep learning based methods have become the mainstream had methods, due to their powerful spatial-spectral feature extraction ability. however, the current deep detection models are optimized to complete a proxy task (two-step paradigm), such as background reconstruction or generation, rather than achieving anomaly detection directly. this leads to suboptimal results and poor transferability, which means that the deep model is trained and tested on the same image. in this paper, an unsupervised transferred direct detection (tdd) model is proposed, which is optimized directly for the anomaly detection task (one-step paradigm) and has transferability. specially, the tdd model is optimized to identify the spectral deviation relationship according to the anomaly definition. compared to learning the specific background distribution as most models do, the spectral deviation relationship is universal for different images and guarantees the model transferability. to train the tdd model in an unsupervised manner, an anomaly sample simulation strategy is proposed to generate numerous pairs of anomaly samples. furthermore, a global self-attention module and a local self-attention module are designed to help the model focus on the \"spectrally deviating\" relationship. the tdd model was validated on four public had datasets. the results show that the proposed tdd model can successfully overcome the limitation of traditional model training and testing on a single image, and the model has a powerful detection ability and excellent transferability.', 'doi': '', 'created': '2023-03-22', 'url': 'https://arxiv.org/abs/2303.12342', 'authors': ['jingtao li', 'xinyu wang', 'shaoyu wang', 'hengwei zhao', 'liangpei zhang', 'yanfei zhong']}, {'id': '2303.16421', 'title': 'chatgpt is a knowledgeable but inexperienced solver: an investigation of   commonsense problem in large language models', 'abstract': \"large language models (llms) have made significant progress in nlp. however, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point. in this paper, we specifically focus on chatgpt, a widely used and easily accessible llm, and ask the following questions: (1) can chatgpt effectively answer commonsense questions? (2) is chatgpt aware of the underlying commonsense knowledge for answering a specific question? (3) is chatgpt knowledgeable in commonsense? (4) can chatgpt effectively leverage commonsense for answering questions? we conduct a series of experiments on 11 datasets to evaluate chatgpt's commonsense abilities, including answering commonsense questions, identifying necessary knowledge, generating knowledge descriptions, and using knowledge descriptions to answer questions again. experimental results show that: (1) chatgpt can achieve good qa accuracies in commonsense tasks, while still struggling with certain domains of datasets. (2) chatgpt is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) despite its knowledge, chatgpt is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense for answering a specific question. these findings raise the need to explore improved mechanisms for effectively incorporating commonsense into llms like chatgpt, such as better instruction following and commonsense guidance.\", 'doi': '', 'created': '2023-03-28', 'url': 'https://arxiv.org/abs/2303.16421', 'authors': ['ning bian', 'xianpei han', 'le sun', 'hongyu lin', 'yaojie lu', 'ben he', 'shanshan jiang', 'bin dong']}, {'id': '2304.00372', 'title': 'auxiliary-variable adaptive control barrier functions for safety   critical systems', 'abstract': 'this paper studies safety guarantees for systems with time-varying control bounds. it has been shown that optimizing quadratic costs subject to state and control constraints can be reduced to a sequence of quadratic programs (qps) using control barrier functions (cbfs). one of the main challenges in this method is that the cbf-based qp could easily become infeasible under tight control bounds, especially when the control bounds are time-varying. the recently proposed adaptive cbfs have addressed such infeasibility issues, but require extensive and non-trivial hyperparameter tuning for the cbf-based qp and may introduce overshooting control near the boundaries of safe sets. to address these issues, we propose a new type of adaptive cbfs called auxiliary-variable adaptive cbfs (avcbfs). specifically, we introduce an auxiliary variable that multiplies each cbf itself, and define dynamics for the auxiliary variable to adapt it in constructing the corresponding cbf constraint. in this way, we can improve the feasibility of the cbf-based qp while avoiding extensive parameter tuning with non-overshooting control since the formulation is identical to classical cbf methods. we demonstrate the advantages of using avcbfs and compare them with existing techniques on an adaptive cruise control (acc) problem with time-varying control bounds.', 'doi': '', 'created': '2023-04-01', 'url': 'https://arxiv.org/abs/2304.00372', 'authors': ['shuo liu', 'wei xiao', 'calin a. belta']}, {'id': '2304.05166', 'title': 'learning distributions over trajectories for human behavior prediction', 'abstract': 'predicting the future behavior of human road users is an important aspect for the development of risk-aware autonomous vehicles. while many models have been developed towards this end, effectively capturing and predicting the variability inherent to human behavior still remains an open challenge. this paper proposes trajflow - a new approach for probabilistic trajectory prediction based on normalizing flows. we reformulate the problem of capturing distributions over trajectories into capturing distributions over abstracted trajectory features using an autoencoder, simplifying the learning task of the normalizing flows. trajflow outperforms state-of-the-art behavior prediction models in capturing full trajectory distributions in two synthetic benchmarks with known true distributions, and is competitive on the naturalistic datasets eth/ucy, round, and nuscenes. our results demonstrate the effectiveness of trajflow in probabilistic prediction of human behavior.', 'doi': '', 'created': '2023-04-11', 'url': 'https://arxiv.org/abs/2304.05166', 'authors': ['anna mészáros', 'julian f. schumann', 'javier alonso-mora', 'arkady zgonnikov', 'jens kober']}, {'id': '2304.07468', 'title': 'limited diffusion of scientific knowledge forecasts collapse', 'abstract': 'market bubbles emerge when asset prices are driven unsustainably higher than asset values and shifts in belief burst them. we demonstrate the same phenomenon for biomedical knowledge when promising research receives inflated attention. we predict deflationary events by developing a diffusion index that captures whether research areas have been amplified within social and scientific bubbles or have diffused and become evaluated more broadly. we illustrate our diffusion approach contrasting the trajectories of cardiac stem cell research and cancer immunotherapy. we then trace the diffusion of unique 28,504 subfields in biomedicine comprising nearly 1.9m papers and more than 80m citations and demonstrate that limited diffusion of biomedical knowledge anticipates abrupt decreases in popularity. our analysis emphasizes that restricted diffusion, implying a socio-epistemic bubble, leads to dramatic collapses in relevance and attention accorded to scientific knowledge.', 'doi': '', 'created': '2023-04-15', 'url': 'https://arxiv.org/abs/2304.07468', 'authors': ['donghyun kang', 'robert s. danziger', 'jalees rehman', 'james a. evans']}, {'id': '2304.09779', 'title': 'equalised odds is not equal individual odds: post-processing for group   and individual fairness', 'abstract': \"group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. these two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. this procedure may provide two similar individuals from the same protected group with classification odds that are disparately different -- a clear violation of individual fairness. assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving equal chances of a positive outcome to another, which we argue is another type of unfairness called individual odds. we reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their lipschitz constant. our solution preserves the model's predictive power, individual fairness and robustness while ensuring group fairness.\", 'doi': '10.1145/3630106.3658989', 'created': '2023-04-19', 'url': 'https://arxiv.org/abs/2304.09779', 'authors': ['edward a. small', 'kacper sokol', 'daniel manning', 'flora d. salim', 'jeffrey chan']}, {'id': '2304.10286', 'title': 'on the computational power of particle methods', 'abstract': 'we investigate the computational power of particle methods, a well-established class of algorithms with applications in scientific computing and computer simulation. the computational power of a compute model determines the class of problems it can solve. automata theory allows describing the computational power of abstract machines (automata) and the problems they can solve. at the top of the chomsky hierarchy of formal languages and grammars are turing machines, which resemble the concept on which most modern computers are built. although particle methods can be interpreted as automata based on their formal definition, their computational power has so far not been studied. we address this by analyzing turing completeness of particle methods. in particular, we prove two sets of restrictions under which a particle method is still turing powerful, and we show when it loses turing powerfulness. this contributes to understanding the theoretical foundations of particle methods and provides insight into the powerfulness of computer simulations.', 'doi': '', 'created': '2023-04-20', 'url': 'https://arxiv.org/abs/2304.10286', 'authors': ['johannes pahlke', 'ivo f. sbalzarini']}, {'id': '2304.13029', 'title': 'bake off redux: a review and experimental evaluation of recent time   series classification algorithms', 'abstract': \"in 2017, a research paper compared 18 time series classification (tsc) algorithms on 85 datasets from the university of california, riverside (ucr) archive. this study, commonly referred to as a `bake off', identified that only nine algorithms performed significantly better than the dynamic time warping (dtw) and rotation forest benchmarks that were used. the study categorised each algorithm by the type of feature they extract from time series data, forming a taxonomy of five main algorithm types. this categorisation of algorithms alongside the provision of code and accessible results for reproducibility has helped fuel an increase in popularity of the tsc field. over six years have passed since this bake off, the ucr archive has expanded to 112 datasets and there have been a large number of new algorithms proposed. we revisit the bake off, seeing how each of the proposed categories have advanced since the original publication, and evaluate the performance of newer algorithms against the previous best-of-category using an expanded ucr archive. we extend the taxonomy to include three new categories to reflect recent developments. alongside the originally proposed distance, interval, shapelet, dictionary and hybrid based algorithms, we compare newer convolution and feature based algorithms as well as deep learning approaches. we introduce 30 classification datasets either recently donated to the archive or reformatted to the tsc format, and use these to further evaluate the best performing algorithm from each category. overall, we find that two recently proposed algorithms, hydra+multirocket and hive-cotev2, perform significantly better than other approaches on both the current and new tsc problems.\", 'doi': '', 'created': '2023-04-25', 'url': 'https://arxiv.org/abs/2304.13029', 'authors': ['matthew middlehurst', 'patrick schäfer', 'anthony bagnall']}, {'id': '2305.03803', 'title': 'a survey of trojans in neural models of source code: taxonomy and   techniques', 'abstract': 'in this work, we study literature in explainable ai and safe ai to understand poisoning of neural models of code. in order to do so, we first establish a novel taxonomy for trojan ai for code, and present a new aspect-based classification of triggers in neural models of code. next, we highlight recent works that help us deepen our conception of how these models understand software code. then we pick some of the recent, state-of-art poisoning strategies that can be used to manipulate such models. the insights we draw can potentially help to foster future research in the area of trojan ai for code.', 'doi': '', 'created': '2023-05-05', 'url': 'https://arxiv.org/abs/2305.03803', 'authors': ['aftab hussain', 'md rafiqul islam rabin', 'toufique ahmed', 'navid ayoobi', 'bowen xu', 'prem devanbu', 'mohammad amin alipour']}, {'id': '2305.07877', 'title': 'differentiating viral and bacterial infections: a machine learning model   based on routine blood test values', 'abstract': 'the growing threat of antibiotic resistance necessitates accurate differentiation between bacterial and viral infections for proper antibiotic administration. in this study, a virus vs. bacteria machine learning model was developed to distinguish between these infection types using 16 routine blood test results, c-reactive protein concentration (crp), biological sex, and age. with a dataset of 44,120 cases from a single medical center, the model achieved an accuracy of 82.2 %, a sensitivity of 79.7 %, a specificity of 84.5 %, a brier score of 0.129, and an area under the roc curve (auc) of 0.905, outperforming a crp-based decision rule. notably, the machine learning model enhanced accuracy within the crp range of 10-40 mg/l, a range where crp alone is less informative. these results highlight the advantage of integrating multiple blood parameters in diagnostics. the \"virus vs. bacteria\" model paves the way for advanced diagnostic tools, leveraging machine learning to optimize infection management.', 'doi': '10.1016/j.heliyon.2024.e29372', 'created': '2023-05-13', 'url': 'https://arxiv.org/abs/2305.07877', 'authors': ['gregor gunčar', 'matjaž kukar', 'tim smole', 'sašo moškon', 'tomaž vovko', 'simon podnar', 'peter černelč', 'miran brvar', 'mateja notar', 'manca köster', 'marjeta tušek jelenc', 'marko notar']}, {'id': '2305.18453', 'title': 'conditional diffusion models for semantic 3d brain mri synthesis', 'abstract': 'artificial intelligence (ai) in healthcare, especially in medical imaging, faces challenges due to data scarcity and privacy concerns. addressing these, we introduce med-ddpm, a diffusion model designed for 3d semantic brain mri synthesis. this model effectively tackles data scarcity and privacy issues by integrating semantic conditioning. this involves the channel-wise concatenation of a conditioning image to the model input, enabling control in image generation. med-ddpm demonstrates superior stability and performance compared to existing 3d brain imaging synthesis methods. it generates diverse, anatomically coherent images with high visual fidelity. in terms of dice score accuracy in the tumor segmentation task, med-ddpm achieves 0.6207, close to the 0.6531 accuracy of real images, and outperforms baseline models. combined with real images, it further increases segmentation accuracy to 0.6675, showing the potential of our proposed method for data augmentation. this model represents the first use of a diffusion model in 3d semantic brain mri synthesis, producing high-quality images. its semantic conditioning feature also shows potential for image anonymization in biomedical imaging, addressing data and privacy issues. we provide the code and model weights for med-ddpm on our github repository (https://github.com/mobaidoctor/med-ddpm/) to support reproducibility.', 'doi': '10.1109/jbhi.2024.3385504', 'created': '2023-05-29', 'url': 'https://arxiv.org/abs/2305.18453', 'authors': ['zolnamar dorjsembe', 'hsing-kuo pao', 'sodtavilan odonchimed', 'furen xiao']}, {'id': '2306.03027', 'title': 'explicit feedback synthesis for nonlinear robust model predictive   control driven by quasi-interpolation', 'abstract': 'we present quifs (quasi-interpolation driven feedback synthesis): an offline feedback synthesis algorithm for explicit nonlinear robust minmax model predictive control (mpc) problems with guaranteed quality of approximation. the underlying technique is driven by a particular type of grid-based quasi-interpolation scheme. the quifs algorithm departs drastically from conventional approximation algorithms that are employed in the mpc industry (in particular, it is neither based on multi-parametric programming tools and nor does it involve kernel methods), and the essence of its point of departure is encoded in the following challenge-answer approach: given an error margin $\\\\varepsilon>0$, compute in a single stroke a feasible feedback policy that is uniformly $\\\\varepsilon$-close to the optimal mpc feedback policy for a given nonlinear system subjected to constraints and bounded uncertainties. closed-loop stability and recursive feasibility under the approximate feedback policy are also established. we provide a library of numerical examples to illustrate our results.', 'doi': '', 'created': '2023-06-05', 'url': 'https://arxiv.org/abs/2306.03027', 'authors': ['siddhartha ganguly', 'debasish chatterjee']}, {'id': '2306.06449', 'title': 'list homomorphisms to separable signed graphs', 'abstract': 'the complexity of the list homomorphism problem for signed graphs appears difficult to classify. existing results focus on special classes of signed graphs, such as trees and reflexive signed graphs. irreflexive signed graphs are in a certain sense the heart of the problem, as noted by a recent paper of kim and siggers. we focus on a special class of irreflexive signed graphs, namely those in which the unicoloured edges form a spanning path or cycle, which we call separable signed graphs. we classify the complexity of list homomorphisms to these separable signed graphs; we believe that these signed graphs will play an important role for the general resolution of the irreflexive case. we also relate our results to a conjecture of kim and siggers concerning the special case of semi-balanced irreflexive signed graphs; we have proved the conjecture in another paper, and the present results add structural information to that topic.', 'doi': '10.1016/j.tcs.2024.114580', 'created': '2023-06-10', 'url': 'https://arxiv.org/abs/2306.06449', 'authors': ['jan bok', 'richard brewster', 'tomás feder', 'pavol hell', 'nikola jedličková']}, {'id': '2306.08386', 'title': 'efficient backdoor attacks for deep neural networks in real-world   scenarios', 'abstract': 'recent deep neural networks (dnns) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. however, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. in this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. we refer to this scenario as data-constrained backdoor attacks. in such cases, previous attack methods suffer from severe efficiency degradation due to the entanglement between benign and poisoning features during the backdoor injection process. to tackle this problem, we introduce three clip-based technologies from two distinct streams: clean feature suppression and poisoning feature augmentation.effective solution for data-constrained backdoor attacks. the results demonstrate remarkable improvements, with some settings achieving over 100% improvement compared to existing attacks in data-constrained scenarios. code is available at https://github.com/sunh1113/efficient-backdoor-attacks-for-deep-neural-networks-in-real-world-scenarios', 'doi': '', 'created': '2023-06-14', 'url': 'https://arxiv.org/abs/2306.08386', 'authors': ['ziqiang li', 'hong sun', 'pengfei xia', 'heng li', 'beihao xia', 'yi wu', 'bin li']}, {'id': '2306.08553', 'title': 'noise stability optimization for flat minima with tight rates', 'abstract': \"we consider minimizing a perturbed function $f(w) = \\\\mathbb{e}_{u}[f(w + u)]$, given a function $f: \\\\mathbb{r}^d \\\\rightarrow \\\\mathbb{r}$ and a random sample $u$ from a distribution $\\\\mathcal{p}$ with mean zero. when $\\\\mathcal{p}$ is the isotropic gaussian, $f(w)$ is roughly equal to $f(w)$ plus a penalty on the trace of $\\\\nabla^2 f(w)$, scaled by the variance of $\\\\mathcal{p}$. this penalty on the hessian has the benefit of improving generalization, through pac-bayes analysis. it is useful in low-sample regimes, for instance, when a (large) pre-trained model is fine-tuned on a small data set. one way to minimize $f$ is by adding $u$ to $w$, and then run sgd. we observe, empirically, that this noise injection does not provide significant gains over sgd, in our experiments of conducting fine-tuning on three image classification data sets. we design a simple, practical algorithm that adds noise along both $u$ and $-u$, with the option of adding several perturbations and taking their average. we analyze the convergence of this algorithm, showing tight rates on the norm of the output's gradient.   we provide a comprehensive empirical analysis of our algorithm, by first showing that in an over-parameterized matrix sensing problem, it can find solutions with lower test loss than naive noise injection. then, we compare our algorithm with four sharpness-reducing training methods (such as the sharpness-aware minimization (foret et al., 2021)). we find that our algorithm can outperform them by up to 1.8% test accuracy, for fine-tuning resnet on six image classification data sets. it leads to a 17.7% (and 12.8%) reduction in the trace (and largest eigenvalue) of the hessian matrix of the loss surface. this form of regularization on the hessian is compatible with $\\\\ell_2$ weight decay (and data augmentation), in the sense that combining both can lead to improved empirical performance.\", 'doi': '', 'created': '2023-06-14', 'url': 'https://arxiv.org/abs/2306.08553', 'authors': ['haotian ju', 'dongyue li', 'hongyang r. zhang']}, {'id': '2306.10091', 'title': 'acoustic identification of ae. aegypti mosquitoes using smartphone apps   and residual convolutional neural networks', 'abstract': 'in this paper, we advocate in favor of smartphone apps as low-cost, easy-to-deploy solution for raising awareness among the population on the proliferation of aedes aegypti mosquitoes. nevertheless, devising such a smartphone app is challenging, for many reasons, including the required maturity level of techniques for identifying mosquitoes based on features that can be captured using smartphone resources. in this paper, we identify a set of (non-exhaustive) requirements that smartphone apps must meet to become an effective tooling in the fight against ae. aegypti, and advance the state-of-the-art with (i) a residual convolutional neural network for classifying ae. aegypti mosquitoes from their wingbeat sound, (ii) a methodology for reducing the influence of background noise in the classification process, and (iii) a dataset for benchmarking solutions for detecting ae. aegypti mosquitoes from wingbeat sound recordings. from the analysis of accuracy and recall, we provide evidence that convolutional neural networks have potential as a cornerstone for tracking mosquito apps for smartphones.', 'doi': '10.1016/j.bspc.2024.106342', 'created': '2023-06-16', 'url': 'https://arxiv.org/abs/2306.10091', 'authors': ['kayuã oleques paim', 'ricardo rohweder', 'mariana recamonde-mendoza', 'rodrigo brandão mansilha1', 'weverton cordeiro']}, {'id': '2306.17808', 'title': 'circular systems engineering', 'abstract': \"the perception of the value and propriety of modern engineered systems is changing. in addition to their functional and extra-functional properties, nowadays' systems are also evaluated by their sustainability properties. the next generation of systems will be characterized by an overall elevated sustainability -- including their post-life, driven by efficient value retention mechanisms. current systems engineering practices fall short of supporting these ambitions and need to be revised appropriately. in this paper, we introduce the concept of circular systems engineering, a novel paradigm for systems sustainability, and define two principles to successfully implement it: end-to-end sustainability and bipartite sustainability. we outline typical organizational evolution patterns that lead to the implementation and adoption of circularity principles, and outline key challenges and research opportunities.\", 'doi': '10.1007/s10270-024-01154-4', 'created': '2023-06-30', 'url': 'https://arxiv.org/abs/2306.17808', 'authors': ['istvan david', 'dominik bork', 'gerti kappel']}, {'id': '2307.01004', 'title': 'joint coordinate regression and association for multi-person pose   estimation, a pure neural network approach', 'abstract': 'we introduce a novel one-stage end-to-end multi-person 2d pose estimation algorithm, known as joint coordinate regression and association (jcra), that produces human pose joints and associations without requiring any post-processing. the proposed algorithm is fast, accurate, effective, and simple. the one-stage end-to-end network architecture significantly improves the inference speed of jcra. meanwhile, we devised a symmetric network structure for both the encoder and decoder, which ensures high accuracy in identifying keypoints. it follows an architecture that directly outputs part positions via a transformer network, resulting in a significant improvement in performance. extensive experiments on the ms coco and crowdpose benchmarks demonstrate that jcra outperforms state-of-the-art approaches in both accuracy and efficiency. moreover, jcra demonstrates 69.2 map and is 78\\\\% faster at inference acceleration than previous state-of-the-art bottom-up algorithms. the code for this algorithm will be publicly available.', 'doi': '', 'created': '2023-07-03', 'url': 'https://arxiv.org/abs/2307.01004', 'authors': ['dongyang yu', 'yunshi xie', 'wangpeng an', 'li zhang', 'yufeng yao']}, {'id': '2307.02620', 'title': 'dynamic observation policies in observation cost-sensitive reinforcement   learning', 'abstract': 'reinforcement learning (rl) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. the action-perception cycle in rl, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. in applications such as materials design, deep-sea and planetary robot exploration and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. in this paper, we survey the recently growing literature that adopts the perspective that an rl agent might not need, or even want, a costly measurement at each time step. within this context, we propose the deep dynamic multi-step observationless agent (dmsoa), contrast it with the literature and empirically evaluate it on openai gym and atari pong environments. our results, show that dmsoa learns a better policy with fewer decision steps and measurements than the considered alternative from the literature.', 'doi': '', 'created': '2023-07-05', 'url': 'https://arxiv.org/abs/2307.02620', 'authors': ['colin bellinger', 'mark crowley', 'isaac tamblyn']}, {'id': '2307.02913', 'title': 'numerical methods with coordinate transforms for efficient brownian   dynamics simulations', 'abstract': 'many stochastic processes in the physical and biological sciences can be modelled as brownian dynamics with multiplicative noise. however, numerical integrators for these processes can lose accuracy or even fail to converge when the diffusion term is configuration-dependent. one remedy is to construct a transform to a constant-diffusion process and sample the transformed process instead. in this work, we explain how coordinate-based and time-rescaling-based transforms can be used either individually or in combination to map a general class of variable-diffusion brownian motion processes into constant-diffusion ones. the transforms are invertible, thus allowing recovery of the original dynamics. we motivate our methodology using examples in one dimension before then considering multivariate diffusion processes. we illustrate the benefits of the transforms through numerical simulations, demonstrating how the right combination of integrator and transform can improve computational efficiency and the order of convergence to the invariant distribution. notably, the transforms that we derive are applicable to a class of multibody, anisotropic stokes-einstein diffusion that has applications in biophysical modelling.', 'doi': '', 'created': '2023-07-06', 'url': 'https://arxiv.org/abs/2307.02913', 'authors': ['dominic phillips', 'charles matthews', 'benedict leimkuhler']}, {'id': '2307.06917', 'title': 'llm-assisted knowledge graph engineering: experiments with chatgpt', 'abstract': 'knowledge graphs (kg) provide us with a structured, flexible, transparent, cross-system, and collaborative way of organizing our knowledge and data across various domains in society and industrial as well as scientific disciplines. kgs surpass any other form of representation in terms of effectiveness. however, knowledge graph engineering (kge) requires in-depth experiences of graph structures, web technologies, existing models and vocabularies, rule sets, logic, as well as best practices. it also demands a significant amount of work. considering the advancements in large language models (llms) and their interfaces and applications in recent years, we have conducted comprehensive experiments with chatgpt to explore its potential in supporting kge. in this paper, we present a selection of these experiments and their results to demonstrate how chatgpt can assist us in the development and management of kgs.', 'doi': '10.1007/978-3-658-43705-3_8', 'created': '2023-07-13', 'url': 'https://arxiv.org/abs/2307.06917', 'authors': ['lars-peter meyer', 'claus stadler', 'johannes frey', 'norman radtke', 'kurt junghanns', 'roy meissner', 'gordian dziwis', 'kirill bulert', 'michael martin']}, {'id': '2307.07520', 'title': 'translating scientific latin texts with artificial intelligence: the   works of euler and contemporaries', 'abstract': 'the major hindrance in the study of earlier scientific literature is the availability of latin translations into modern languages. this is particular true for the works of euler who authored about 850 manuscripts and wrote a thousand letters and received back almost two thousand more. the translation of many of these manuscripts, books and letters have been published in various sources over the last two centuries, but many more have not yet appeared. fortunately, nowadays, artificial intelligence (ai) translation can be used to circumvent the challenges of translating such substantial number of texts. to validate this tool, benchmark tests have been performed to compare the performance of two popular ai translating algorithms, namely google translate and chatgpt. additional tests were accomplished in translating an excerpt of a 1739 letter from johann bernoulli to euler, where he announces that he was sending euler the first part of his manuscript hydraulica. overall, the comparative results show that chatgpt performed better that google translate not only in the benchmark tests but also in the translation of this letter, highlighting the superiority of chatgpt as a translation tool, catering not only to general latin practitioners but also proving beneficial for specialized latin translators.', 'doi': '', 'created': '2023-07-03', 'url': 'https://arxiv.org/abs/2307.07520', 'authors': ['sylvio r. bistafa']}, {'id': '2307.08523', 'title': 'generic bidirectional typing for dependent type theories', 'abstract': 'bidirectional typing is a discipline in which the typing judgment is decomposed explicitly into inference and checking modes, allowing to control the flow of type information in typing rules and to specify algorithmically how they should be used. bidirectional typing has been fruitfully studied and bidirectional systems have been developed for many type theories. however, the formal development of bidirectional typing has until now been kept confined to specific theories, with general guidelines remaining informal. in this work, we give a generic account of bidirectional typing for a general class of dependent type theories. this is done by first giving a general definition of type theories (or equivalently, a logical framework), for which we define declarative and bidirectional type systems. we then show, in a theory-independent fashion, that the two systems are equivalent. finally, we establish the decidability of bidirectional typing for normalizing theories, yielding a generic type-checking algorithm that has been implemented in a prototype and used in practice with many theories.', 'doi': '', 'created': '2023-07-17', 'url': 'https://arxiv.org/abs/2307.08523', 'authors': ['thiago felicissimo']}, {'id': '2307.12162', 'title': 'a refinement of expurgation', 'abstract': 'we show that for a wide range of channels and code ensembles with pairwise-independent codewords, with probability tending to 1 with the code length, expurgating an arbitrarily small fraction of codewords from a randomly selected code results in a code attaining the expurgated exponent.', 'doi': '10.1109/tit.2024.3388563', 'created': '2023-07-22', 'url': 'https://arxiv.org/abs/2307.12162', 'authors': ['giuseppe cocco', 'albert guillén i fàbregas', 'josep font-segura']}, {'id': '2307.12513', 'title': 'correcting matrix products over the ring of integers', 'abstract': 'let $a$, $b$, and $c$ be three $n\\\\times n$ matrices. we investigate the problem of verifying whether $ab=c$ over the ring of integers and finding the correct product $ab$. given that $c$ is different from $ab$ by at most $k$ entries, we propose an algorithm that uses $o(\\\\sqrt{k}n^2+k^2n)$ operations.   let $\\\\alpha$ be the largest absolute value of an entry in $a$, $b$, and $c$. the integers involved in the computation are of $o(n^3\\\\alpha^2)$.', 'doi': '10.1016/j.ipl.2024.106496', 'created': '2023-07-24', 'url': 'https://arxiv.org/abs/2307.12513', 'authors': ['yu-lun wu', 'hung-lung wang']}, {'id': '2307.15517', 'title': 'a dataflow compiler for efficient llm inference using custom   microscaling formats', 'abstract': 'model quantization represents both parameters (weights) and intermediate values (activations) in a more compact format, thereby directly reducing both computational and memory cost in hardware. the quantization of recent large language models (llms) faces challenges to achieve competitive memory density compared to other models such as convolutional neural networks, since values in llms require larger dynamic ranges.   current hardware can expedite computation for llms using compact numerical formats such as low-bitwidth integers or floating-point numbers. each has advantages: integer operations simplify circuit design, whereas floating-point calculations can enhance accuracy when a wider dynamic range is required. in this work, we seek an efficient data format that combines the best of both worlds: microscaling (mx) formats. mx formats are efficient data formats that achieve both large dynamic ranges and high memory density.   in this paper, we propose a compiler named mase for exploring mixed-precision mx formats on dataflow hardware accelerators for llm inference. our main contributions are twofold. first, we propose a novel orchestration abstraction to explore both software and hardware optimizations with new data formats. second, mase achieves llm inference at an average precision of 4-bits, with minimal to no accuracy degradation. to our knowledge, mase represents the first effort to harness fine-grain multi-precision mx formats in the design of llm hardware accelerators. over a range of llms and datasets, mase achieves an average improvement of 24% in $\\\\delta$ accuracy with an overhead of only 3% in energy efficiency compared to designs using 8-bit fixed-point numbers.', 'doi': '', 'created': '2023-07-28', 'url': 'https://arxiv.org/abs/2307.15517', 'authors': ['jianyi cheng', 'cheng zhang', 'zhewen yu', 'christos-savvas bouganis', 'george a. constantinides', 'yiren zhao']}, {'id': '2308.04725', 'title': 'self-supervised learning of rotation-invariant 3d point set features   using transformer and its self-distillation', 'abstract': 'invariance against rotations of 3d objects is an important property in analyzing 3d point set data. conventional 3d point set dnns having rotation invariance typically obtain accurate 3d shape features via supervised learning by using labeled 3d point sets as training samples. however, due to the rapid increase in 3d point set data and the high cost of labeling, a framework to learn rotation-invariant 3d shape features from numerous unlabeled 3d point sets is required. this paper proposes a novel self-supervised learning framework for acquiring accurate and rotation-invariant 3d point set features at object-level. our proposed lightweight dnn architecture decomposes an input 3d point set into multiple global-scale regions, called tokens, that preserve the spatial layout of partial shapes composing the 3d object. we employ a self-attention mechanism to refine the tokens and aggregate them into an expressive rotation-invariant feature per 3d point set. our dnn is effectively trained by using pseudo-labels generated by a self-distillation framework. to facilitate the learning of accurate features, we propose to combine multi-crop and cut-mix data augmentation techniques to diversify 3d point sets for training. through a comprehensive evaluation, we empirically demonstrate that, (1) existing rotation-invariant dnn architectures designed for supervised learning do not necessarily learn accurate 3d shape features under a self-supervised learning scenario, and (2) our proposed algorithm learns rotation-invariant 3d point set features that are more accurate than those learned by existing algorithms. code is available at https://github.com/takahikof/ript_sdmm', 'doi': '10.1016/j.cviu.2024.104025', 'created': '2023-08-09', 'url': 'https://arxiv.org/abs/2308.04725', 'authors': ['takahiko furuya', 'zhoujie chen', 'ryutarou ohbuchi', 'zhenzhong kuang']}, {'id': '2308.06979', 'title': 'the sound demixing challenge 2023 $\\\\unicode{x2013}$ music demixing track', 'abstract': \"this paper summarizes the music demixing (mdx) track of the sound demixing challenge (sdx'23). we provide a summary of the challenge setup and introduce the task of robust music source separation (mss), i.e., training mss models in the presence of errors in the training data. we propose a formalization of the errors that can occur in the design of a training dataset for mss systems and introduce two new datasets that simulate such errors: sdxdb23_labelnoise and sdxdb23_bleeding. we describe the methods that achieved the highest scores in the competition. moreover, we present a direct comparison with the previous edition of the challenge (the music demixing challenge 2021): the best performing system achieved an improvement of over 1.6db in signal-to-distortion ratio over the winner of the previous competition, when evaluated on mdxdb21. besides relying on the signal-to-distortion ratio as objective metric, we also performed a listening test with renowned producers and musicians to study the perceptual quality of the systems and report here the results. finally, we provide our insights into the organization of the competition and our prospects for future editions.\", 'doi': '10.5334/tismir.171', 'created': '2023-08-14', 'url': 'https://arxiv.org/abs/2308.06979', 'authors': ['giorgio fabbro', 'stefan uhlich', 'chieh-hsin lai', 'woosung choi', 'marco martínez-ramírez', 'weihsiang liao', 'igor gadelha', 'geraldo ramos', 'eddie hsu', 'hugo rodrigues', 'fabian-robert stöter', 'alexandre défossez', 'yi luo', 'jianwei yu', 'dipam chakraborty', 'sharada mohanty', 'roman solovyev', 'alexander stempkovskiy', 'tatiana habruseva', 'nabarun goswami', 'tatsuya harada', 'minseok kim', 'jun hyung lee', 'yuanliang dong', 'xinran zhang', 'jiafeng liu', 'yuki mitsufuji']}, {'id': '2308.08945', 'title': 'interpretable graph neural networks for tabular data', 'abstract': 'data in tabular format is frequently occurring in real-world applications. graph neural networks (gnns) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. however, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. we propose an approach, called ignnet (interpretable graph neural network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. a large-scale empirical investigation is presented, showing that ignnet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including xgboost, random forests, and tabnet. at the same time, the results show that the explanations obtained from ignnet are aligned with the true shapley values of the features without incurring any additional computational overhead.', 'doi': '', 'created': '2023-08-17', 'url': 'https://arxiv.org/abs/2308.08945', 'authors': ['amr alkhatib', 'sofiane ennadir', 'henrik boström', 'michalis vazirgiannis']}, {'id': '2308.09084', 'title': 'movepose: a high-performance human pose estimation algorithm on mobile   and edge devices', 'abstract': 'we present movepose, an optimized lightweight convolutional neural network designed specifically for real-time body pose estimation on cpu-based mobile devices. the current solutions do not provide satisfactory accuracy and speed for human posture estimation, and movepose addresses this gap. it aims to maintain real-time performance while improving the accuracy of human posture estimation for mobile devices. our movepose algorithm has attained an mean average precision (map) score of 68.0 on the coco \\\\cite{cocodata} validation dataset. the movepose algorithm displayed efficiency with a performance of 69+ frames per second (fps) when run on an intel i9-10920x cpu. additionally, it showcased an increased performance of 452+ fps on an nvidia rtx3090 gpu. on an android phone equipped with a snapdragon 8 + 4g processor, the fps reached above 11. to enhance accuracy, we incorporated three techniques: deconvolution, large kernel convolution, and coordinate classification methods. compared to basic upsampling, deconvolution is trainable, improves model capacity, and enhances the receptive field. large kernel convolution strengthens these properties at a decreased computational cost. in summary, movepose provides high accuracy and real-time performance, marking it a potential tool for a variety of applications, including those focused on mobile-side human posture estimation. the code and models for this algorithm will be made publicly accessible.', 'doi': '', 'created': '2023-08-17', 'url': 'https://arxiv.org/abs/2308.09084', 'authors': ['dongyang yu', 'haoyue zhang', 'ruisheng zhao', 'guoqi chen', 'wangpeng an', 'yanhong yang']}, {'id': '2308.10838', 'title': 'an impossibility result for markov chain monte carlo sampling from   micro-canonical bipartite graph ensembles', 'abstract': 'markov chain monte carlo (mcmc) algorithms are commonly used to sample from graph ensembles. two graphs are neighbors in the state space if one can be obtained from the other with only a few modifications, e.g., edge rewirings. for many common ensembles, e.g., those preserving the degree sequences of bipartite graphs, rewiring operations involving two edges are sufficient to create a fully-connected state space, and they can be performed efficiently. we show that, for ensembles of bipartite graphs with fixed degree sequences and number of butterflies (k2,2 bi-cliques), there is no universal constant c such that a rewiring of at most c edges at every step is sufficient for any such ensemble to be fully connected. our proof relies on an explicit construction of a family of pairs of graphs with the same degree sequences and number of butterflies, with each pair indexed by a natural c, and such that any sequence of rewiring operations transforming one graph into the other must include at least one rewiring operation involving at least c edges. whether rewiring these many edges is sufficient to guarantee the full connectivity of the state space of any such ensemble remains an open question. our result implies the impossibility of developing efficient, graph-agnostic, mcmc algorithms for these ensembles, as the necessity to rewire an impractically large number of edges may hinder taking a step on the state space.', 'doi': '', 'created': '2023-08-21', 'url': 'https://arxiv.org/abs/2308.10838', 'authors': ['giulia preti', 'gianmarco de francisci morales', 'matteo riondato']}, {'id': '2308.11098', 'title': 'on the interpretability of quantum neural networks', 'abstract': 'interpretability of artificial intelligence (ai) methods, particularly deep neural networks, is of great interest. this heightened focus stems from the widespread use of ai-backed systems. these systems, often relying on intricate neural architectures, can exhibit behavior that is challenging to explain and comprehend. the interpretability of such models is a crucial component of building trusted systems. many methods exist to approach this problem, but they do not apply straightforwardly to the quantum setting. here, we explore the interpretability of quantum neural networks using local model-agnostic interpretability measures commonly utilized for classical neural networks. following this analysis, we generalize a classical technique called lime, introducing q-lime, which produces explanations of quantum neural networks. a feature of our explanations is the delineation of the region in which data samples have been given a random label, likely subjects of inherently random quantum measurements. we view this as a step toward understanding how to build responsible and accountable quantum ai models.', 'doi': '', 'created': '2023-08-21', 'url': 'https://arxiv.org/abs/2308.11098', 'authors': ['lirandë pira', 'chris ferrie']}, {'id': '2308.12462', 'title': 'overcoming generic knowledge loss with selective parameter update', 'abstract': 'foundation models encompass an extensive knowledge base and offer remarkable transferability. however, this knowledge becomes outdated or insufficient over time. the challenge lies in continuously updating foundation models to accommodate novel information while retaining their original capabilities. leveraging the fact that foundation models have initial knowledge on various tasks and domains, we propose a novel approach that, instead of updating all parameters equally, localizes the updates to a sparse set of parameters relevant to the task being learned. we strike a balance between efficiency and new task performance, while maintaining the transferability and generalizability of foundation models. we extensively evaluate our method on foundational vision-language models with a diverse spectrum of continual learning tasks. our method achieves improvements on the accuracy of the newly learned tasks up to 7% while preserving the pretraining knowledge with a negligible decrease of 0.9% on a representative control set accuracy.', 'doi': '', 'created': '2023-08-23', 'url': 'https://arxiv.org/abs/2308.12462', 'authors': ['wenxuan zhang', 'paul janson', 'rahaf aljundi', 'mohamed elhoseiny']}, {'id': '2308.16042', 'title': 'optimal non-adaptive cell probe dictionaries and hashing', 'abstract': 'we present a simple and provably optimal non-adaptive cell probe data structure for the static dictionary problem. our data structure supports storing a set of n key-value pairs from [u]x[u] using s words of space and answering key lookup queries in t = o(lg(u/n)/ lg(s/n)) nonadaptive probes. this generalizes a solution to the membership problem (i.e., where no values are associated with keys) due to buhrman et al. we also present matching lower bounds for the non-adaptive static membership problem in the deterministic setting. our lower bound implies that both our dictionary algorithm and the preceding membership algorithm are optimal, and in particular that there is an inherent complexity gap in these problems between no adaptivity and one round of adaptivity (with which hashing-based algorithms solve these problems in constant time). using the ideas underlying our data structure, we also obtain the first implementation of a n-wise independent family of hash functions with optimal evaluation time in the cell probe model.', 'doi': '', 'created': '2023-08-30', 'url': 'https://arxiv.org/abs/2308.16042', 'authors': ['kasper green larsen', 'rasmus pagh', 'giuseppe persiano', 'toniann pitassi', 'kevin yeo', 'or zamir']}, {'id': '2309.00380', 'title': 'learning multi-modal generative models with permutation-invariant   encoders and tighter variational bounds', 'abstract': 'devising deep latent variable models for multi-modal data has been a long-standing theme in machine learning research. multi-modal variational autoencoders (vaes) have been a popular generative model class that learns latent representations that jointly explain multiple modalities. various objective functions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic considerations. to encode latent variables from different modality subsets, product-of-experts (poe) or mixture-of-experts (moe) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regarding their generative quality or consistency across multiple modalities. in this work, we consider a variational bound that can tightly approximate the data log-likelihood. we develop more flexible aggregation schemes that generalize poe or moe approaches by combining encoded features from different modalities based on permutation-invariant neural networks. our numerical experiments illustrate trade-offs for multi-modal variational bounds and various aggregation schemes. we show that tighter variational bounds and more flexible aggregation models can become beneficial when one wants to approximate the true joint distribution over observed modalities and latent variables in identifiable models.', 'doi': '', 'created': '2023-09-01', 'url': 'https://arxiv.org/abs/2309.00380', 'authors': ['marcel hirt', 'domenico campolo', 'victoria leong', 'juan-pablo ortega']}, {'id': '2309.02208', 'title': 'convergent finite difference schemes for stochastic transport equations', 'abstract': 'we present difference schemes for stochastic transport equations with low-regularity velocity fields. we establish $l^2$ stability and convergence of the difference approximations under conditions that are less strict than those required for deterministic transport equations. the $l^2$ estimate, crucial for the analysis, is obtained through a discrete duality argument and a comprehensive examination of a class of backward parabolic difference schemes.', 'doi': '', 'created': '2023-09-05', 'url': 'https://arxiv.org/abs/2309.02208', 'authors': ['ulrik s. fjordholm', 'kenneth h. karlsen', 'peter h. c. pang']}, {'id': '2309.02354', 'title': 'a lightweight and transferable design for robust lego manipulation', 'abstract': 'lego is a well-known platform for prototyping pixelized objects. however, robotic lego prototyping (i.e., manipulating lego bricks) is challenging due to the tight connections and accuracy requirements. this paper investigates safe and efficient robotic lego manipulation. in particular, this paper reduces the complexity of the manipulation by hardware-software co-design. an end-of-arm tool (eoat) is designed, which reduces the problem dimension and allows large industrial robots to manipulate small lego bricks. in addition, this paper uses evolution strategy to optimize the robot motion for lego manipulation. experiments demonstrate that the eoat can reliably manipulate lego bricks and the learning framework can effectively and safely improve the manipulation performance to a 100% success rate. the co-design is deployed to multiple robots (i.e., fanuc lr-mate 200id/7l and yaskawa gp4) to demonstrate its generalizability and transferability. in the end, we show that the proposed solution enables sustainable robotic lego prototyping, in which the robot can repeatedly assemble and disassemble different prototypes.', 'doi': '', 'created': '2023-09-05', 'url': 'https://arxiv.org/abs/2309.02354', 'authors': ['ruixuan liu', 'yifan sun', 'changliu liu']}, {'id': '2309.04001', 'title': 'mmsformer: multimodal transformer for material and semantic segmentation', 'abstract': 'leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. however, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. in this paper, we propose a novel fusion strategy that can effectively fuse information from different modality combinations. we also propose a new model named multi-modal segmentation transformer (mmsformer) that incorporates the proposed fusion strategy to perform multimodal material and semantic segmentation tasks. mmsformer outperforms current state-of-the-art models on three different datasets. as we begin with only one input modality, performance improves progressively as additional modalities are incorporated, showcasing the effectiveness of the fusion block in combining useful information from diverse input modalities. ablation studies show that different modules in the fusion block are crucial for overall model performance. furthermore, our ablation studies also highlight the capacity of different input modalities to improve performance in the identification of different types of materials. the code and pretrained models will be made available at https://github.com/csiplab/mmsformer.', 'doi': '10.1109/ojsp.2024.3389812', 'created': '2023-09-07', 'url': 'https://arxiv.org/abs/2309.04001', 'authors': ['md kaykobad reza', 'ashley prater-bennette', 'm. salman asif']}, {'id': '2309.05442', 'title': 'testing spreading behavior in networks with arbitrary topologies', 'abstract': 'inspired by the works of goldreich and ron (j. acm, 2017) and nakar and ron (icalp, 2021), we initiate the study of property testing in dynamic environments with arbitrary topologies. our focus is on the simplest non-trivial rule that can be tested, which corresponds to the 1-bp rule of bootstrap percolation and models a simple spreading behavior: every \"infected\" node stays infected forever, and each \"healthy\" node becomes infected if and only if it has at least one infected neighbor. we show various results for both the case where we test a single time step of evolution and where the evolution spans several time steps. in the first, we show that the worst-case query complexity is $o(\\\\delta/\\\\varepsilon)$ or $\\\\tilde{o}(\\\\sqrt{n}/\\\\varepsilon)$ (whichever is smaller), where $\\\\delta$ and $n$ are the maximum degree of a node and number of vertices, respectively, in the underlying graph, and we also show lower bounds for both one- and two-sided error testers that match our upper bounds up to $\\\\delta = o(\\\\sqrt{n})$ and $\\\\delta = o(n^{1/3})$, respectively. in the second setting of testing the environment over $t$ time steps, we show upper bounds of $o(\\\\delta^{t-1}/\\\\varepsilon t)$ and $\\\\tilde{o}(|e|/\\\\varepsilon t)$, where $e$ is the set of edges of the underlying graph. all of our algorithms are one-sided error, and all of them are also time-conforming and non-adaptive, with the single exception of the more complex $\\\\tilde{o}(\\\\sqrt{n}/\\\\varepsilon)$-query tester for the case $t = 2$.', 'doi': '10.4230/lipics.icalp.2024.40', 'created': '2023-09-11', 'url': 'https://arxiv.org/abs/2309.05442', 'authors': ['augusto modanese', 'yuichi yoshida']}, {'id': '2309.05927', 'title': 'frequency-aware masked autoencoders for multimodal pretraining on   biosignals', 'abstract': \"leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. however, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. to achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder ($\\\\texttt{bio}$fame) that learns to parameterize the representation of biosignals in the frequency space. $\\\\texttt{bio}$fame incorporates a frequency-aware transformer, which leverages a fixed-size fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. to maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. the resulting architecture effectively utilizes multimodal information during pretraining, and can be seamlessly adapted to diverse tasks and modalities at test time, regardless of input size and order. we evaluated our approach on a diverse set of transfer experiments on unimodal time series, achieving an average of $\\\\uparrow$5.5% improvement in classification accuracy over the previous state-of-the-art. furthermore, we demonstrated that our architecture is robust in modality mismatch scenarios, including unpredicted modality dropout or substitution, proving its practical utility in real-world applications. code is available at https://github.com/apple/ml-famae .\", 'doi': '', 'created': '2023-09-11', 'url': 'https://arxiv.org/abs/2309.05927', 'authors': ['ran liu', 'ellen l. zippi', 'hadi pouransari', 'chris sandino', 'jingping nie', 'hanlin goh', 'erdrin azemi', 'ali moin']}, {'id': '2309.06978', 'title': 'differentiable jpeg: the devil is in the details', 'abstract': 'jpeg remains one of the most widespread lossy image coding methods. however, the non-differentiable nature of jpeg restricts the application in deep learning pipelines. several differentiable approximations of jpeg have recently been proposed to address this issue. this paper conducts a comprehensive review of existing diff. jpeg approaches and identifies critical details that have been missed by previous methods. to this end, we propose a novel diff. jpeg approach, overcoming previous limitations. our approach is differentiable w.r.t. the input image, the jpeg quality, the quantization tables, and the color conversion parameters. we evaluate the forward and backward performance of our diff. jpeg approach against existing methods. additionally, extensive ablations are performed to evaluate crucial design choices. our proposed diff. jpeg resembles the (non-diff.) reference implementation best, significantly surpassing the recent-best diff. approach by $3.47$db (psnr) on average. for strong compression rates, we can even improve psnr by $9.51$db. strong adversarial attack results are yielded by our diff. jpeg, demonstrating the effective gradient approximation. our code is available at https://github.com/necla-ml/diff-jpeg.', 'doi': '10.1109/wacv57701.2024.00408', 'created': '2023-09-13', 'url': 'https://arxiv.org/abs/2309.06978', 'authors': ['christoph reich', 'biplob debnath', 'deep patel', 'srimat chakradhar']}, {'id': '2309.07918', 'title': 'unified human-scene interaction via prompted chain-of-contacts', 'abstract': 'human-scene interaction (hsi) is a vital component of fields like embodied ai and virtual reality. despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration before the practical application of hsi. this paper presents a unified hsi framework, unihsi, which supports unified control of diverse interactions through language commands. this framework is built upon the definition of interaction as chain of contacts (coc): steps of human joint-object part pairs, which is inspired by the strong correlation between interaction types and human-object contact regions. based on the definition, unihsi constitutes a large language model (llm) planner to translate language prompts into task plans in the form of coc, and a unified controller that turns coc into uniform task execution. to facilitate training and evaluation, we collect a new dataset named sceneplan that encompasses thousands of task plans generated by llms based on diverse scenarios. comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes. the project page is at https://github.com/openrobotlab/unihsi .', 'doi': '', 'created': '2023-09-14', 'url': 'https://arxiv.org/abs/2309.07918', 'authors': ['zeqi xiao', 'tai wang', 'jingbo wang', 'jinkun cao', 'wenwei zhang', 'bo dai', 'dahua lin', 'jiangmiao pang']}, {'id': '2309.12830', 'title': 'axocs: scaling fpga-based approximate operators using configuration   supersampling', 'abstract': 'the rising usage of ai and ml-based processing across application domains has exacerbated the need for low-cost ml implementation, specifically for resource-constrained embedded systems. to this end, approximate computing, an approach that explores the power, performance, area (ppa), and behavioral accuracy (behav) trade-offs, has emerged as a possible solution for implementing embedded machine learning. due to the predominance of mac operations in ml, designing platform-specific approximate arithmetic operators forms one of the major research problems in approximate computing. recently there has been a rising usage of ai/ml-based design space exploration techniques for implementing approximate operators. however, most of these approaches are limited to using ml-based surrogate functions for predicting the ppa and behav impact of a set of related design decisions. while this approach leverages the regression capabilities of ml methods, it does not exploit the more advanced approaches in ml. to this end, we propose axocs, a methodology for designing approximate arithmetic operators through ml-based supersampling. specifically, we present a method to leverage the correlation of ppa and behav metrics across operators of varying bit-widths for generating larger bit-width operators. the proposed approach involves traversing the relatively smaller design space of smaller bit-width operators and employing its associated design-ppa-behav relationship to generate initial solutions for metaheuristics-based optimization for larger operators. the experimental evaluation of axocs for fpga-optimized approximate operators shows that the proposed approach significantly improves the quality-resulting hypervolume for multi-objective optimization-of 8x8 signed approximate multipliers.', 'doi': '10.1109/tcsi.2024.3385333', 'created': '2023-09-22', 'url': 'https://arxiv.org/abs/2309.12830', 'authors': ['siva satyendra sahoo', 'salim ullah', 'soumyo bhattacharjee', 'akash kumar']}, {'id': '2309.15136', 'title': 'a multi-modal approach for identifying schizophrenia using cross-modal   attention', 'abstract': 'this study focuses on how different modalities of human communication can be used to distinguish between healthy controls and subjects with schizophrenia who exhibit strong positive symptoms. we developed a multi-modal schizophrenia classification system using audio, video, and text. facial action units and vocal tract variables were extracted as low-level features from video and audio respectively, which were then used to compute high-level coordination features that served as the inputs to the audio and video modalities. context-independent text embeddings extracted from transcriptions of speech were used as the input for the text modality. the multi-modal system is developed by fusing a segment-to-session-level classifier for video and audio modalities with a text model based on a hierarchical attention network (han) with cross-modal attention. the proposed multi-modal system outperforms the previous state-of-the-art multi-modal system by 8.53% in the weighted average f1 score.', 'doi': '', 'created': '2023-09-26', 'url': 'https://arxiv.org/abs/2309.15136', 'authors': ['gowtham premananth', 'yashish m. siriwardena', 'philip resnik', 'carol espy-wilson']}, {'id': '2309.16108', 'title': 'channel vision transformers: an image is worth 1 x 16 x 16 words', 'abstract': 'vision transformer (vit) has emerged as a powerful architecture in the realm of modern computer vision. however, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. in these domains, images often contain multiple channels, each carrying semantically distinct and independent information. furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. in this paper, we propose a modification to the vit architecture that enhances reasoning across the input channels and introduce hierarchical channel sampling (hcs) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. our proposed model, channelvit, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. we evaluate the performance of channelvit on imagenet, jump-cp (microscopy cell imaging), and so2sat (satellite imaging). our results show that channelvit outperforms vit on classification tasks and generalizes well, even when a subset of input channels is used during testing. across our experiments, hcs proves to be a powerful regularizer, independent of the architecture employed, suggesting itself as a straightforward technique for robust vit training. lastly, we find that channelvit generalizes effectively even when there is limited access to all channels during training, highlighting its potential for multi-channel imaging under real-world conditions with sparse sensors. our code is available at https://github.com/insitro/channelvit.', 'doi': '', 'created': '2023-09-27', 'url': 'https://arxiv.org/abs/2309.16108', 'authors': ['yujia bao', 'srinivasan sivanandan', 'theofanis karaletsos']}, {'id': '2310.00074', 'title': 'socreval: large language models with the socratic method for   reference-free reasoning evaluation', 'abstract': 'to comprehensively gauge the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. established reference-based evaluation metrics rely on human-annotated reasoning chains as references to assess the model-derived chains. however, such \"gold-standard\" human-written reasoning chains may not be unique and their acquisition is often labor-intensive. existing reference-free reasoning evaluation metrics, while eliminating the need for human-crafted reasoning chains as references, often require fine-tuning with human-derived chains before evaluation, complicating the process and questioning their adaptability to other datasets. to address these challenges, we harness gpt-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes. leveraging the socratic method, we develop socreval ({\\\\bf soc}ratic method-inspired {\\\\bf r}easoning {\\\\bf eval}uation), a novel approach for prompt design in reference-free reasoning evaluation. empirical results from four human annotated datasets reveal that socreval significantly improves gpt-4\\'s performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. beyond its demonstrated efficacy, socreval, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis.', 'doi': '', 'created': '2023-09-29', 'url': 'https://arxiv.org/abs/2310.00074', 'authors': ['hangfeng he', 'hongming zhang', 'dan roth']}, {'id': '2310.00132', 'title': 'qdformer: towards robust audiovisual segmentation in complex   environments with quantization-based semantic decomposition', 'abstract': 'audiovisual segmentation (avs) is a challenging task that aims to segment visual objects in videos according to their associated acoustic cues. with multiple sound sources and background disturbances involved, establishing robust correspondences between audio and visual contents poses unique challenges due to (1) complex entanglement across sound sources and (2) frequent changes in the occurrence of distinct sound events. assuming sound events occur independently, the multi-source semantic space can be represented as the cartesian product of single-source sub-spaces. we are motivated to decompose the multi-source audio semantics into single-source semantics for more effective interactions with visual content. we propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several disentangled and noise-suppressed single-source semantics. furthermore, we introduce a global-to-local quantization mechanism, which distills knowledge from stable global (clip-level) features into local (frame-level) ones, to handle frequent changes in audio semantics. extensive experiments demonstrate that our semantically decomposed audio representation significantly improves avs performance, e.g., +21.2% miou on the challenging avs-semantic benchmark with resnet50 backbone. https://github.com/lxa9867/qsd.', 'doi': '', 'created': '2023-09-29', 'url': 'https://arxiv.org/abs/2310.00132', 'authors': ['xiang li', 'jinglu wang', 'xiaohao xu', 'xiulian peng', 'rita singh', 'yan lu', 'bhiksha raj']}, {'id': '2310.01143', 'title': 'preliminary performance evaluation of a satellite-to-hap communication   link', 'abstract': 'the emergence of fifth-generation (5g) communication networks has brought forth unprecedented connectivity with ultra-low latency, high data rates, and pervasive coverage. however, meeting the increasing demands of applications for seamless and high-quality communication, especially in rural areas, requires exploring innovative solutions that expand 5g beyond traditional terrestrial networks. within the context of non-terrestrial networks (ntns), two promising technologies with vast potential are high altitude platforms (haps) and satellites. the combination of these two platforms is able to provide wide coverage and reliable communication in remote and inaccessible areas, and/or where terrestrial infrastructure is unavailable. this study evaluates the performance of the communication link between a geostationary equatorial orbit (geo) satellite and a hap using the internet of drones simulator (iod-sim), implemented in ns-3 and incorporating the 3gpp tr 38.811 channel model. the code base of iod-sim is extended to simulate haps, accounting for the earths curvature in various geographic coordinate systems, and considering realistic mobility patterns. a simulation campaign is conducted to evaluate the geo-to-hap communication link in terms of signal-to-noise ratio (snr) in two different scenarios, considering the mobility of the hap, and as a function of the frequency and the distance.', 'doi': '', 'created': '2023-10-02', 'url': 'https://arxiv.org/abs/2310.01143', 'authors': ['giovanni grieco', 'giovanni iacovelli', 'mattia sandri', 'marco giordani', 'michele zorzi', 'luigi alfredo grieco']}, {'id': '2310.01880', 'title': 'autocast++: enhancing world event prediction with zero-shot   ranking-based context retrieval', 'abstract': 'machine-based prediction of real-world events is garnering attention due to its potential for informed decision-making. whereas traditional forecasting predominantly hinges on structured data like time-series, recent breakthroughs in language models enable predictions using unstructured text. in particular, (zou et al., 2022) unveils autocast, a new benchmark that employs news articles for answering forecasting queries. nevertheless, existing methods still trail behind human performance. the cornerstone of accurate forecasting, we argue, lies in identifying a concise, yet rich subset of news snippets from a vast corpus. with this motivation, we introduce autocast++, a zero-shot ranking-based context retrieval system, tailored to sift through expansive news document collections for event forecasting. our approach first re-ranks articles based on zero-shot question-passage relevance, honing in on semantically pertinent news. following this, the chosen articles are subjected to zero-shot summarization to attain succinct context. leveraging a pre-trained language model, we conduct both the relevance evaluation and article summarization without needing domain-specific training. notably, recent articles can sometimes be at odds with preceding ones due to new facts or unanticipated incidents, leading to fluctuating temporal dynamics. to tackle this, our re-ranking mechanism gives preference to more recent articles, and we further regularize the multi-passage representation learning to align with human forecaster responses made on different dates. empirical results underscore marked improvements across multiple metrics, improving the performance for multiple-choice questions (mcq) by 48% and true/false (tf) questions by up to 8%. code is available at https://github.com/borealisai/autocast-plus-plus.', 'doi': '', 'created': '2023-10-03', 'url': 'https://arxiv.org/abs/2310.01880', 'authors': ['qi yan', 'raihan seraj', 'jiawei he', 'lili meng', 'tristan sylvain']}, {'id': '2310.01945', 'title': 'homotopy-aware multi-agent path planning in plane', 'abstract': 'we propose an efficient framework using the dynnikov coordinates for homotopy-aware multi-agent path planning in the plane. we developed a method to generate homotopically distinct solutions of multi-agent path planning problem in the plane by combining our framework with revised prioritized planning and proved its completeness in the grid world under specific assumptions. experimentally, we demonstrated the scalability of our method for the number of agents. we also confirmed the usefulness of homotopy-awareness by showing experimentally that generation of homotopically distinct solutions by our method contributes to planning low-cost trajectories for a swarm of agents.', 'doi': '', 'created': '2023-10-03', 'url': 'https://arxiv.org/abs/2310.01945', 'authors': ['kazumi kasaura']}, {'id': '2310.03624', 'title': 'high-degrees-of-freedom dynamic neural fields for robot self-modeling   and motion planning', 'abstract': \"a robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in the absence of a classical geometric kinematic model. in particular, when the latter is hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. in this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2d images annotated with camera poses and configurations. this enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. to this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (dofs). in a 7-dof robot test setup, the learned self-model achieves a chamfer-l2 distance of 2% of the robot's workspace dimension. we demonstrate the capabilities of this model on motion planning tasks as an exemplary downstream application.\", 'doi': '', 'created': '2023-10-05', 'url': 'https://arxiv.org/abs/2310.03624', 'authors': ['lennart schulze', 'hod lipson']}, {'id': '2310.05898', 'title': 'lion secretly solves constrained optimization: as lyapunov predicts', 'abstract': \"lion (evolved sign momentum), a new optimizer discovered through program search, has shown promising results in training large ai models. it performs comparably or favorably to adamw but with greater memory efficiency. as we can expect from the results of a random search program, lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, polak, and nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. thus, even though lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. this lack of theoretical clarity limits opportunities to further enhance and expand lion's efficacy.   this work aims to demystify lion. based on both continuous-time and discrete-time analysis, we demonstrate that lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint $\\\\|x\\\\|_\\\\infty \\\\leq 1/\\\\lambda$. lion achieves this through the incorporation of decoupled weight decay, where $\\\\lambda$ represents the weight decay coefficient. our analysis is made possible by the development of a new lyapunov function for the lion updates. it applies to a broader family of lion-$\\\\kappa$ algorithms, where the $\\\\text{sign}(\\\\cdot)$ operator in lion is replaced by the subgradient of a convex function $\\\\kappa$, leading to the solution of a general composite optimization problem of $\\\\min_x f(x) + \\\\kappa^*(x)$. our findings provide valuable insights into the dynamics of lion and pave the way for further improvements and extensions of lion-related algorithms.\", 'doi': '', 'created': '2023-10-09', 'url': 'https://arxiv.org/abs/2310.05898', 'authors': ['lizhang chen', 'bo liu', 'kaizhao liang', 'qiang liu']}, {'id': '2310.07446', 'title': 'position paper: an integrated perspective on data, metrics, and   methodology for deep time-series forecasting', 'abstract': 'deep time-series forecasting plays an integral role in numerous practical applications. however, existing research fall short by focusing narrowly on either neural architecture designs for long-term point forecasts or probabilistic models for short-term scenarios. by proposing a comprehensive framework, facilitated by a novel tool, probts, that integrates diverse data scenarios, evaluation metrics, and methodological focuses, we aim to transcend the limitations of current forecasting practices. rigorous experimentation uncovers pivotal insights, including the supreme importance of aligning forecasting methodologies with the unique characteristics of the data; the necessity of a broad spectrum of metrics for accurately assessing both point and distributional forecasts; and the challenges inherent in adapting existing forecasting methods to a wider range of scenarios. these findings not only challenge conventional approaches but also illuminate promising avenues for future research, suggesting a more nuanced and effective strategy for advancing the field of deep time-series forecasting.', 'doi': '', 'created': '2023-10-11', 'url': 'https://arxiv.org/abs/2310.07446', 'authors': ['jiawen zhang', 'xumeng wen', 'shun zheng', 'jia li', 'jiang bian']}, {'id': '2310.07983', 'title': 'revisiting decentralized proxskip: achieving linear speedup', 'abstract': 'the proxskip algorithm for decentralized and federated learning is gaining increasing attention due to its proven benefits in accelerating communication complexity while maintaining robustness against data heterogeneity. however, existing analyses of proxskip are limited to the strongly convex setting and do not achieve linear speedup, where convergence performance increases linearly with respect to the number of nodes. so far, questions remain open about how proxskip behaves in the non-convex setting and whether linear speedup is achievable.   in this paper, we revisit decentralized proxskip and address both questions. we demonstrate that the leading communication complexity of proxskip is $\\\\mathcal{o}\\\\left(\\\\frac{p\\\\sigma^2}{n\\\\epsilon^2}\\\\right)$ for non-convex and convex settings, and $\\\\mathcal{o}\\\\left(\\\\frac{p\\\\sigma^2}{n\\\\epsilon}\\\\right)$ for the strongly convex setting, where $n$ represents the number of nodes, $p$ denotes the probability of communication, $\\\\sigma^2$ signifies the level of stochastic noise, and $\\\\epsilon$ denotes the desired accuracy level. this result illustrates that proxskip achieves linear speedup and can asymptotically reduce communication overhead proportional to the probability of communication. additionally, for the strongly convex setting, we further prove that proxskip can achieve linear speedup with network-independent stepsizes.', 'doi': '', 'created': '2023-10-11', 'url': 'https://arxiv.org/abs/2310.07983', 'authors': ['luyao guo', 'sulaiman a. alghunaim', 'kun yuan', 'laurent condat', 'jinde cao']}, {'id': '2310.10404', 'title': 'llm4sgg: large language models for weakly supervised scene graph   generation', 'abstract': \"weakly-supervised scene graph generation (wssgg) research has recently emerged as an alternative to the fully-supervised approach that heavily relies on costly annotations. in this regard, studies on wssgg have utilized image captions to obtain unlocalized triplets while primarily focusing on grounding the unlocalized triplets over image regions. however, they have overlooked the two issues involved in the triplet formation process from the captions: 1) semantic over-simplification issue arises when extracting triplets from captions, where fine-grained predicates in captions are undesirably converted into coarse-grained predicates, resulting in a long-tailed predicate distribution, and 2) low-density scene graph issue arises when aligning the triplets in the caption with entity/predicate classes of interest, where many triplets are discarded and not used in training, leading to insufficient supervision. to tackle the two issues, we propose a new approach, i.e., large language model for weakly-supervised sgg (llm4sgg), where we mitigate the two issues by leveraging the llm's in-depth understanding of language and reasoning ability during the extraction of triplets from captions and alignment of entity/predicate classes with target data. to further engage the llm in these processes, we adopt the idea of chain-of-thought and the in-context few-shot learning strategy. to validate the effectiveness of llm4sgg, we conduct extensive experiments on visual genome and gqa datasets, showing significant improvements in both recall@k and mean recall@k compared to the state-of-the-art wssgg methods. a further appeal is that llm4sgg is data-efficient, enabling effective model training with a small amount of training images.\", 'doi': '', 'created': '2023-10-16', 'url': 'https://arxiv.org/abs/2310.10404', 'authors': ['kibum kim', 'kanghoon yoon', 'jaehyeong jeon', 'yeonjun in', 'jinyoung moon', 'donghyun kim', 'chanyoung park']}, {'id': '2310.12079', 'title': 'differential equation scaling limits of shaped and unshaped neural   networks', 'abstract': 'recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. however, these results do not a priori tell us anything about \"ordinary\" unshaped networks, where the activation is unchanged as the network size grows. in this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks.   firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected resnet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (mlp) with depth $d \\\\ll$ width $n$ and shaped relu activation at rate $d^{-1/2}$.   secondly, for an unshaped mlp at initialization, we derive the first order asymptotic correction to the layerwise correlation. in particular, if $\\\\rho_\\\\ell$ is the correlation at layer $\\\\ell$, then $q_t = \\\\ell^2 (1 - \\\\rho_\\\\ell)$ with $t = \\\\frac{\\\\ell}{n}$ converges to an sde with a singularity at $t=0$.   these results together provide a connection between shaped and unshaped network architectures, and opens up the possibility of studying the effect of normalization methods and how it connects with shaping activation functions.', 'doi': '', 'created': '2023-10-18', 'url': 'https://arxiv.org/abs/2310.12079', 'authors': ['mufan bill li', 'mihai nica']}, {'id': '2310.14724', 'title': 'a survey on llm-generated text detection: necessity, methods, and future   directions', 'abstract': 'the powerful ability to understand, follow, and generate complex language emerging from large language models (llms) makes llm-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. as llms continue to expand, there is an imperative need to develop detectors that can detect llm-generated text. this is crucial to mitigate potential misuse of llms and safeguard realms like artistic expression and social networks from harmful influence of llm-generated content. the llm-generated text detection aims to discern if a piece of text was produced by an llm, which is essentially a binary classification task. the detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, statistics-based detectors, neural-base detectors, and human-assisted methods. in this survey, we collate recent research breakthroughs in this area and underscore the pressing need to bolster detector research. we also delve into prevalent datasets, elucidating their limitations and developmental requirements. furthermore, we analyze various llm-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, real-world data issues and the lack of effective evaluation framework. conclusively, we highlight interesting directions for future research in llm-generated text detection to advance the implementation of responsible artificial intelligence (ai). our aim with this survey is to provide a clear and comprehensive introduction for newcomers while also offering seasoned researchers a valuable update in the field of llm-generated text detection. the useful resources are publicly available at: https://github.com/nlp2ct/llm-generated-text-detection.', 'doi': '', 'created': '2023-10-23', 'url': 'https://arxiv.org/abs/2310.14724', 'authors': ['junchao wu', 'shu yang', 'runzhe zhan', 'yulin yuan', 'derek f. wong', 'lidia s. chao']}, {'id': '2310.17304', 'title': 'static semantics reconstruction for enhancing javascript-webassembly   multilingual malware detection', 'abstract': 'the emergence of webassembly allows attackers to hide the malicious functionalities of javascript malware in cross-language interoperations, termed javascript-webassembly multilingual malware (jwmm). however, existing anti-virus solutions based on static program analysis are still limited to monolingual code. as a result, their detection effectiveness decreases significantly against jwmm. the detection of jwmm is challenging due to the complex interoperations and semantic diversity between javascript and webassembly. to bridge this gap, we present jwbinder, the first technique aimed at enhancing the static detection of jwmm. jwbinder performs a language-specific data-flow analysis to capture the cross-language interoperations and then characterizes the functionalities of jwmm through a unified high-level structure called inter-language program dependency graph. the extensive evaluation on one of the most representative real-world anti-virus platforms, virustotal, shows that \\\\system effectively enhances anti-virus systems from various vendors and increases the overall successful detection rate against jwmm from 49.1\\\\% to 86.2\\\\%. additionally, we assess the side effects and runtime overhead of jwbinder, corroborating its practical viability in real-world applications.', 'doi': '', 'created': '2023-10-26', 'url': 'https://arxiv.org/abs/2310.17304', 'authors': ['yifan xia', 'ping he', 'xuhong zhang', 'peiyu liu', 'shouling ji', 'wenhai wang']}, {'id': '2310.18784', 'title': 'high-probability convergence bounds for nonlinear stochastic gradient   descent under heavy-tailed noise', 'abstract': 'we study high-probability convergence guarantees of learning on streaming data in the presence of heavy-tailed noise. in the proposed scenario, the model is updated in an online fashion, as new information is observed, without storing any additional data. to combat the heavy-tailed noise, we consider a general framework of nonlinear stochastic gradient descent (sgd), providing several strong results. first, for non-convex costs and component-wise nonlinearities, we establish a convergence rate arbitrarily close to $\\\\mathcal{o}\\\\left(t^{-\\\\frac{1}{4}}\\\\right)$, whose exponent is independent of noise and problem parameters. second, for strongly convex costs and a broader class of nonlinearities, we establish convergence of the last iterate to the optimum, with a rate $\\\\mathcal{o}\\\\left(t^{-\\\\zeta} \\\\right)$, where $\\\\zeta \\\\in (0,1)$ depends on problem parameters, noise and nonlinearity. as we show analytically and numerically, $\\\\zeta$ can be used to inform the preferred choice of nonlinearity for given problem settings. compared to state-of-the-art, who only consider clipping, require bounded noise moments of order $\\\\eta \\\\in (1,2]$, and establish convergence rates whose exponents go to zero as $\\\\eta \\\\rightarrow 1$, we provide high-probability guarantees for a much broader class of nonlinearities and symmetric density noise, with convergence rates whose exponents are bounded away from zero, even when the noise has finite first moment only. moreover, in the case of strongly convex functions, we demonstrate analytically and numerically that clipping is not always the optimal nonlinearity, further underlining the value of our general framework.', 'doi': '', 'created': '2023-10-28', 'url': 'https://arxiv.org/abs/2310.18784', 'authors': ['aleksandar armacki', 'pranay sharma', 'gauri joshi', 'dragana bajovic', 'dusan jakovetic', 'soummya kar']}, {'id': '2310.19454', 'title': 'mmm and mmmsynth: clustering of heterogeneous tabular data, and   synthetic data generation', 'abstract': \"we provide new algorithms for two tasks relating to heterogeneous tabular datasets: clustering, and synthetic data generation. tabular datasets typically consist of heterogeneous data types (numerical, ordinal, categorical) in columns, but may also have hidden cluster structure in their rows: for example, they may be drawn from heterogeneous (geographical, socioeconomic, methodological) sources, such that the outcome variable they describe (such as the presence of a disease) may depend not only on the other variables but on the cluster context. moreover, sharing of biomedical data is often hindered by patient confidentiality laws, and there is current interest in algorithms to generate synthetic tabular data from real data, for example via deep learning.   we demonstrate a novel em-based clustering algorithm, mmm (``madras mixture model''), that outperforms standard algorithms in determining clusters in synthetic heterogeneous data, and recovers structure in real data. based on this, we demonstrate a synthetic tabular data generation algorithm, mmmsynth, that pre-clusters the input data, and generates cluster-wise synthetic data assuming cluster-specific data distributions for the input columns. we benchmark this algorithm by testing the performance of standard ml algorithms when they are trained on synthetic data and tested on real published datasets. our synthetic data generation algorithm outperforms other literature tabular-data generators, and approaches the performance of training purely with real data.\", 'doi': '10.1371/journal.pone.0302271', 'created': '2023-10-30', 'url': 'https://arxiv.org/abs/2310.19454', 'authors': ['chandrani kumari', 'rahul siddharthan']}, {'id': '2311.00944', 'title': 'stochastic smoothed gradient descent ascent for federated minimax   optimization', 'abstract': 'in recent years, federated minimax optimization has attracted growing interest due to its extensive applications in various machine learning tasks. while smoothed alternative gradient descent ascent (smoothed-agda) has proved its success in centralized nonconvex minimax optimization, how and whether smoothing technique could be helpful in federated setting remains unexplored. in this paper, we propose a new algorithm termed federated stochastic smoothed gradient descent ascent (fess-gda), which utilizes the smoothing technique for federated minimax optimization. we prove that fess-gda can be uniformly used to solve several classes of federated minimax problems and prove new or better analytical convergence results for these settings. we showcase the practical efficiency of fess-gda in practical federated learning tasks of training generative adversarial networks (gans) and fair classification.', 'doi': '', 'created': '2023-11-01', 'url': 'https://arxiv.org/abs/2311.00944', 'authors': ['wei shen', 'minhui huang', 'jiawei zhang', 'cong shen']}, {'id': '2311.02909', 'title': 'distributed matrix-based sampling for graph neural network training', 'abstract': 'graph neural networks (gnns) offer a compact and computationally efficient way to learn embeddings and classifications on graph data. gnn models are frequently large, making distributed minibatch training necessary.   the primary contribution of this paper is new methods for reducing communication in the sampling step for distributed gnn training. here, we propose a matrix-based bulk sampling approach that expresses sampling as a sparse matrix multiplication (spgemm) and samples multiple minibatches at once. when the input graph topology does not fit on a single device, our method distributes the graph and use communication-avoiding spgemm algorithms to scale gnn minibatch sampling, enabling gnn training on much larger graphs than those that can fit into a single device memory. when the input graph topology (but not the embeddings) fits in the memory of one gpu, our approach (1) performs sampling without communication, (2) amortizes the overheads of sampling a minibatch, and (3) can represent multiple sampling algorithms by simply using different matrix constructions. in addition to new methods for sampling, we introduce a pipeline that uses our matrix-based bulk sampling approach to provide end-to-end training results. we provide experimental results on the largest open graph benchmark (ogb) datasets on $128$ gpus, and show that our pipeline is $2.5\\\\times$ faster than quiver (a distributed extension to pytorch-geometric) on a $3$-layer graphsage network. on datasets outside of ogb, we show a $8.46\\\\times$ speedup on $128$ gpus in per-epoch time. finally, we show scaling when the graph is distributed across gpus and scaling for both node-wise and layer-wise sampling algorithms.', 'doi': '', 'created': '2023-11-06', 'url': 'https://arxiv.org/abs/2311.02909', 'authors': ['alok tripathy', 'katherine yelick', 'aydin buluc']}, {'id': '2311.03056', 'title': 'litsumm: large language models for literature summarisation of   non-coding rnas', 'abstract': 'motivation: curation of literature in life sciences is a growing challenge. the continued increase in the rate of publication, coupled with the relatively fixed number of curators worldwide presents a major challenge to developers of biomedical knowledgebases. very few knowledgebases have resources to scale to the whole relevant literature and all have to prioritise their efforts.   results: in this work, we take a first step to alleviating the lack of curator time in rna science by generating summaries of literature for non-coding rnas using large language models (llms). we demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial llm and a chain of prompts and checks. manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality. we also applied the most commonly used automated evaluation approaches, finding that they do not correlate with human assessment. finally, we apply our tool to a selection of over 4,600 ncrnas and make the generated summaries available via the rnacentral resource. we conclude that automated literature summarization is feasible with the current generation of llms, provided careful prompting and automated checking are applied.   availability: code used to produce these summaries can be found here: https://github.com/rnacentral/litscan-summarization and the dataset of contexts and summaries can be found here: https://huggingface.co/datasets/rnacentral/litsumm-v1. summaries are also displayed on the rna report pages in rnacentral (https://rnacentral.org/)', 'doi': '', 'created': '2023-11-06', 'url': 'https://arxiv.org/abs/2311.03056', 'authors': ['andrew green', 'carlos ribas', 'nancy ontiveros-palacios', 'sam griffiths-jones', 'anton i. petrov', 'alex bateman', 'blake sweeney']}, {'id': '2311.04205', 'title': 'rephrase and respond: let large language models ask better questions for   themselves', 'abstract': \"misunderstandings arise not only in interpersonal communication but also between humans and large language models (llms). such discrepancies can make llms interpret seemingly unambiguous questions in unexpected ways, yielding incorrect responses. while it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by llms, a systematic method for crafting questions that llms can better comprehend is still underdeveloped. in this paper, we present a method named `rephrase and respond' (rar), which allows llms to rephrase and expand questions posed by humans and provide responses in a single prompt. this approach serves as a simple yet effective prompting method for improving performance. we also introduce a two-step variant of rar, where a rephrasing llm first rephrases the question and then passes the original and rephrased questions together to a different responding llm. this facilitates the effective utilization of rephrased questions generated by one llm with another. our experiments demonstrate that our methods significantly improve the performance of different models across a wide range to tasks. we further provide a comprehensive comparison between rar and the popular chain-of-thought (cot) methods, both theoretically and empirically. we show that rar is complementary to cot and can be combined with cot to achieve even better performance. our work not only contributes to enhancing llm performance efficiently and effectively but also sheds light on a fair evaluation of llm capabilities. data and codes are available at https://github.com/uclaml/rephrase-and-respond.\", 'doi': '', 'created': '2023-11-07', 'url': 'https://arxiv.org/abs/2311.04205', 'authors': ['yihe deng', 'weitong zhang', 'zixiang chen', 'quanquan gu']}, {'id': '2311.04253', 'title': 'blind federated learning via over-the-air q-qam', 'abstract': 'in this work, we investigate federated edge learning over a fading multiple access channel. to alleviate the communication burden between the edge devices and the access point, we introduce a pioneering digital over-the-air computation strategy employing q-ary quadrature amplitude modulation, culminating in a low latency communication scheme. indeed, we propose a new federated edge learning framework in which edge devices use digital modulation for over-the-air uplink transmission to the edge server while they have no access to the channel state information. furthermore, we incorporate multiple antennas at the edge server to overcome the fading inherent in wireless communication. we analyze the number of antennas required to mitigate the fading impact effectively. we prove a non-asymptotic upper bound for the mean squared error for the proposed federated learning with digital over-the-air uplink transmissions under both noisy and fading conditions. leveraging the derived upper bound, we characterize the convergence rate of the learning process of a non-convex loss function in terms of the mean square error of gradients due to the fading channel. furthermore, we substantiate the theoretical assurances through numerical experiments concerning mean square error and the convergence efficacy of the digital federated edge learning framework. notably, the results demonstrate that augmenting the number of antennas at the edge server and adopting higher-order modulations improve the model accuracy up to 60\\\\%.', 'doi': '', 'created': '2023-11-07', 'url': 'https://arxiv.org/abs/2311.04253', 'authors': ['saeed razavikia', 'josé mairton barros da silva júnior', 'carlo fischione']}, {'id': '2311.05734', 'title': 'cut-set and stability constrained optimal power flow for resilient   operation during wildfires', 'abstract': 'resilient operation of the power system during ongoing wildfires is challenging because of the uncertain ways in which the fires impact the electric power infrastructure (multiple arc-faults, complete melt-down). to address this challenge, we propose a novel cut-set and stability-constrained optimal power flow (opf) that quickly mitigates both static and dynamic insecurities as wildfires progress through a region. first, a feasibility test (ft) algorithm that quickly desaturates overloaded cut-sets to prevent cascading line outages is integrated with the opf problem. then, the resulting formulation is combined with a data-driven transient stability analyzer that predicts the correction factors for eliminating dynamic insecurities. the proposed model considers the possibility of generation rescheduling as well as load shed. the results obtained using the ieee 118-bus system indicate that the proposed approach alleviates vulnerability of the system to wildfires while minimizing operational cost.', 'doi': '', 'created': '2023-11-09', 'url': 'https://arxiv.org/abs/2311.05734', 'authors': ['satyaprajna sahoo', 'anamitra pal']}, {'id': '2311.08097', 'title': 'empowering multi-step reasoning across languages via tree-of-thoughts', 'abstract': 'reasoning methods, best exemplified by the well-known chain-of-thought (cot), empower the reasoning abilities of large language models (llms) by eliciting them to solve complex tasks in a step-by-step manner. although they are achieving significant success, the ability to deliver multi-step reasoning remains limited to english because of the imbalance in the distribution of pre-training data, which makes other languages a barrier. in this paper, we propose cross-lingual tree-of-thoughts (cross-tot), a method for aligning cross-lingual cot reasoning across languages. the proposed method, through a self-consistent cross-lingual prompting mechanism inspired by the tree-of-thoughts approach, provides multi-step reasoning paths in different languages that, during the steps, lead to the final solution. experimental evaluations show that our method significantly outperforms existing prompting methods by reducing the number of interactions and achieving state-of-the-art performance.', 'doi': '', 'created': '2023-11-14', 'url': 'https://arxiv.org/abs/2311.08097', 'authors': ['leonardo ranaldi', 'giulia pucci', 'federico ranaldi', 'elena sofia ruzzetti', 'fabio massimo zanzotto']}, {'id': '2311.08990', 'title': 'squlearn -- a python library for quantum machine learning', 'abstract': \"squlearn introduces a user-friendly, nisq-ready python library for quantum machine learning (qml), designed for seamless integration with classical machine learning tools like scikit-learn. the library's dual-layer architecture serves both qml researchers and practitioners, enabling efficient prototyping, experimentation, and pipelining. squlearn provides a comprehensive toolset that includes both quantum kernel methods and quantum neural networks, along with features like customizable data encoding strategies, automated execution handling, and specialized kernel regularization techniques. by focusing on nisq-compatibility and end-to-end automation, squlearn aims to bridge the gap between current quantum computing capabilities and practical machine learning applications. the library provides substantial flexibility, enabling quick transitions between the underlying quantum frameworks qiskit and pennylane, as well as between simulation and running on actual hardware.\", 'doi': '', 'created': '2023-11-15', 'url': 'https://arxiv.org/abs/2311.08990', 'authors': ['david a. kreplin', 'moritz willmann', 'jan schnabel', 'frederic rapp', 'manuel hagelüken', 'marco roth']}, {'id': '2311.09049', 'title': 'adapting large language models by integrating collaborative semantics   for recommendation', 'abstract': \"recently, large language models (llms) have shown great potential in recommender systems, either improving existing recommendation models or serving as the backbone. however, there exists a large semantic gap between llms and recommender systems, since items to be recommended are often indexed by discrete identifiers (item id) out of the llm's vocabulary. in essence, llms capture language semantics while recommender systems imply collaborative semantics, making it difficult to sufficiently leverage the model capacity of llms for recommendation. to address this challenge, in this paper, we propose a new llm-based recommendation model called lc-rec, which can better integrate language and collaborative semantics for recommender systems. our approach can directly generate items from the entire item set for recommendation, without relying on candidate items. specifically, we make two major contributions in our approach. for item indexing, we design a learning-based vector quantization method with uniform semantic mapping, which can assign meaningful and non-conflicting ids (called item indices) for items. for alignment tuning, we propose a series of specially designed tuning tasks to enhance the integration of collaborative semantics in llms. our fine-tuning tasks enforce llms to deeply integrate language and collaborative semantics (characterized by the learned item indices), so as to achieve an effective adaptation to recommender systems. extensive experiments demonstrate the effectiveness of our method, showing that our approach can outperform a number of competitive baselines including traditional recommenders and existing llm-based recommenders. our code is available at https://github.com/rucaibox/lc-rec/.\", 'doi': '', 'created': '2023-11-15', 'url': 'https://arxiv.org/abs/2311.09049', 'authors': ['bowen zheng', 'yupeng hou', 'hongyu lu', 'yu chen', 'wayne xin zhao', 'ming chen', 'ji-rong wen']}]\n",
      "Data type: <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 15:46:28,776 - INFO - Generated bibcodes for arXiv ID 2304.09779: 2023arXiv230409779S, 2023arXiv2304.09779S\n",
      "2024-04-22 15:46:28,777 - WARNING - No references found for arXiv ID: 2304.09779\n",
      "2024-04-22 15:46:28,784 - INFO - Generated bibcodes for arXiv ID 2304.10286: 2023arXiv230410286P, 2023arXiv2304.10286P\n",
      "2024-04-22 15:46:28,785 - WARNING - No references found for arXiv ID: 2304.10286\n",
      "2024-04-22 15:46:28,785 - INFO - Generated bibcodes for arXiv ID 2304.13029: 2023arXiv230413029M, 2023arXiv2304.13029M\n",
      "2024-04-22 15:46:28,788 - WARNING - No references found for arXiv ID: 2304.13029\n",
      "2024-04-22 15:46:28,789 - INFO - Generated bibcodes for arXiv ID 2305.03803: 2023arXiv230503803H, 2023arXiv2305.03803H\n",
      "2024-04-22 15:46:28,790 - WARNING - No references found for arXiv ID: 2305.03803\n",
      "2024-04-22 15:46:28,791 - INFO - Generated bibcodes for arXiv ID 2305.07877: 2023arXiv230507877G, 2023arXiv2305.07877G\n",
      "2024-04-22 15:46:28,793 - WARNING - No references found for arXiv ID: 2305.07877\n",
      "2024-04-22 15:46:28,794 - INFO - Generated bibcodes for arXiv ID 2305.18453: 2023arXiv230518453D, 2023arXiv2305.18453D\n",
      "2024-04-22 15:46:28,795 - WARNING - No references found for arXiv ID: 2305.18453\n",
      "2024-04-22 15:46:28,795 - INFO - Generated bibcodes for arXiv ID 2306.03027: 2023arXiv230603027G, 2023arXiv2306.03027G\n",
      "2024-04-22 15:46:28,796 - WARNING - No references found for arXiv ID: 2306.03027\n",
      "2024-04-22 15:46:28,797 - INFO - Generated bibcodes for arXiv ID 2306.06449: 2023arXiv230606449B, 2023arXiv2306.06449B\n",
      "2024-04-22 15:46:28,797 - WARNING - No references found for arXiv ID: 2306.06449\n",
      "2024-04-22 15:46:28,801 - INFO - Generated bibcodes for arXiv ID 2306.08386: 2023arXiv230608386L, 2023arXiv2306.08386L\n",
      "2024-04-22 15:46:28,802 - WARNING - No references found for arXiv ID: 2306.08386\n",
      "2024-04-22 15:46:28,803 - INFO - Generated bibcodes for arXiv ID 2306.08553: 2023arXiv230608553J, 2023arXiv2306.08553J\n",
      "2024-04-22 15:46:28,806 - WARNING - No references found for arXiv ID: 2306.08553\n",
      "2024-04-22 15:46:28,807 - INFO - Generated bibcodes for arXiv ID 2306.10091: 2023arXiv230610091P, 2023arXiv2306.10091P\n",
      "2024-04-22 15:46:28,808 - WARNING - No references found for arXiv ID: 2306.10091\n",
      "2024-04-22 15:46:28,809 - INFO - Generated bibcodes for arXiv ID 2306.17808: 2023arXiv230617808D, 2023arXiv2306.17808D\n",
      "2024-04-22 15:46:28,810 - WARNING - No references found for arXiv ID: 2306.17808\n",
      "2024-04-22 15:46:28,811 - INFO - Generated bibcodes for arXiv ID 2307.01004: 2023arXiv230701004Y, 2023arXiv2307.01004Y\n",
      "2024-04-22 15:46:28,811 - WARNING - No references found for arXiv ID: 2307.01004\n",
      "2024-04-22 15:46:28,813 - INFO - Generated bibcodes for arXiv ID 2307.02620: 2023arXiv230702620B, 2023arXiv2307.02620B\n",
      "2024-04-22 15:46:28,813 - WARNING - No references found for arXiv ID: 2307.02620\n",
      "2024-04-22 15:46:28,814 - INFO - Generated bibcodes for arXiv ID 2307.02913: 2023arXiv230702913P, 2023arXiv2307.02913P\n",
      "2024-04-22 15:46:28,815 - WARNING - No references found for arXiv ID: 2307.02913\n",
      "2024-04-22 15:46:28,816 - INFO - Generated bibcodes for arXiv ID 2307.06917: 2023arXiv230706917M, 2023arXiv2307.06917M\n",
      "2024-04-22 15:46:28,818 - WARNING - No references found for arXiv ID: 2307.06917\n",
      "2024-04-22 15:46:28,820 - INFO - Generated bibcodes for arXiv ID 2307.07520: 2023arXiv230707520B, 2023arXiv2307.07520B\n",
      "2024-04-22 15:46:28,822 - WARNING - No references found for arXiv ID: 2307.07520\n",
      "2024-04-22 15:46:28,822 - INFO - Generated bibcodes for arXiv ID 2307.08523: 2023arXiv230708523F, 2023arXiv2307.08523F\n",
      "2024-04-22 15:46:28,823 - WARNING - No references found for arXiv ID: 2307.08523\n",
      "2024-04-22 15:46:28,824 - INFO - Generated bibcodes for arXiv ID 2307.12162: 2023arXiv230712162C, 2023arXiv2307.12162C\n",
      "2024-04-22 15:46:28,825 - WARNING - No references found for arXiv ID: 2307.12162\n",
      "2024-04-22 15:46:28,826 - INFO - Generated bibcodes for arXiv ID 2307.12513: 2023arXiv230712513W, 2023arXiv2307.12513W\n",
      "2024-04-22 15:46:28,830 - WARNING - No references found for arXiv ID: 2307.12513\n",
      "2024-04-22 15:46:28,831 - INFO - Generated bibcodes for arXiv ID 2307.15517: 2023arXiv230715517C, 2023arXiv2307.15517C\n",
      "2024-04-22 15:46:28,832 - WARNING - No references found for arXiv ID: 2307.15517\n",
      "2024-04-22 15:46:28,833 - INFO - Generated bibcodes for arXiv ID 2308.04725: 2023arXiv230804725F, 2023arXiv2308.04725F\n",
      "2024-04-22 15:46:28,835 - WARNING - No references found for arXiv ID: 2308.04725\n",
      "2024-04-22 15:46:28,836 - INFO - Generated bibcodes for arXiv ID 2308.06979: 2023arXiv230806979F, 2023arXiv2308.06979F\n",
      "2024-04-22 15:46:28,837 - WARNING - No references found for arXiv ID: 2308.06979\n",
      "2024-04-22 15:46:28,837 - INFO - Generated bibcodes for arXiv ID 2308.08945: 2023arXiv230808945A, 2023arXiv2308.08945A\n",
      "2024-04-22 15:46:28,839 - WARNING - No references found for arXiv ID: 2308.08945\n",
      "2024-04-22 15:46:28,840 - INFO - Generated bibcodes for arXiv ID 2308.09084: 2023arXiv230809084Y, 2023arXiv2308.09084Y\n",
      "2024-04-22 15:46:28,840 - WARNING - No references found for arXiv ID: 2308.09084\n",
      "2024-04-22 15:46:28,841 - INFO - Generated bibcodes for arXiv ID 2308.10838: 2023arXiv230810838P, 2023arXiv2308.10838P\n",
      "2024-04-22 15:46:28,842 - WARNING - No references found for arXiv ID: 2308.10838\n",
      "2024-04-22 15:46:28,842 - INFO - Generated bibcodes for arXiv ID 2308.11098: 2023arXiv230811098P, 2023arXiv2308.11098P\n",
      "2024-04-22 15:46:28,843 - WARNING - No references found for arXiv ID: 2308.11098\n",
      "2024-04-22 15:46:28,844 - INFO - Generated bibcodes for arXiv ID 2308.12462: 2023arXiv230812462Z, 2023arXiv2308.12462Z\n",
      "2024-04-22 15:46:28,844 - WARNING - No references found for arXiv ID: 2308.12462\n",
      "2024-04-22 15:46:28,847 - INFO - Generated bibcodes for arXiv ID 2308.16042: 2023arXiv230816042L, 2023arXiv2308.16042L\n",
      "2024-04-22 15:46:28,847 - WARNING - No references found for arXiv ID: 2308.16042\n",
      "2024-04-22 15:46:28,849 - INFO - Generated bibcodes for arXiv ID 2309.00380: 2023arXiv230900380H, 2023arXiv2309.00380H\n",
      "2024-04-22 15:46:28,849 - WARNING - No references found for arXiv ID: 2309.00380\n",
      "2024-04-22 15:46:28,851 - INFO - Generated bibcodes for arXiv ID 2309.02208: 2023arXiv230902208F, 2023arXiv2309.02208F\n",
      "2024-04-22 15:46:28,852 - WARNING - No references found for arXiv ID: 2309.02208\n",
      "2024-04-22 15:46:28,853 - INFO - Generated bibcodes for arXiv ID 2309.02354: 2023arXiv230902354L, 2023arXiv2309.02354L\n",
      "2024-04-22 15:46:28,853 - WARNING - No references found for arXiv ID: 2309.02354\n",
      "2024-04-22 15:46:28,854 - INFO - Generated bibcodes for arXiv ID 2309.04001: 2023arXiv230904001R, 2023arXiv2309.04001R\n",
      "2024-04-22 15:46:28,854 - WARNING - No references found for arXiv ID: 2309.04001\n",
      "2024-04-22 15:46:28,855 - INFO - Generated bibcodes for arXiv ID 2309.05442: 2023arXiv230905442M, 2023arXiv2309.05442M\n",
      "2024-04-22 15:46:28,855 - WARNING - No references found for arXiv ID: 2309.05442\n",
      "2024-04-22 15:46:28,856 - INFO - Generated bibcodes for arXiv ID 2309.05927: 2023arXiv230905927L, 2023arXiv2309.05927L\n",
      "2024-04-22 15:46:28,856 - WARNING - No references found for arXiv ID: 2309.05927\n",
      "2024-04-22 15:46:28,857 - INFO - Generated bibcodes for arXiv ID 2309.06978: 2023arXiv230906978R, 2023arXiv2309.06978R\n",
      "2024-04-22 15:46:28,858 - WARNING - No references found for arXiv ID: 2309.06978\n",
      "2024-04-22 15:46:28,858 - INFO - Generated bibcodes for arXiv ID 2309.07918: 2023arXiv230907918X, 2023arXiv2309.07918X\n",
      "2024-04-22 15:46:28,859 - WARNING - No references found for arXiv ID: 2309.07918\n",
      "2024-04-22 15:46:28,860 - INFO - Generated bibcodes for arXiv ID 2309.12830: 2023arXiv230912830S, 2023arXiv2309.12830S\n",
      "2024-04-22 15:46:28,860 - WARNING - No references found for arXiv ID: 2309.12830\n",
      "2024-04-22 15:46:28,861 - INFO - Generated bibcodes for arXiv ID 2309.15136: 2023arXiv230915136P, 2023arXiv2309.15136P\n",
      "2024-04-22 15:46:28,862 - WARNING - No references found for arXiv ID: 2309.15136\n",
      "2024-04-22 15:46:28,862 - INFO - Generated bibcodes for arXiv ID 2309.16108: 2023arXiv230916108B, 2023arXiv2309.16108B\n",
      "2024-04-22 15:46:28,863 - WARNING - No references found for arXiv ID: 2309.16108\n",
      "2024-04-22 15:46:28,864 - INFO - Generated bibcodes for arXiv ID 2310.00074: 2023arXiv231000074H, 2023arXiv2310.00074H\n",
      "2024-04-22 15:46:28,865 - WARNING - No references found for arXiv ID: 2310.00074\n",
      "2024-04-22 15:46:28,865 - INFO - Generated bibcodes for arXiv ID 2310.00132: 2023arXiv231000132L, 2023arXiv2310.00132L\n",
      "2024-04-22 15:46:28,866 - WARNING - No references found for arXiv ID: 2310.00132\n",
      "2024-04-22 15:46:28,866 - INFO - Generated bibcodes for arXiv ID 2310.01143: 2023arXiv231001143G, 2023arXiv2310.01143G\n",
      "2024-04-22 15:46:28,867 - WARNING - No references found for arXiv ID: 2310.01143\n",
      "2024-04-22 15:46:28,868 - INFO - Generated bibcodes for arXiv ID 2310.01880: 2023arXiv231001880Y, 2023arXiv2310.01880Y\n",
      "2024-04-22 15:46:28,868 - WARNING - No references found for arXiv ID: 2310.01880\n",
      "2024-04-22 15:46:28,869 - INFO - Generated bibcodes for arXiv ID 2310.01945: 2023arXiv231001945K, 2023arXiv2310.01945K\n",
      "2024-04-22 15:46:28,870 - WARNING - No references found for arXiv ID: 2310.01945\n",
      "2024-04-22 15:46:28,870 - INFO - Generated bibcodes for arXiv ID 2310.03624: 2023arXiv231003624S, 2023arXiv2310.03624S\n",
      "2024-04-22 15:46:28,871 - WARNING - No references found for arXiv ID: 2310.03624\n",
      "2024-04-22 15:46:28,871 - INFO - Generated bibcodes for arXiv ID 2310.05898: 2023arXiv231005898C, 2023arXiv2310.05898C\n",
      "2024-04-22 15:46:28,872 - WARNING - No references found for arXiv ID: 2310.05898\n",
      "2024-04-22 15:46:28,872 - INFO - Generated bibcodes for arXiv ID 2310.07446: 2023arXiv231007446Z, 2023arXiv2310.07446Z\n",
      "2024-04-22 15:46:28,873 - WARNING - No references found for arXiv ID: 2310.07446\n",
      "2024-04-22 15:46:28,873 - INFO - Generated bibcodes for arXiv ID 2310.07983: 2023arXiv231007983G, 2023arXiv2310.07983G\n",
      "2024-04-22 15:46:28,874 - WARNING - No references found for arXiv ID: 2310.07983\n",
      "2024-04-22 15:46:28,875 - INFO - Generated bibcodes for arXiv ID 2310.10404: 2023arXiv231010404K, 2023arXiv2310.10404K\n",
      "2024-04-22 15:46:28,875 - WARNING - No references found for arXiv ID: 2310.10404\n",
      "2024-04-22 15:46:28,876 - INFO - Generated bibcodes for arXiv ID 2310.12079: 2023arXiv231012079L, 2023arXiv2310.12079L\n",
      "2024-04-22 15:46:28,876 - WARNING - No references found for arXiv ID: 2310.12079\n",
      "2024-04-22 15:46:28,877 - INFO - Generated bibcodes for arXiv ID 2310.14724: 2023arXiv231014724W, 2023arXiv2310.14724W\n",
      "2024-04-22 15:46:28,877 - WARNING - No references found for arXiv ID: 2310.14724\n",
      "2024-04-22 15:46:28,878 - INFO - Generated bibcodes for arXiv ID 2310.17304: 2023arXiv231017304X, 2023arXiv2310.17304X\n",
      "2024-04-22 15:46:28,878 - WARNING - No references found for arXiv ID: 2310.17304\n",
      "2024-04-22 15:46:28,879 - INFO - Generated bibcodes for arXiv ID 2310.18784: 2023arXiv231018784A, 2023arXiv2310.18784A\n",
      "2024-04-22 15:46:28,879 - WARNING - No references found for arXiv ID: 2310.18784\n",
      "2024-04-22 15:46:28,882 - INFO - Generated bibcodes for arXiv ID 2310.19454: 2023arXiv231019454K, 2023arXiv2310.19454K\n",
      "2024-04-22 15:46:28,883 - WARNING - No references found for arXiv ID: 2310.19454\n",
      "2024-04-22 15:46:28,884 - INFO - Generated bibcodes for arXiv ID 2311.00944: 2023arXiv231100944S, 2023arXiv2311.00944S\n",
      "2024-04-22 15:46:28,885 - WARNING - No references found for arXiv ID: 2311.00944\n",
      "2024-04-22 15:46:28,885 - INFO - Generated bibcodes for arXiv ID 2311.02909: 2023arXiv231102909T, 2023arXiv2311.02909T\n",
      "2024-04-22 15:46:28,886 - WARNING - No references found for arXiv ID: 2311.02909\n",
      "2024-04-22 15:46:28,886 - INFO - Generated bibcodes for arXiv ID 2311.03056: 2023arXiv231103056G, 2023arXiv2311.03056G\n",
      "2024-04-22 15:46:28,887 - WARNING - No references found for arXiv ID: 2311.03056\n",
      "2024-04-22 15:46:28,888 - INFO - Generated bibcodes for arXiv ID 2311.04205: 2023arXiv231104205D, 2023arXiv2311.04205D\n",
      "2024-04-22 15:46:28,888 - WARNING - No references found for arXiv ID: 2311.04205\n",
      "2024-04-22 15:46:28,889 - INFO - Generated bibcodes for arXiv ID 2311.04253: 2023arXiv231104253R, 2023arXiv2311.04253R\n",
      "2024-04-22 15:46:28,889 - WARNING - No references found for arXiv ID: 2311.04253\n",
      "2024-04-22 15:46:28,890 - INFO - Generated bibcodes for arXiv ID 2311.05734: 2023arXiv231105734S, 2023arXiv2311.05734S\n",
      "2024-04-22 15:46:28,890 - WARNING - No references found for arXiv ID: 2311.05734\n",
      "2024-04-22 15:46:28,891 - INFO - Generated bibcodes for arXiv ID 2311.08097: 2023arXiv231108097R, 2023arXiv2311.08097R\n",
      "2024-04-22 15:46:28,891 - WARNING - No references found for arXiv ID: 2311.08097\n",
      "2024-04-22 15:46:28,892 - INFO - Generated bibcodes for arXiv ID 2311.08990: 2023arXiv231108990K, 2023arXiv2311.08990K\n",
      "2024-04-22 15:46:28,893 - WARNING - No references found for arXiv ID: 2311.08990\n",
      "2024-04-22 15:46:28,893 - INFO - Generated bibcodes for arXiv ID 2311.09049: 2023arXiv231109049Z, 2023arXiv2311.09049Z\n",
      "2024-04-22 15:46:28,894 - WARNING - No references found for arXiv ID: 2311.09049\n",
      "2024-04-22 15:46:28,906 - INFO - Generated bibcodes for arXiv ID 2110.15517: 2021arXiv211015517H, 2021arXiv2110.15517H\n",
      "2024-04-22 15:46:28,907 - WARNING - No references found for arXiv ID: 2110.15517\n",
      "2024-04-22 15:46:28,908 - INFO - Generated bibcodes for arXiv ID 2202.04573: 2022arXiv220204573H, 2022arXiv2202.04573H\n",
      "2024-04-22 15:46:28,908 - WARNING - No references found for arXiv ID: 2202.04573\n",
      "2024-04-22 15:46:28,909 - INFO - Generated bibcodes for arXiv ID 2211.13605: 2022arXiv221113605V, 2022arXiv2211.13605V\n",
      "2024-04-22 15:46:28,909 - WARNING - No references found for arXiv ID: 2211.13605\n",
      "2024-04-22 15:46:28,910 - INFO - Generated bibcodes for arXiv ID 2307.12695: 2023arXiv230712695B, 2023arXiv2307.12695B\n",
      "2024-04-22 15:46:28,910 - WARNING - No references found for arXiv ID: 2307.12695\n",
      "2024-04-22 15:46:28,911 - INFO - Generated bibcodes for arXiv ID 2308.00179: 2023arXiv230800179A, 2023arXiv2308.00179A\n",
      "2024-04-22 15:46:28,911 - WARNING - No references found for arXiv ID: 2308.00179\n",
      "2024-04-22 15:46:28,912 - INFO - Generated bibcodes for arXiv ID 2310.05971: 2023arXiv231005971O, 2023arXiv2310.05971O\n",
      "2024-04-22 15:46:28,912 - WARNING - No references found for arXiv ID: 2310.05971\n",
      "2024-04-22 15:46:28,913 - INFO - Generated bibcodes for arXiv ID 2311.05883: 2023arXiv231105883C, 2023arXiv2311.05883C\n",
      "2024-04-22 15:46:28,913 - WARNING - No references found for arXiv ID: 2311.05883\n",
      "2024-04-22 15:46:28,914 - INFO - Generated bibcodes for arXiv ID 2402.11394: 2024arXiv240211394P, 2024arXiv2402.11394P\n",
      "2024-04-22 15:46:28,914 - WARNING - No references found for arXiv ID: 2402.11394\n",
      "2024-04-22 15:46:28,915 - INFO - Generated bibcodes for arXiv ID 2403.18521: 2024arXiv240318521G, 2024arXiv2403.18521G\n",
      "2024-04-22 15:46:28,915 - WARNING - No references found for arXiv ID: 2403.18521\n",
      "2024-04-22 15:46:28,916 - INFO - Generated bibcodes for arXiv ID 2404.12462: 2024arXiv240412462Z, 2024arXiv2404.12462Z\n",
      "2024-04-22 15:46:28,917 - WARNING - No references found for arXiv ID: 2404.12462\n",
      "2024-04-22 15:46:28,917 - INFO - Generated bibcodes for arXiv ID 2404.12477: 2024arXiv240412477S, 2024arXiv2404.12477S\n",
      "2024-04-22 15:46:28,918 - WARNING - No references found for arXiv ID: 2404.12477\n",
      "2024-04-22 15:46:28,918 - INFO - Generated bibcodes for arXiv ID 2404.12581: 2024arXiv240412581W, 2024arXiv2404.12581W\n",
      "2024-04-22 15:46:28,919 - WARNING - No references found for arXiv ID: 2404.12581\n",
      "2024-04-22 15:46:28,920 - INFO - Generated bibcodes for arXiv ID 2404.12882: 2024arXiv240412882K, 2024arXiv2404.12882K\n",
      "2024-04-22 15:46:28,920 - WARNING - No references found for arXiv ID: 2404.12882\n",
      "2024-04-22 15:46:28,921 - INFO - Generated bibcodes for arXiv ID 2404.12974: 2024arXiv240412974F, 2024arXiv2404.12974F\n",
      "2024-04-22 15:46:28,922 - WARNING - No references found for arXiv ID: 2404.12974\n",
      "2024-04-22 15:46:28,922 - INFO - Generated bibcodes for arXiv ID 2404.12988: 2024arXiv240412988Z, 2024arXiv2404.12988Z\n",
      "2024-04-22 15:46:28,923 - WARNING - No references found for arXiv ID: 2404.12988\n",
      "2024-04-22 15:46:28,923 - INFO - Generated bibcodes for arXiv ID 2404.12997: 2024arXiv240412997H, 2024arXiv2404.12997H\n",
      "2024-04-22 15:46:28,924 - WARNING - No references found for arXiv ID: 2404.12997\n",
      "2024-04-22 15:46:28,925 - INFO - Generated bibcodes for arXiv ID 1711.08265: 2017arXiv171108265L, 2017arXiv1711.08265L\n",
      "2024-04-22 15:46:28,926 - INFO - Generated bibcodes for arXiv ID 1907.05325: 2019arXiv190705325M, 2019arXiv1907.05325M\n",
      "2024-04-22 15:46:28,926 - INFO - Generated bibcodes for arXiv ID 2101.01157: 2021arXiv210101157A, 2021arXiv2101.01157A\n",
      "2024-04-22 15:46:28,927 - INFO - Generated bibcodes for arXiv ID 2110.15517: 2021arXiv211015517H, 2021arXiv2110.15517H\n",
      "2024-04-22 15:46:28,927 - INFO - Generated bibcodes for arXiv ID 2111.04652: 2021arXiv211104652M, 2021arXiv2111.04652M\n",
      "2024-04-22 15:46:28,928 - INFO - Generated bibcodes for arXiv ID 2201.09648: 2022arXiv220109648P, 2022arXiv2201.09648P\n",
      "2024-04-22 15:46:28,929 - INFO - Generated bibcodes for arXiv ID 2210.02171: 2022arXiv221002171C, 2022arXiv2210.02171C\n",
      "2024-04-22 15:46:28,929 - INFO - Generated bibcodes for arXiv ID 2210.16655: 2022arXiv221016655J, 2022arXiv2210.16655J\n",
      "2024-04-22 15:46:28,935 - INFO - Generated bibcodes for arXiv ID 2212.04911: 2022arXiv221204911L, 2022arXiv2212.04911L\n",
      "2024-04-22 15:46:28,935 - INFO - Generated bibcodes for arXiv ID 2302.03558: 2023arXiv230203558G, 2023arXiv2302.03558G\n",
      "2024-04-22 15:46:28,936 - INFO - Generated bibcodes for arXiv ID 2303.06434: 2023arXiv230306434T, 2023arXiv2303.06434T\n",
      "2024-04-22 15:46:28,937 - INFO - Generated bibcodes for arXiv ID 2303.10322: 2023arXiv230310322S, 2023arXiv2303.10322S\n",
      "2024-04-22 15:46:28,937 - INFO - Generated bibcodes for arXiv ID 2304.02025: 2023arXiv230402025B, 2023arXiv2304.02025B\n",
      "2024-04-22 15:46:28,938 - INFO - Generated bibcodes for arXiv ID 2306.08553: 2023arXiv230608553J, 2023arXiv2306.08553J\n",
      "2024-04-22 15:46:28,939 - INFO - Generated bibcodes for arXiv ID 2309.00380: 2023arXiv230900380H, 2023arXiv2309.00380H\n",
      "2024-04-22 15:46:28,939 - INFO - Generated bibcodes for arXiv ID 2310.05898: 2023arXiv231005898C, 2023arXiv2310.05898C\n",
      "2024-04-22 15:46:28,940 - INFO - Generated bibcodes for arXiv ID 2310.07983: 2023arXiv231007983G, 2023arXiv2310.07983G\n",
      "2024-04-22 15:46:28,941 - INFO - Generated bibcodes for arXiv ID 2310.12079: 2023arXiv231012079L, 2023arXiv2310.12079L\n",
      "2024-04-22 15:46:28,943 - INFO - Generated bibcodes for arXiv ID 2310.18784: 2023arXiv231018784A, 2023arXiv2310.18784A\n",
      "2024-04-22 15:46:28,944 - INFO - Generated bibcodes for arXiv ID 2310.19454: 2023arXiv231019454K, 2023arXiv2310.19454K\n",
      "2024-04-22 15:46:28,945 - INFO - Generated bibcodes for arXiv ID 2311.00944: 2023arXiv231100944S, 2023arXiv2311.00944S\n",
      "2024-04-22 15:46:28,946 - INFO - Generated bibcodes for arXiv ID 2311.05883: 2023arXiv231105883C, 2023arXiv2311.05883C\n",
      "2024-04-22 15:46:28,946 - INFO - Generated bibcodes for arXiv ID 2311.14653: 2023arXiv231114653H, 2023arXiv2311.14653H\n",
      "2024-04-22 15:46:28,947 - INFO - Generated bibcodes for arXiv ID 2312.00417: 2023arXiv231200417D, 2023arXiv2312.00417D\n",
      "2024-04-22 15:46:28,948 - INFO - Generated bibcodes for arXiv ID 2402.10043: 2024arXiv240210043P, 2024arXiv2402.10043P\n",
      "2024-04-22 15:46:28,950 - INFO - Generated bibcodes for arXiv ID 2402.11394: 2024arXiv240211394P, 2024arXiv2402.11394P\n",
      "2024-04-22 15:46:28,950 - INFO - Generated bibcodes for arXiv ID 2402.15984: 2024arXiv240215984S, 2024arXiv2402.15984S\n",
      "2024-04-22 15:46:28,951 - INFO - Generated bibcodes for arXiv ID 2403.12459: 2024arXiv240312459W, 2024arXiv2403.12459W\n",
      "2024-04-22 15:46:28,952 - INFO - Generated bibcodes for arXiv ID 2404.03701: 2024arXiv240403701F, 2024arXiv2404.03701F\n",
      "2024-04-22 15:46:28,952 - INFO - Generated bibcodes for arXiv ID 2404.12215: 2024arXiv240412215H, 2024arXiv2404.12215H\n",
      "2024-04-22 15:46:28,953 - INFO - Generated bibcodes for arXiv ID 2404.12219: 2024arXiv240412219A, 2024arXiv2404.12219A\n",
      "2024-04-22 15:46:28,954 - INFO - Generated bibcodes for arXiv ID 2404.12396: 2024arXiv240412396V, 2024arXiv2404.12396V\n",
      "2024-04-22 15:46:28,954 - INFO - Generated bibcodes for arXiv ID 2404.12408: 2024arXiv240412408C, 2024arXiv2404.12408C\n",
      "2024-04-22 15:46:28,955 - INFO - Generated bibcodes for arXiv ID 2404.12418: 2024arXiv240412418G, 2024arXiv2404.12418G\n",
      "2024-04-22 15:46:28,956 - INFO - Generated bibcodes for arXiv ID 2404.12463: 2024arXiv240412463K, 2024arXiv2404.12463K\n",
      "2024-04-22 15:46:28,956 - INFO - Generated bibcodes for arXiv ID 2404.12478: 2024arXiv240412478R, 2024arXiv2404.12478R\n",
      "2024-04-22 15:46:28,957 - INFO - Generated bibcodes for arXiv ID 2404.12481: 2024arXiv240412481L, 2024arXiv2404.12481L\n",
      "2024-04-22 15:46:28,958 - INFO - Generated bibcodes for arXiv ID 2404.12483: 2024arXiv240412483X, 2024arXiv2404.12483X\n",
      "2024-04-22 15:46:28,958 - INFO - Generated bibcodes for arXiv ID 2404.12484: 2024arXiv240412484Z, 2024arXiv2404.12484Z\n",
      "2024-04-22 15:46:28,959 - INFO - Generated bibcodes for arXiv ID 2404.12499: 2024arXiv240412499D, 2024arXiv2404.12499D\n",
      "2024-04-22 15:46:28,959 - INFO - Generated bibcodes for arXiv ID 2404.12534: 2024arXiv240412534S, 2024arXiv2404.12534S\n",
      "2024-04-22 15:46:28,960 - INFO - Generated bibcodes for arXiv ID 2404.12544: 2024arXiv240412544E, 2024arXiv2404.12544E\n",
      "2024-04-22 15:46:28,961 - INFO - Generated bibcodes for arXiv ID 2404.12553: 2024arXiv240412553J, 2024arXiv2404.12553J\n",
      "2024-04-22 15:46:28,961 - INFO - Generated bibcodes for arXiv ID 2404.12556: 2024arXiv240412556B, 2024arXiv2404.12556B\n",
      "2024-04-22 15:46:28,962 - INFO - Generated bibcodes for arXiv ID 2404.12583: 2024arXiv240412583K, 2024arXiv2404.12583K\n",
      "2024-04-22 15:46:28,962 - INFO - Generated bibcodes for arXiv ID 2404.12586: 2024arXiv240412586C, 2024arXiv2404.12586C\n",
      "2024-04-22 15:46:28,963 - INFO - Generated bibcodes for arXiv ID 2404.12589: 2024arXiv240412589C, 2024arXiv2404.12589C\n",
      "2024-04-22 15:46:28,964 - INFO - Generated bibcodes for arXiv ID 2404.12592: 2024arXiv240412592X, 2024arXiv2404.12592X\n",
      "2024-04-22 15:46:28,964 - INFO - Generated bibcodes for arXiv ID 2404.12597: 2024arXiv240412597Z, 2024arXiv2404.12597Z\n",
      "2024-04-22 15:46:28,965 - INFO - Generated bibcodes for arXiv ID 2404.12610: 2024arXiv240412610D, 2024arXiv2404.12610D\n",
      "2024-04-22 15:46:28,966 - INFO - Generated bibcodes for arXiv ID 2404.12613: 2024arXiv240412613L, 2024arXiv2404.12613L\n",
      "2024-04-22 15:46:28,966 - INFO - Generated bibcodes for arXiv ID 2404.12648: 2024arXiv240412648H, 2024arXiv2404.12648H\n",
      "2024-04-22 15:46:28,967 - INFO - Generated bibcodes for arXiv ID 2404.12657: 2024arXiv240412657J, 2024arXiv2404.12657J\n",
      "2024-04-22 15:46:28,968 - INFO - Generated bibcodes for arXiv ID 2404.12684: 2024arXiv240412684M, 2024arXiv2404.12684M\n",
      "2024-04-22 15:46:28,968 - INFO - Generated bibcodes for arXiv ID 2404.12685: 2024arXiv240412685M, 2024arXiv2404.12685M\n",
      "2024-04-22 15:46:28,969 - INFO - Generated bibcodes for arXiv ID 2404.12692: 2024arXiv240412692M, 2024arXiv2404.12692M\n",
      "2024-04-22 15:46:28,969 - INFO - Generated bibcodes for arXiv ID 2404.12696: 2024arXiv240412696W, 2024arXiv2404.12696W\n",
      "2024-04-22 15:46:28,970 - INFO - Generated bibcodes for arXiv ID 2404.12756: 2024arXiv240412756C, 2024arXiv2404.12756C\n",
      "2024-04-22 15:46:28,970 - INFO - Generated bibcodes for arXiv ID 2404.12812: 2024arXiv240412812C, 2024arXiv2404.12812C\n",
      "2024-04-22 15:46:28,971 - INFO - Generated bibcodes for arXiv ID 2404.12828: 2024arXiv240412828M, 2024arXiv2404.12828M\n",
      "2024-04-22 15:46:28,972 - INFO - Generated bibcodes for arXiv ID 2404.12862: 2024arXiv240412862E, 2024arXiv2404.12862E\n",
      "2024-04-22 15:46:28,972 - INFO - Generated bibcodes for arXiv ID 2404.12889: 2024arXiv240412889K, 2024arXiv2404.12889K\n",
      "2024-04-22 15:46:28,973 - INFO - Generated bibcodes for arXiv ID 2404.12923: 2024arXiv240412923L, 2024arXiv2404.12923L\n",
      "2024-04-22 15:46:28,973 - INFO - Generated bibcodes for arXiv ID 2404.12940: 2024arXiv240412940B, 2024arXiv2404.12940B\n",
      "2024-04-22 15:46:28,974 - INFO - Generated bibcodes for arXiv ID 2404.12943: 2024arXiv240412943C, 2024arXiv2404.12943C\n",
      "2024-04-22 15:46:28,974 - INFO - Generated bibcodes for arXiv ID 2404.12949: 2024arXiv240412949G, 2024arXiv2404.12949G\n",
      "2024-04-22 15:46:28,975 - INFO - Generated bibcodes for arXiv ID 2404.12967: 2024arXiv240412967J, 2024arXiv2404.12967J\n",
      "2024-04-22 15:46:28,976 - INFO - Generated bibcodes for arXiv ID 2404.12968: 2024arXiv240412968K, 2024arXiv2404.12968K\n",
      "2024-04-22 15:46:28,976 - INFO - Generated bibcodes for arXiv ID 2404.13016: 2024arXiv240413016L, 2024arXiv2404.13016L\n",
      "2024-04-22 15:46:28,977 - INFO - Generated bibcodes for arXiv ID 1906.02358: 2019arXiv190602358S, 2019arXiv1906.02358S\n",
      "2024-04-22 15:46:28,977 - INFO - Generated bibcodes for arXiv ID 1907.05325: 2019arXiv190705325M, 2019arXiv1907.05325M\n",
      "2024-04-22 15:46:28,978 - INFO - Generated bibcodes for arXiv ID 1912.10642: 2019arXiv191210642P, 2019arXiv1912.10642P\n",
      "2024-04-22 15:46:28,979 - INFO - Generated bibcodes for arXiv ID 2101.07223: 2021arXiv210107223A, 2021arXiv2101.07223A\n",
      "2024-04-22 15:46:28,979 - INFO - Generated bibcodes for arXiv ID 2101.09180: 2021arXiv210109180Z, 2021arXiv2101.09180Z\n",
      "2024-04-22 15:46:28,980 - INFO - Generated bibcodes for arXiv ID 2105.05716: 2021arXiv210505716R, 2021arXiv2105.05716R\n",
      "2024-04-22 15:46:28,981 - INFO - Generated bibcodes for arXiv ID 2111.01993: 2021arXiv211101993U, 2021arXiv2111.01993U\n",
      "2024-04-22 15:46:28,981 - INFO - Generated bibcodes for arXiv ID 2202.07595: 2022arXiv220207595H, 2022arXiv2202.07595H\n",
      "2024-04-22 15:46:28,982 - INFO - Generated bibcodes for arXiv ID 2203.07976: 2022arXiv220307976R, 2022arXiv2203.07976R\n",
      "2024-04-22 15:46:28,984 - INFO - Generated bibcodes for arXiv ID 2203.11593: 2022arXiv220311593J, 2022arXiv2203.11593J\n",
      "2024-04-22 15:46:28,985 - INFO - Generated bibcodes for arXiv ID 2204.10325: 2022arXiv220410325D, 2022arXiv2204.10325D\n",
      "2024-04-22 15:46:28,985 - INFO - Generated bibcodes for arXiv ID 2205.01438: 2022arXiv220501438Z, 2022arXiv2205.01438Z\n",
      "2024-04-22 15:46:28,986 - INFO - Generated bibcodes for arXiv ID 2205.02533: 2022arXiv220502533X, 2022arXiv2205.02533X\n",
      "2024-04-22 15:46:28,987 - INFO - Generated bibcodes for arXiv ID 2205.14223: 2022arXiv220514223Z, 2022arXiv2205.14223Z\n",
      "2024-04-22 15:46:28,987 - INFO - Generated bibcodes for arXiv ID 2206.09325: 2022arXiv220609325Z, 2022arXiv2206.09325Z\n",
      "2024-04-22 15:46:28,988 - INFO - Generated bibcodes for arXiv ID 2206.15101: 2022arXiv220615101R, 2022arXiv2206.15101R\n",
      "2024-04-22 15:46:28,988 - INFO - Generated bibcodes for arXiv ID 2209.01621: 2022arXiv220901621B, 2022arXiv2209.01621B\n",
      "2024-04-22 15:46:28,989 - INFO - Generated bibcodes for arXiv ID 2209.10192: 2022arXiv220910192J, 2022arXiv2209.10192J\n",
      "2024-04-22 15:46:28,989 - INFO - Generated bibcodes for arXiv ID 2210.01988: 2022arXiv221001988Z, 2022arXiv2210.01988Z\n",
      "2024-04-22 15:46:28,990 - INFO - Generated bibcodes for arXiv ID 2210.08298: 2022arXiv221008298F, 2022arXiv2210.08298F\n",
      "2024-04-22 15:46:28,991 - INFO - Generated bibcodes for arXiv ID 2211.02866: 2022arXiv221102866B, 2022arXiv2211.02866B\n",
      "2024-04-22 15:46:28,991 - INFO - Generated bibcodes for arXiv ID 2211.07440: 2022arXiv221107440R, 2022arXiv2211.07440R\n",
      "2024-04-22 15:46:28,992 - INFO - Generated bibcodes for arXiv ID 2211.11424: 2022arXiv221111424X, 2022arXiv2211.11424X\n",
      "2024-04-22 15:46:28,993 - INFO - Generated bibcodes for arXiv ID 2212.03218: 2022arXiv221203218K, 2022arXiv2212.03218K\n",
      "2024-04-22 15:46:28,993 - INFO - Generated bibcodes for arXiv ID 2301.00185: 2022arXiv230100185S, 2022arXiv2301.00185S\n",
      "2024-04-22 15:46:28,994 - INFO - Generated bibcodes for arXiv ID 2301.00812: 2022arXiv230100812Y, 2022arXiv2301.00812Y\n",
      "2024-04-22 15:46:28,994 - INFO - Generated bibcodes for arXiv ID 2301.06280: 2023arXiv230106280J, 2023arXiv2301.06280J\n",
      "2024-04-22 15:46:28,995 - INFO - Generated bibcodes for arXiv ID 2301.06520: 2023arXiv230106520M, 2023arXiv2301.06520M\n",
      "2024-04-22 15:46:28,995 - INFO - Generated bibcodes for arXiv ID 2303.04467: 2023arXiv230304467S, 2023arXiv2303.04467S\n",
      "2024-04-22 15:46:28,996 - INFO - Generated bibcodes for arXiv ID 2303.10216: 2023arXiv230310216K, 2023arXiv2303.10216K\n",
      "2024-04-22 15:46:28,997 - INFO - Generated bibcodes for arXiv ID 2303.10322: 2023arXiv230310322S, 2023arXiv2303.10322S\n",
      "2024-04-22 15:46:28,997 - INFO - Generated bibcodes for arXiv ID 2303.12342: 2023arXiv230312342L, 2023arXiv2303.12342L\n",
      "2024-04-22 15:46:28,998 - INFO - Generated bibcodes for arXiv ID 2303.16421: 2023arXiv230316421B, 2023arXiv2303.16421B\n",
      "2024-04-22 15:46:28,998 - INFO - Generated bibcodes for arXiv ID 2304.00372: 2023arXiv230400372L, 2023arXiv2304.00372L\n",
      "2024-04-22 15:46:28,999 - INFO - Generated bibcodes for arXiv ID 2304.05166: 2023arXiv230405166M, 2023arXiv2304.05166M\n",
      "2024-04-22 15:46:28,999 - INFO - Generated bibcodes for arXiv ID 2304.07468: 2023arXiv230407468K, 2023arXiv2304.07468K\n",
      "2024-04-22 15:46:29,000 - INFO - Generated bibcodes for arXiv ID 2304.09779: 2023arXiv230409779S, 2023arXiv2304.09779S\n",
      "2024-04-22 15:46:29,001 - INFO - Generated bibcodes for arXiv ID 2304.10286: 2023arXiv230410286P, 2023arXiv2304.10286P\n",
      "2024-04-22 15:46:29,001 - INFO - Generated bibcodes for arXiv ID 2304.13029: 2023arXiv230413029M, 2023arXiv2304.13029M\n",
      "2024-04-22 15:46:29,002 - INFO - Generated bibcodes for arXiv ID 2305.03803: 2023arXiv230503803H, 2023arXiv2305.03803H\n",
      "2024-04-22 15:46:29,002 - INFO - Generated bibcodes for arXiv ID 2305.07877: 2023arXiv230507877G, 2023arXiv2305.07877G\n",
      "2024-04-22 15:46:29,003 - INFO - Generated bibcodes for arXiv ID 2305.18453: 2023arXiv230518453D, 2023arXiv2305.18453D\n",
      "2024-04-22 15:46:29,004 - INFO - Generated bibcodes for arXiv ID 2306.03027: 2023arXiv230603027G, 2023arXiv2306.03027G\n",
      "2024-04-22 15:46:29,004 - INFO - Generated bibcodes for arXiv ID 2306.06449: 2023arXiv230606449B, 2023arXiv2306.06449B\n",
      "2024-04-22 15:46:29,005 - INFO - Generated bibcodes for arXiv ID 2306.08386: 2023arXiv230608386L, 2023arXiv2306.08386L\n",
      "2024-04-22 15:46:29,006 - INFO - Generated bibcodes for arXiv ID 2306.08553: 2023arXiv230608553J, 2023arXiv2306.08553J\n",
      "2024-04-22 15:46:29,007 - INFO - Generated bibcodes for arXiv ID 2306.10091: 2023arXiv230610091P, 2023arXiv2306.10091P\n",
      "2024-04-22 15:46:29,007 - INFO - Generated bibcodes for arXiv ID 2306.17808: 2023arXiv230617808D, 2023arXiv2306.17808D\n",
      "2024-04-22 15:46:29,008 - INFO - Generated bibcodes for arXiv ID 2307.01004: 2023arXiv230701004Y, 2023arXiv2307.01004Y\n",
      "2024-04-22 15:46:29,008 - INFO - Generated bibcodes for arXiv ID 2307.02620: 2023arXiv230702620B, 2023arXiv2307.02620B\n",
      "2024-04-22 15:46:29,009 - INFO - Generated bibcodes for arXiv ID 2307.02913: 2023arXiv230702913P, 2023arXiv2307.02913P\n",
      "2024-04-22 15:46:29,010 - INFO - Generated bibcodes for arXiv ID 2307.06917: 2023arXiv230706917M, 2023arXiv2307.06917M\n",
      "2024-04-22 15:46:29,010 - INFO - Generated bibcodes for arXiv ID 2307.07520: 2023arXiv230707520B, 2023arXiv2307.07520B\n",
      "2024-04-22 15:46:29,010 - INFO - Generated bibcodes for arXiv ID 2307.08523: 2023arXiv230708523F, 2023arXiv2307.08523F\n",
      "2024-04-22 15:46:29,011 - INFO - Generated bibcodes for arXiv ID 2307.12162: 2023arXiv230712162C, 2023arXiv2307.12162C\n",
      "2024-04-22 15:46:29,012 - INFO - Generated bibcodes for arXiv ID 2307.12513: 2023arXiv230712513W, 2023arXiv2307.12513W\n",
      "2024-04-22 15:46:29,012 - INFO - Generated bibcodes for arXiv ID 2307.15517: 2023arXiv230715517C, 2023arXiv2307.15517C\n",
      "2024-04-22 15:46:29,013 - INFO - Generated bibcodes for arXiv ID 2308.04725: 2023arXiv230804725F, 2023arXiv2308.04725F\n",
      "2024-04-22 15:46:29,013 - INFO - Generated bibcodes for arXiv ID 2308.06979: 2023arXiv230806979F, 2023arXiv2308.06979F\n",
      "2024-04-22 15:46:29,014 - INFO - Generated bibcodes for arXiv ID 2308.08945: 2023arXiv230808945A, 2023arXiv2308.08945A\n",
      "2024-04-22 15:46:29,014 - INFO - Generated bibcodes for arXiv ID 2308.09084: 2023arXiv230809084Y, 2023arXiv2308.09084Y\n",
      "2024-04-22 15:46:29,015 - INFO - Generated bibcodes for arXiv ID 2308.10838: 2023arXiv230810838P, 2023arXiv2308.10838P\n",
      "2024-04-22 15:46:29,015 - INFO - Generated bibcodes for arXiv ID 2308.11098: 2023arXiv230811098P, 2023arXiv2308.11098P\n",
      "2024-04-22 15:46:29,016 - INFO - Generated bibcodes for arXiv ID 2308.12462: 2023arXiv230812462Z, 2023arXiv2308.12462Z\n",
      "2024-04-22 15:46:29,017 - INFO - Generated bibcodes for arXiv ID 2308.16042: 2023arXiv230816042L, 2023arXiv2308.16042L\n",
      "2024-04-22 15:46:29,017 - INFO - Generated bibcodes for arXiv ID 2309.00380: 2023arXiv230900380H, 2023arXiv2309.00380H\n",
      "2024-04-22 15:46:29,018 - INFO - Generated bibcodes for arXiv ID 2309.02208: 2023arXiv230902208F, 2023arXiv2309.02208F\n",
      "2024-04-22 15:46:29,018 - INFO - Generated bibcodes for arXiv ID 2309.02354: 2023arXiv230902354L, 2023arXiv2309.02354L\n",
      "2024-04-22 15:46:29,019 - INFO - Generated bibcodes for arXiv ID 2309.04001: 2023arXiv230904001R, 2023arXiv2309.04001R\n",
      "2024-04-22 15:46:29,019 - INFO - Generated bibcodes for arXiv ID 2309.05442: 2023arXiv230905442M, 2023arXiv2309.05442M\n",
      "2024-04-22 15:46:29,020 - INFO - Generated bibcodes for arXiv ID 2309.05927: 2023arXiv230905927L, 2023arXiv2309.05927L\n",
      "2024-04-22 15:46:29,020 - INFO - Generated bibcodes for arXiv ID 2309.06978: 2023arXiv230906978R, 2023arXiv2309.06978R\n",
      "2024-04-22 15:46:29,021 - INFO - Generated bibcodes for arXiv ID 2309.07918: 2023arXiv230907918X, 2023arXiv2309.07918X\n",
      "2024-04-22 15:46:29,021 - INFO - Generated bibcodes for arXiv ID 2309.12830: 2023arXiv230912830S, 2023arXiv2309.12830S\n",
      "2024-04-22 15:46:29,022 - INFO - Generated bibcodes for arXiv ID 2309.15136: 2023arXiv230915136P, 2023arXiv2309.15136P\n",
      "2024-04-22 15:46:29,023 - INFO - Generated bibcodes for arXiv ID 2309.16108: 2023arXiv230916108B, 2023arXiv2309.16108B\n",
      "2024-04-22 15:46:29,023 - INFO - Generated bibcodes for arXiv ID 2310.00074: 2023arXiv231000074H, 2023arXiv2310.00074H\n",
      "2024-04-22 15:46:29,024 - INFO - Generated bibcodes for arXiv ID 2310.00132: 2023arXiv231000132L, 2023arXiv2310.00132L\n",
      "2024-04-22 15:46:29,024 - INFO - Generated bibcodes for arXiv ID 2310.01143: 2023arXiv231001143G, 2023arXiv2310.01143G\n",
      "2024-04-22 15:46:29,025 - INFO - Generated bibcodes for arXiv ID 2310.01880: 2023arXiv231001880Y, 2023arXiv2310.01880Y\n",
      "2024-04-22 15:46:29,025 - INFO - Generated bibcodes for arXiv ID 2310.01945: 2023arXiv231001945K, 2023arXiv2310.01945K\n",
      "2024-04-22 15:46:29,026 - INFO - Generated bibcodes for arXiv ID 2310.03624: 2023arXiv231003624S, 2023arXiv2310.03624S\n",
      "2024-04-22 15:46:29,026 - INFO - Generated bibcodes for arXiv ID 2310.05898: 2023arXiv231005898C, 2023arXiv2310.05898C\n",
      "2024-04-22 15:46:29,027 - INFO - Generated bibcodes for arXiv ID 2310.07446: 2023arXiv231007446Z, 2023arXiv2310.07446Z\n",
      "2024-04-22 15:46:29,027 - INFO - Generated bibcodes for arXiv ID 2310.07983: 2023arXiv231007983G, 2023arXiv2310.07983G\n",
      "2024-04-22 15:46:29,028 - INFO - Generated bibcodes for arXiv ID 2310.10404: 2023arXiv231010404K, 2023arXiv2310.10404K\n",
      "2024-04-22 15:46:29,028 - INFO - Generated bibcodes for arXiv ID 2310.12079: 2023arXiv231012079L, 2023arXiv2310.12079L\n",
      "2024-04-22 15:46:29,029 - INFO - Generated bibcodes for arXiv ID 2310.14724: 2023arXiv231014724W, 2023arXiv2310.14724W\n",
      "2024-04-22 15:46:29,030 - INFO - Generated bibcodes for arXiv ID 2310.17304: 2023arXiv231017304X, 2023arXiv2310.17304X\n",
      "2024-04-22 15:46:29,031 - INFO - Generated bibcodes for arXiv ID 2310.18784: 2023arXiv231018784A, 2023arXiv2310.18784A\n",
      "2024-04-22 15:46:29,031 - INFO - Generated bibcodes for arXiv ID 2310.19454: 2023arXiv231019454K, 2023arXiv2310.19454K\n",
      "2024-04-22 15:46:29,032 - INFO - Generated bibcodes for arXiv ID 2311.00944: 2023arXiv231100944S, 2023arXiv2311.00944S\n",
      "2024-04-22 15:46:29,032 - INFO - Generated bibcodes for arXiv ID 2311.02909: 2023arXiv231102909T, 2023arXiv2311.02909T\n",
      "2024-04-22 15:46:29,033 - INFO - Generated bibcodes for arXiv ID 2311.03056: 2023arXiv231103056G, 2023arXiv2311.03056G\n",
      "2024-04-22 15:46:29,033 - INFO - Generated bibcodes for arXiv ID 2311.04205: 2023arXiv231104205D, 2023arXiv2311.04205D\n",
      "2024-04-22 15:46:29,034 - INFO - Generated bibcodes for arXiv ID 2311.04253: 2023arXiv231104253R, 2023arXiv2311.04253R\n",
      "2024-04-22 15:46:29,034 - INFO - Generated bibcodes for arXiv ID 2311.05734: 2023arXiv231105734S, 2023arXiv2311.05734S\n",
      "2024-04-22 15:46:29,035 - INFO - Generated bibcodes for arXiv ID 2311.08097: 2023arXiv231108097R, 2023arXiv2311.08097R\n",
      "2024-04-22 15:46:29,035 - INFO - Generated bibcodes for arXiv ID 2311.08990: 2023arXiv231108990K, 2023arXiv2311.08990K\n",
      "2024-04-22 15:46:29,036 - INFO - Generated bibcodes for arXiv ID 2311.09049: 2023arXiv231109049Z, 2023arXiv2311.09049Z\n",
      "2024-04-22 15:46:29,036 - INFO - Generated bibcodes for arXiv ID 2110.15517: 2021arXiv211015517H, 2021arXiv2110.15517H\n",
      "2024-04-22 15:46:29,037 - INFO - Generated bibcodes for arXiv ID 2202.04573: 2022arXiv220204573H, 2022arXiv2202.04573H\n",
      "2024-04-22 15:46:29,038 - INFO - Generated bibcodes for arXiv ID 2211.13605: 2022arXiv221113605V, 2022arXiv2211.13605V\n",
      "2024-04-22 15:46:29,038 - INFO - Generated bibcodes for arXiv ID 2307.12695: 2023arXiv230712695B, 2023arXiv2307.12695B\n",
      "2024-04-22 15:46:29,039 - INFO - Generated bibcodes for arXiv ID 2308.00179: 2023arXiv230800179A, 2023arXiv2308.00179A\n",
      "2024-04-22 15:46:29,039 - INFO - Generated bibcodes for arXiv ID 2310.05971: 2023arXiv231005971O, 2023arXiv2310.05971O\n",
      "2024-04-22 15:46:29,040 - INFO - Generated bibcodes for arXiv ID 2311.05883: 2023arXiv231105883C, 2023arXiv2311.05883C\n",
      "2024-04-22 15:46:29,040 - INFO - Generated bibcodes for arXiv ID 2402.11394: 2024arXiv240211394P, 2024arXiv2402.11394P\n",
      "2024-04-22 15:46:29,041 - INFO - Generated bibcodes for arXiv ID 2403.18521: 2024arXiv240318521G, 2024arXiv2403.18521G\n",
      "2024-04-22 15:46:29,041 - INFO - Generated bibcodes for arXiv ID 2404.12462: 2024arXiv240412462Z, 2024arXiv2404.12462Z\n",
      "2024-04-22 15:46:29,042 - INFO - Generated bibcodes for arXiv ID 2404.12477: 2024arXiv240412477S, 2024arXiv2404.12477S\n",
      "2024-04-22 15:46:29,042 - INFO - Generated bibcodes for arXiv ID 2404.12581: 2024arXiv240412581W, 2024arXiv2404.12581W\n",
      "2024-04-22 15:46:29,043 - INFO - Generated bibcodes for arXiv ID 2404.12882: 2024arXiv240412882K, 2024arXiv2404.12882K\n",
      "2024-04-22 15:46:29,044 - INFO - Generated bibcodes for arXiv ID 2404.12974: 2024arXiv240412974F, 2024arXiv2404.12974F\n",
      "2024-04-22 15:46:29,044 - INFO - Generated bibcodes for arXiv ID 2404.12988: 2024arXiv240412988Z, 2024arXiv2404.12988Z\n",
      "2024-04-22 15:46:29,045 - INFO - Generated bibcodes for arXiv ID 2404.12997: 2024arXiv240412997H, 2024arXiv2404.12997H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: [{'id': '2110.15517', 'title': 'cp factor model for dynamic tensors', 'abstract': 'observations in various applications are frequently represented as a time series of multidimensional arrays, called tensor time series, preserving the inherent multidimensional structure. in this paper, we present a factor model approach, in a form similar to tensor cp decomposition, to the analysis of high-dimensional dynamic tensor time series. as the loading vectors are uniquely defined but not necessarily orthogonal, it is significantly different from the existing tensor factor models based on tucker-type tensor decomposition. the model structure allows for a set of uncorrelated one-dimensional latent dynamic factor processes, making it much more convenient to study the underlying dynamics of the time series. a new high order projection estimator is proposed for such a factor model, utilizing the special structure and the idea of the higher order orthogonal iteration procedures commonly used in tucker-type tensor factor model and general tensor cp decomposition procedures. theoretical investigation provides statistical error bounds for the proposed methods, which shows the significant advantage of utilizing the special model structure. simulation study is conducted to further demonstrate the finite sample properties of the estimators. real data application is used to illustrate the model and its interpretations.', 'doi': '', 'created': '2021-10-28', 'url': 'https://arxiv.org/abs/2110.15517', 'authors': ['yuefeng han', 'dan yang', 'cun-hui zhang', 'rong chen']}, {'id': '2202.04573', 'title': 'on the uniqueness and stability of the equilibrium price in quasi-linear   economies', 'abstract': 'in this paper, we show that if every consumer in an economy has a quasi-linear utility function, then the normalized equilibrium price is unique, and is locally stable with respect to the t\\\\^atonnement process. our study can be seen as that extends the results in partial equilibrium theory to economies with more than two dimensional consumption space. moreover, we discuss the surplus analysis in such economies.', 'doi': '', 'created': '2022-02-09', 'url': 'https://arxiv.org/abs/2202.04573', 'authors': ['yuhki hosoya']}, {'id': '2211.13605', 'title': 'efficient communication in organizations', 'abstract': 'this paper studies the organization of communication between biased senders and a receiver. senders can misreport their private information at a cost. efficiency is achieved by clearing information asymmetries without incurring costs. results show that only one communication protocol is efficient, robust to collusion, and free from unnecessary complexities. this protocol has a simple, adversarial, and public structure. it always induces efficient equilibria, for which a closed-form characterization is provided. the findings are relevant for the design of organizations that seek to improve decision-making while limiting wasteful influence activities.', 'doi': '', 'created': '2022-11-24', 'url': 'https://arxiv.org/abs/2211.13605', 'authors': ['federico vaccari']}, {'id': '2307.12695', 'title': 'propagation of a carbon price in a credit portfolio through   macroeconomic factors', 'abstract': \"we study how the climate transition through a low-carbon economy, implemented by carbon pricing, propagates in a credit portfolio and precisely describe how carbon price dynamics affects credit risk measures such as probability of default, expected and unexpected losses. we adapt a stochastic multisectoral model to take into account the greenhouse gases (ghg) emissions costs of both sectoral firms' production and consumption, as well as sectoral household's consumption. ghg emissions costs are the product of carbon prices, provided by the ngfs transition scenarios, and of ghg emissions. for each sector, our model yields the sensitivity of firms' production and households' consumption to carbon price and the relationships between sectors. it allows us to analyze the short-term effects of the carbon price as opposed to standard iam (such as remind), which are deterministic and only capture long-term trends. finally, we use a dcf methodology to compute firms' values which we then combine with a structural credit risk model to describe how the carbon price impacts credit risk measures. we obtain that the carbon price distorts the distribution of the firm's value, increases banking fees charged to clients (materialized by the bank provisions), and reduces banks' profitability (translated by the economic capital). in addition, the randomness we introduce provides extra flexibility to take into account uncertainties on the productivity and on the different transition scenarios. we also compute the sensitivities of the credit risk measures with respect to changes in the carbon price, yielding further criteria for a more accurate assessment of climate transition risk in a credit portfolio. this work provides a preliminary methodology to calculate the evolution of credit risk measures of a credit portfolio, starting from a given climate transition scenario described by a carbon price.\", 'doi': '', 'created': '2023-07-24', 'url': 'https://arxiv.org/abs/2307.12695', 'authors': ['géraldine bouveret', 'jean-françois chassagneux', 'smail ibbou', 'antoine jacquier', 'lionel sopgoui']}, {'id': '2308.00179', 'title': 'position uncertainty in a sequential public goods game: an experiment', 'abstract': \"gallice and monz\\\\'on (2019) present a natural environment that sustains full co-operation in one-shot social dilemmas among a finite number of self-interested agents. they demonstrate that in a sequential public goods game, where agents lack knowledge of their position in the sequence but can observe some predecessors' actions, full contribution emerges in equilibrium due to agents' incentive to induce potential successors to follow suit. in this study, we aim to test the theoretical predictions of this model through an economic experiment. we conducted three treatments, varying the amount of information about past actions that a subject can observe, as well as their positional awareness. through rigorous structural econometric analysis, we found that approximately 25% of the subjects behaved in line with the theoretical predictions. however, we also observed the presence of alternative behavioural types among the remaining subjects. the majority were classified as conditional co-operators, showing a willingness to cooperate based on others' actions. some subjects exhibited altruistic tendencies, while only a small minority engaged in free-riding behaviour.\", 'doi': '', 'created': '2023-07-31', 'url': 'https://arxiv.org/abs/2308.00179', 'authors': ['chowdhury mohammad sakib anwar', 'konstantinos georgalos']}, {'id': '2310.05971', 'title': 'theoretical economics as successive approximations of statistical   moments', 'abstract': 'this paper studies the links between the descriptions of macroeconomic variables and statistical moments of market trade, price, and return. the randomness of market trade values and volumes during the averaging interval {\\\\delta} results in the random properties of price and return. we describe how averages and volatilities of price and return depend on the averages, volatilities, and correlations of market trade values and volumes. the averages, volatilities, and correlations of market trade, price, and return can behave randomly during the long interval {\\\\delta}2>>{\\\\delta}. to describe their statistical properties during the long interval {\\\\delta}2, we introduce the secondary averaging procedure of trade, price, and return. we explain why, in the coming years, predictions of market-based probabilities of price and return will be limited by gaussian distributions. we discuss the roots of the internal weakness of the commonly used hedging tool, value-at-risk, that cannot be solved and remains the source of additional risks and losses. one should consider theoretical economics as a set of successive approximations, each of which describes the next array of the n-th statistical moments of market trades, price, return, and macroeconomic variables, which are repeatedly averaged during the sequence of increasing time intervals.', 'doi': '', 'created': '2023-09-28', 'url': 'https://arxiv.org/abs/2310.05971', 'authors': ['victor olkhov']}, {'id': '2311.05883', 'title': 'time-varying identification of monetary policy shocks', 'abstract': \"we propose a new bayesian heteroskedastic markov-switching structural vector autoregression with data-driven time-varying identification. the model selects alternative exclusion restrictions over time and, as a condition for the search, allows to verify identification through heteroskedasticity within each regime. based on four alternative monetary policy rules, we show that a monthly six-variable system supports time variation in us monetary policy shock identification. in the sample-dominating first regime, systematic monetary policy follows a taylor rule extended by the term spread, effectively curbing inflation. in the second regime, occurring after 2000 and gaining more persistence after the global financial and covid crises, it is characterized by a money-augmented taylor rule. this regime's unconventional monetary policy provides economic stimulus, features the liquidity effect, and is complemented by a pure term spread shock. absent the specific monetary policy of the second regime, inflation would be over one percentage point higher on average after 2008.\", 'doi': '', 'created': '2023-11-10', 'url': 'https://arxiv.org/abs/2311.05883', 'authors': ['annika camehl', 'tomasz woźniak']}, {'id': '2402.11394', 'title': 'maximal inequalities for empirical processes under general mixing   conditions with an application to strong approximations', 'abstract': 'this paper provides a bound for the supremum of sample averages over a class of functions for a general class of mixing stochastic processes with arbitrary mixing rates. regardless of the speed of mixing, the bound is comprised of a concentration rate and a novel measure of complexity. the speed of mixing, however, affects the former quantity implying a phase transition. fast mixing leads to the standard root-n concentration rate, while slow mixing leads to a slower concentration rate, its speed depends on the mixing structure. our findings are applied to derive strong approximation results for a general class of mixing processes with arbitrary mixing rates.', 'doi': '', 'created': '2024-02-17', 'url': 'https://arxiv.org/abs/2402.11394', 'authors': ['demian pouzo']}, {'id': '2403.18521', 'title': 'redirecting flows -- navigating the future of the amazon', 'abstract': 'the amazon basin and the latin america and caribbean (lac) region stands at a critical juncture, grappling with pressing environmental challenges while holding immense potential for transformative change through innovative solutions. this report illuminates the diverse landscape of social-ecological issues, technological advancements, community-led initiatives, and strategic actions that could help foster biosphere-based sustainability and resilience across the region.', 'doi': '', 'created': '2024-03-27', 'url': 'https://arxiv.org/abs/2403.18521', 'authors': ['victor galaz', 'megan meacham']}, {'id': '2404.12462', 'title': 'axiomatic modeling of fixed proportion technologies', 'abstract': 'understanding input substitution and output transformation possibilities is critical for efficient resource allocation and firm strategy. there are important examples of fixed proportion technologies where certain inputs are non-substitutable and/or certain outputs are non-transformable. however, there is widespread confusion about the appropriate modeling of fixed proportion technologies in data envelopment analysis. we point out and rectify several misconceptions in the existing literature, and show how fixed proportion technologies can be correctly incorporated into the axiomatic framework. a monte carlo study is performed to demonstrate the proposed solution.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12462', 'authors': ['xun zhou', 'timo kuosmanen']}, {'id': '2404.12477', 'title': 'sustainable regional economic development and land use: a case of russia', 'abstract': 'this paper analyzes sustainable regional economic development and land use employing a case study of russia. the economics of land management in russia which is shaped by both historical legacies and contemporary policies represents an interesting conundrum. following the dissolution of the soviet union, russia embarked on a thorny and complex path towards the economic reforms and transformation characterized, among all, by the privatization and decentralization of land ownership. this transition was aimed at improving agricultural productivity and fostering sustainable regional economic development but also led to new challenges such as uneven distribution of land resources, unclear property rights, and underinvestment in rural infrastructure. however, managing all of that effectively poses significant challenges and opportunities. with the help of the comprehensive bibliographic network analysis, this study sheds some light on the current state of sustainable regional economic development and land use management in russia. its results and outcomes might be helpful for the researchers and stakeholders alike in devising effective strategies aimed at maximizing resources for sustainable land use, particularly within their respective regional economies.', 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12477', 'authors': ['wadim strielkowski', 'oxana mukhoryanova', 'oxana kuznetsova', 'yury syrov']}, {'id': '2404.12581', 'title': 'two-step estimation of network formation models with unobserved   heterogeneities and strategic interactions', 'abstract': \"in this paper, i characterize the network formation process as a static game of incomplete information, where the latent payoff of forming a link between two individuals depends on the structure of the network, as well as private information on agents' attributes. i allow agents' private unobserved attributes to be correlated with observed attributes through individual fixed effects. using data from a single large network, i propose a two-step estimator for the model primitives. in the first step, i estimate agents' equilibrium beliefs of other people's choice probabilities. in the second step, i plug in the first-step estimator to the conditional choice probability expression and estimate the model parameters and the unobserved individual fixed effects together using joint mle. assuming that the observed attributes are discrete, i showed that the first step estimator is uniformly consistent with rate $n^{-1/4}$, where $n$ is the total number of linking proposals. i also show that the second-step estimator converges asymptotically to a normal distribution at the same rate.\", 'doi': '', 'created': '2024-04-18', 'url': 'https://arxiv.org/abs/2404.12581', 'authors': ['shaomin wu']}, {'id': '2404.12882', 'title': 'the modified conditional sum-of-squares estimator for fractionally   integrated models', 'abstract': \"in this paper, we analyse the influence of estimating a constant term on the bias of the conditional sum-of-squares (css) estimator in a stationary or non-stationary type-ii arfima ($p_1$,$d$,$p_2$) model. we derive expressions for the estimator's bias and show that the leading term can be easily removed by a simple modification of the css objective function. we call this new estimator the modified conditional sum-of-squares (mcss) estimator. we show theoretically and by means of monte carlo simulations that its performance relative to that of the css estimator is markedly improved even for small sample sizes. finally, we revisit three classical short datasets that have in the past been described by arfima($p_1$,$d$,$p_2$) models with constant term, namely the post-second world war real gnp data, the extended nelson-plosser data, and the nile data.\", 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12882', 'authors': ['mustafa r. kılınç', 'michael massmann']}, {'id': '2404.12974', 'title': 'quantifying seasonal hydrogen storage demands under cost and market   uptake uncertainties in energy system transformation pathways', 'abstract': 'climate neutrality paradigms put electricity systems at the core of a clean energy supply. at the same time, indirect electrification, with a potential uptake of hydrogen or derived fuel economy, plays a crucial role in decarbonising the energy supply and industrial processes. besides energy markets coordinating the transition, climate and energy policy targets require fundamental changes and expansions in the energy transmission, import, distribution, and storage infrastructures. while existing studies identify relevant demands for hydrogen, critical decisions involve imports versus domestic fuel production and investments in new or repurposing existing pipeline and storage infrastructure. linking the pan-european energy system planning model scope sd with the multiperiod european gas market model imagine, the case study analysis and its transformation pathway results indicate extensive network development of hydrogen infrastructure, including expansion beyond refurbished methane infrastructure. however, the ranges of future hydrogen storage costs and market uptake restrictions expose and quantify the uncertainty of its role in europes transformation. the study finds that rapidly planning the construction of hydrogen storage and pipeline infrastructure is crucial to achieving the required capacity by 2050.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12974', 'authors': ['felix frischmuth', 'mattis berghoff', 'martin braun', 'philipp haertel']}, {'id': '2404.12988', 'title': \"understanding intra-household educational inequalities: gender, birth   order, and ability dynamics in benin's households\", 'abstract': \"this paper investigates the nuanced interplay of gender, birth order, and innate ability in shaping educational disparities among children within households, employing both a reduced-form analysis and a structural model of household resource allocation. by decomposing overall disparities, i identify the contributions of gender, birth order, and innate ability differences. in the context of benin, for households with non-educated heads and mixed-gender children, total inequality comprises 50% gender disparity, 20% birth order effect, and 30% ability gap. conversely, in households with only sons or only daughters and non-educated heads, total inequality is predominantly driven by ability disparities 70% for daughters and 83% for sons, with lesser contributions from birth order effects. furthermore, my analysis reveals that in households with non-educated heads, firstborn daughters surpass their younger brothers in educational attainment on the intensive margin if their innate ability exceeds their brothers' by at least 13%, a figure reduced to 8% in households with college-educated parents. additionally, the study unveils parental preferences favoring sons' education over daughters' among non-educated parents, with a perceived 22% higher average benefit. targeted policies aimed at reducing composite education costs prove effective in mitigating gender and birth order gaps, albeit with a modest 5% reduction in total inequality. overall, this research underscores the complex dynamics influencing intra-household educational inequalities and suggests policy avenues to address them.\", 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12988', 'authors': ['christelle zozoungbo']}, {'id': '2404.12997', 'title': 'on the asymmetric volatility connectedness', 'abstract': 'connectedness measures the degree at which a time-series variable spills over volatility to other variables compared to the rate that it is receiving. the idea is based on the percentage of variance decomposition from one variable to the others, which is estimated by making use of a var model. diebold and yilmaz (2012, 2014) suggested estimating this simple and useful measure of percentage risk spillover impact. their method is symmetric by nature, however. the current paper offers an alternative asymmetric approach for measuring the volatility spillover direction, which is based on estimating the asymmetric variance decompositions introduced by hatemi-j (2011, 2014). this approach accounts explicitly for the asymmetric property in the estimations, which accords better with reality. an application is provided to capture the potential asymmetric volatility spillover impacts between the three largest financial markets in the world.', 'doi': '', 'created': '2024-04-19', 'url': 'https://arxiv.org/abs/2404.12997', 'authors': ['abdulnasser hatemi-j']}]\n",
      "Data type: <class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 15:46:29,231 - INFO - Generated bibcodes for arXiv ID 1711.08265: 2017arXiv171108265L, 2017arXiv1711.08265L\n",
      "2024-04-22 15:46:29,235 - WARNING - No references found for bibcodes: 2017arXiv171108265L, 2017arXiv1711.08265L\n",
      "2024-04-22 15:46:29,236 - INFO - Generated bibcodes for arXiv ID 1907.05325: 2019arXiv190705325M, 2019arXiv1907.05325M\n",
      "2024-04-22 15:46:29,237 - WARNING - No references found for bibcodes: 2019arXiv190705325M, 2019arXiv1907.05325M\n",
      "2024-04-22 15:46:29,238 - INFO - Generated bibcodes for arXiv ID 2101.01157: 2021arXiv210101157A, 2021arXiv2101.01157A\n",
      "2024-04-22 15:46:29,239 - WARNING - No references found for bibcodes: 2021arXiv210101157A, 2021arXiv2101.01157A\n",
      "2024-04-22 15:46:29,240 - INFO - Generated bibcodes for arXiv ID 2110.15517: 2021arXiv211015517H, 2021arXiv2110.15517H\n",
      "2024-04-22 15:46:29,240 - WARNING - No references found for bibcodes: 2021arXiv211015517H, 2021arXiv2110.15517H\n",
      "2024-04-22 15:46:29,241 - INFO - Generated bibcodes for arXiv ID 2111.04652: 2021arXiv211104652M, 2021arXiv2111.04652M\n",
      "2024-04-22 15:46:29,241 - WARNING - No references found for bibcodes: 2021arXiv211104652M, 2021arXiv2111.04652M\n",
      "2024-04-22 15:46:29,242 - INFO - Generated bibcodes for arXiv ID 2201.09648: 2022arXiv220109648P, 2022arXiv2201.09648P\n",
      "2024-04-22 15:46:29,243 - WARNING - No references found for bibcodes: 2022arXiv220109648P, 2022arXiv2201.09648P\n",
      "2024-04-22 15:46:29,243 - INFO - Generated bibcodes for arXiv ID 2210.02171: 2022arXiv221002171C, 2022arXiv2210.02171C\n",
      "2024-04-22 15:46:29,244 - WARNING - No references found for bibcodes: 2022arXiv221002171C, 2022arXiv2210.02171C\n",
      "2024-04-22 15:46:29,244 - INFO - Generated bibcodes for arXiv ID 2210.16655: 2022arXiv221016655J, 2022arXiv2210.16655J\n",
      "2024-04-22 15:46:29,245 - WARNING - No references found for bibcodes: 2022arXiv221016655J, 2022arXiv2210.16655J\n",
      "2024-04-22 15:46:29,248 - INFO - Generated bibcodes for arXiv ID 2212.04911: 2022arXiv221204911L, 2022arXiv2212.04911L\n",
      "2024-04-22 15:46:29,249 - WARNING - No references found for bibcodes: 2022arXiv221204911L, 2022arXiv2212.04911L\n",
      "2024-04-22 15:46:29,250 - INFO - Generated bibcodes for arXiv ID 2302.03558: 2023arXiv230203558G, 2023arXiv2302.03558G\n",
      "2024-04-22 15:46:29,251 - WARNING - No references found for bibcodes: 2023arXiv230203558G, 2023arXiv2302.03558G\n",
      "2024-04-22 15:46:29,252 - INFO - Generated bibcodes for arXiv ID 2303.06434: 2023arXiv230306434T, 2023arXiv2303.06434T\n",
      "2024-04-22 15:46:29,253 - WARNING - No references found for bibcodes: 2023arXiv230306434T, 2023arXiv2303.06434T\n",
      "2024-04-22 15:46:29,254 - INFO - Generated bibcodes for arXiv ID 2303.10322: 2023arXiv230310322S, 2023arXiv2303.10322S\n",
      "2024-04-22 15:46:29,256 - WARNING - No references found for bibcodes: 2023arXiv230310322S, 2023arXiv2303.10322S\n",
      "2024-04-22 15:46:29,257 - INFO - Generated bibcodes for arXiv ID 2304.02025: 2023arXiv230402025B, 2023arXiv2304.02025B\n",
      "2024-04-22 15:46:29,258 - WARNING - No references found for bibcodes: 2023arXiv230402025B, 2023arXiv2304.02025B\n",
      "2024-04-22 15:46:29,259 - INFO - Generated bibcodes for arXiv ID 2306.08553: 2023arXiv230608553J, 2023arXiv2306.08553J\n",
      "2024-04-22 15:46:29,259 - WARNING - No references found for bibcodes: 2023arXiv230608553J, 2023arXiv2306.08553J\n",
      "2024-04-22 15:46:29,260 - INFO - Generated bibcodes for arXiv ID 2309.00380: 2023arXiv230900380H, 2023arXiv2309.00380H\n",
      "2024-04-22 15:46:29,261 - WARNING - No references found for bibcodes: 2023arXiv230900380H, 2023arXiv2309.00380H\n",
      "2024-04-22 15:46:29,262 - INFO - Generated bibcodes for arXiv ID 2310.05898: 2023arXiv231005898C, 2023arXiv2310.05898C\n",
      "2024-04-22 15:46:29,262 - WARNING - No references found for bibcodes: 2023arXiv231005898C, 2023arXiv2310.05898C\n",
      "2024-04-22 15:46:29,263 - INFO - Generated bibcodes for arXiv ID 2310.07983: 2023arXiv231007983G, 2023arXiv2310.07983G\n",
      "2024-04-22 15:46:29,263 - WARNING - No references found for bibcodes: 2023arXiv231007983G, 2023arXiv2310.07983G\n",
      "2024-04-22 15:46:29,265 - INFO - Generated bibcodes for arXiv ID 2310.12079: 2023arXiv231012079L, 2023arXiv2310.12079L\n",
      "2024-04-22 15:46:29,266 - WARNING - No references found for bibcodes: 2023arXiv231012079L, 2023arXiv2310.12079L\n",
      "2024-04-22 15:46:29,267 - INFO - Generated bibcodes for arXiv ID 2310.18784: 2023arXiv231018784A, 2023arXiv2310.18784A\n",
      "2024-04-22 15:46:29,268 - WARNING - No references found for bibcodes: 2023arXiv231018784A, 2023arXiv2310.18784A\n",
      "2024-04-22 15:46:29,269 - INFO - Generated bibcodes for arXiv ID 2310.19454: 2023arXiv231019454K, 2023arXiv2310.19454K\n",
      "2024-04-22 15:46:29,269 - WARNING - No references found for bibcodes: 2023arXiv231019454K, 2023arXiv2310.19454K\n",
      "2024-04-22 15:46:29,270 - INFO - Generated bibcodes for arXiv ID 2311.00944: 2023arXiv231100944S, 2023arXiv2311.00944S\n",
      "2024-04-22 15:46:29,271 - WARNING - No references found for bibcodes: 2023arXiv231100944S, 2023arXiv2311.00944S\n",
      "2024-04-22 15:46:29,272 - INFO - Generated bibcodes for arXiv ID 2311.05883: 2023arXiv231105883C, 2023arXiv2311.05883C\n",
      "2024-04-22 15:46:29,272 - WARNING - No references found for bibcodes: 2023arXiv231105883C, 2023arXiv2311.05883C\n",
      "2024-04-22 15:46:29,273 - INFO - Generated bibcodes for arXiv ID 2311.14653: 2023arXiv231114653H, 2023arXiv2311.14653H\n",
      "2024-04-22 15:46:29,274 - WARNING - No references found for bibcodes: 2023arXiv231114653H, 2023arXiv2311.14653H\n",
      "2024-04-22 15:46:29,275 - INFO - Generated bibcodes for arXiv ID 2312.00417: 2023arXiv231200417D, 2023arXiv2312.00417D\n",
      "2024-04-22 15:46:29,276 - INFO - References found for bibcodes: 2023arXiv231200417D, 2023arXiv2312.00417D\n",
      "2024-04-22 15:46:29,277 - INFO - Generated bibcodes for arXiv ID 2402.10043: 2024arXiv240210043P, 2024arXiv2402.10043P\n",
      "2024-04-22 15:46:29,278 - WARNING - No references found for bibcodes: 2024arXiv240210043P, 2024arXiv2402.10043P\n",
      "2024-04-22 15:46:29,279 - INFO - Generated bibcodes for arXiv ID 2402.11394: 2024arXiv240211394P, 2024arXiv2402.11394P\n",
      "2024-04-22 15:46:29,280 - WARNING - No references found for bibcodes: 2024arXiv240211394P, 2024arXiv2402.11394P\n",
      "2024-04-22 15:46:29,280 - INFO - Generated bibcodes for arXiv ID 2402.15984: 2024arXiv240215984S, 2024arXiv2402.15984S\n",
      "2024-04-22 15:46:29,281 - WARNING - No references found for bibcodes: 2024arXiv240215984S, 2024arXiv2402.15984S\n",
      "2024-04-22 15:46:29,281 - INFO - Generated bibcodes for arXiv ID 2403.12459: 2024arXiv240312459W, 2024arXiv2403.12459W\n",
      "2024-04-22 15:46:29,282 - WARNING - No references found for bibcodes: 2024arXiv240312459W, 2024arXiv2403.12459W\n",
      "2024-04-22 15:46:29,284 - INFO - Generated bibcodes for arXiv ID 2404.03701: 2024arXiv240403701F, 2024arXiv2404.03701F\n",
      "2024-04-22 15:46:29,285 - WARNING - No references found for bibcodes: 2024arXiv240403701F, 2024arXiv2404.03701F\n",
      "2024-04-22 15:46:29,286 - INFO - Generated bibcodes for arXiv ID 2404.12215: 2024arXiv240412215H, 2024arXiv2404.12215H\n",
      "2024-04-22 15:46:29,287 - WARNING - No references found for bibcodes: 2024arXiv240412215H, 2024arXiv2404.12215H\n",
      "2024-04-22 15:46:29,288 - INFO - Generated bibcodes for arXiv ID 2404.12219: 2024arXiv240412219A, 2024arXiv2404.12219A\n",
      "2024-04-22 15:46:29,288 - WARNING - No references found for bibcodes: 2024arXiv240412219A, 2024arXiv2404.12219A\n",
      "2024-04-22 15:46:29,289 - INFO - Generated bibcodes for arXiv ID 2404.12396: 2024arXiv240412396V, 2024arXiv2404.12396V\n",
      "2024-04-22 15:46:29,290 - WARNING - No references found for bibcodes: 2024arXiv240412396V, 2024arXiv2404.12396V\n",
      "2024-04-22 15:46:29,291 - INFO - Generated bibcodes for arXiv ID 2404.12408: 2024arXiv240412408C, 2024arXiv2404.12408C\n",
      "2024-04-22 15:46:29,291 - WARNING - No references found for bibcodes: 2024arXiv240412408C, 2024arXiv2404.12408C\n",
      "2024-04-22 15:46:29,292 - INFO - Generated bibcodes for arXiv ID 2404.12418: 2024arXiv240412418G, 2024arXiv2404.12418G\n",
      "2024-04-22 15:46:29,293 - WARNING - No references found for bibcodes: 2024arXiv240412418G, 2024arXiv2404.12418G\n",
      "2024-04-22 15:46:29,293 - INFO - Generated bibcodes for arXiv ID 2404.12463: 2024arXiv240412463K, 2024arXiv2404.12463K\n",
      "2024-04-22 15:46:29,294 - WARNING - No references found for bibcodes: 2024arXiv240412463K, 2024arXiv2404.12463K\n",
      "2024-04-22 15:46:29,294 - INFO - Generated bibcodes for arXiv ID 2404.12478: 2024arXiv240412478R, 2024arXiv2404.12478R\n",
      "2024-04-22 15:46:29,295 - WARNING - No references found for bibcodes: 2024arXiv240412478R, 2024arXiv2404.12478R\n",
      "2024-04-22 15:46:29,295 - INFO - Generated bibcodes for arXiv ID 2404.12481: 2024arXiv240412481L, 2024arXiv2404.12481L\n",
      "2024-04-22 15:46:29,296 - WARNING - No references found for bibcodes: 2024arXiv240412481L, 2024arXiv2404.12481L\n",
      "2024-04-22 15:46:29,296 - INFO - Generated bibcodes for arXiv ID 2404.12483: 2024arXiv240412483X, 2024arXiv2404.12483X\n",
      "2024-04-22 15:46:29,298 - WARNING - No references found for bibcodes: 2024arXiv240412483X, 2024arXiv2404.12483X\n",
      "2024-04-22 15:46:29,299 - INFO - Generated bibcodes for arXiv ID 2404.12484: 2024arXiv240412484Z, 2024arXiv2404.12484Z\n",
      "2024-04-22 15:46:29,300 - WARNING - No references found for bibcodes: 2024arXiv240412484Z, 2024arXiv2404.12484Z\n",
      "2024-04-22 15:46:29,300 - INFO - Generated bibcodes for arXiv ID 2404.12499: 2024arXiv240412499D, 2024arXiv2404.12499D\n",
      "2024-04-22 15:46:29,301 - WARNING - No references found for bibcodes: 2024arXiv240412499D, 2024arXiv2404.12499D\n",
      "2024-04-22 15:46:29,302 - INFO - Generated bibcodes for arXiv ID 2404.12534: 2024arXiv240412534S, 2024arXiv2404.12534S\n",
      "2024-04-22 15:46:29,302 - WARNING - No references found for bibcodes: 2024arXiv240412534S, 2024arXiv2404.12534S\n",
      "2024-04-22 15:46:29,311 - INFO - Generated bibcodes for arXiv ID 2404.12544: 2024arXiv240412544E, 2024arXiv2404.12544E\n",
      "2024-04-22 15:46:29,312 - WARNING - No references found for bibcodes: 2024arXiv240412544E, 2024arXiv2404.12544E\n",
      "2024-04-22 15:46:29,313 - INFO - Generated bibcodes for arXiv ID 2404.12553: 2024arXiv240412553J, 2024arXiv2404.12553J\n",
      "2024-04-22 15:46:29,313 - WARNING - No references found for bibcodes: 2024arXiv240412553J, 2024arXiv2404.12553J\n",
      "2024-04-22 15:46:29,314 - INFO - Generated bibcodes for arXiv ID 2404.12556: 2024arXiv240412556B, 2024arXiv2404.12556B\n",
      "2024-04-22 15:46:29,314 - WARNING - No references found for bibcodes: 2024arXiv240412556B, 2024arXiv2404.12556B\n",
      "2024-04-22 15:46:29,315 - INFO - Generated bibcodes for arXiv ID 2404.12583: 2024arXiv240412583K, 2024arXiv2404.12583K\n",
      "2024-04-22 15:46:29,316 - WARNING - No references found for bibcodes: 2024arXiv240412583K, 2024arXiv2404.12583K\n",
      "2024-04-22 15:46:29,316 - INFO - Generated bibcodes for arXiv ID 2404.12586: 2024arXiv240412586C, 2024arXiv2404.12586C\n",
      "2024-04-22 15:46:29,317 - WARNING - No references found for bibcodes: 2024arXiv240412586C, 2024arXiv2404.12586C\n",
      "2024-04-22 15:46:29,319 - INFO - Generated bibcodes for arXiv ID 2404.12589: 2024arXiv240412589C, 2024arXiv2404.12589C\n",
      "2024-04-22 15:46:29,320 - WARNING - No references found for bibcodes: 2024arXiv240412589C, 2024arXiv2404.12589C\n",
      "2024-04-22 15:46:29,320 - INFO - Generated bibcodes for arXiv ID 2404.12592: 2024arXiv240412592X, 2024arXiv2404.12592X\n",
      "2024-04-22 15:46:29,321 - WARNING - No references found for bibcodes: 2024arXiv240412592X, 2024arXiv2404.12592X\n",
      "2024-04-22 15:46:29,322 - INFO - Generated bibcodes for arXiv ID 2404.12597: 2024arXiv240412597Z, 2024arXiv2404.12597Z\n",
      "2024-04-22 15:46:29,323 - WARNING - No references found for bibcodes: 2024arXiv240412597Z, 2024arXiv2404.12597Z\n",
      "2024-04-22 15:46:29,323 - INFO - Generated bibcodes for arXiv ID 2404.12610: 2024arXiv240412610D, 2024arXiv2404.12610D\n",
      "2024-04-22 15:46:29,324 - WARNING - No references found for bibcodes: 2024arXiv240412610D, 2024arXiv2404.12610D\n",
      "2024-04-22 15:46:29,324 - INFO - Generated bibcodes for arXiv ID 2404.12613: 2024arXiv240412613L, 2024arXiv2404.12613L\n",
      "2024-04-22 15:46:29,325 - WARNING - No references found for bibcodes: 2024arXiv240412613L, 2024arXiv2404.12613L\n",
      "2024-04-22 15:46:29,325 - INFO - Generated bibcodes for arXiv ID 2404.12648: 2024arXiv240412648H, 2024arXiv2404.12648H\n",
      "2024-04-22 15:46:29,326 - WARNING - No references found for bibcodes: 2024arXiv240412648H, 2024arXiv2404.12648H\n",
      "2024-04-22 15:46:29,327 - INFO - Generated bibcodes for arXiv ID 2404.12657: 2024arXiv240412657J, 2024arXiv2404.12657J\n",
      "2024-04-22 15:46:29,327 - WARNING - No references found for bibcodes: 2024arXiv240412657J, 2024arXiv2404.12657J\n",
      "2024-04-22 15:46:29,328 - INFO - Generated bibcodes for arXiv ID 2404.12684: 2024arXiv240412684M, 2024arXiv2404.12684M\n",
      "2024-04-22 15:46:29,328 - WARNING - No references found for bibcodes: 2024arXiv240412684M, 2024arXiv2404.12684M\n",
      "2024-04-22 15:46:29,329 - INFO - Generated bibcodes for arXiv ID 2404.12685: 2024arXiv240412685M, 2024arXiv2404.12685M\n",
      "2024-04-22 15:46:29,329 - WARNING - No references found for bibcodes: 2024arXiv240412685M, 2024arXiv2404.12685M\n",
      "2024-04-22 15:46:29,330 - INFO - Generated bibcodes for arXiv ID 2404.12692: 2024arXiv240412692M, 2024arXiv2404.12692M\n",
      "2024-04-22 15:46:29,332 - WARNING - No references found for bibcodes: 2024arXiv240412692M, 2024arXiv2404.12692M\n",
      "2024-04-22 15:46:29,333 - INFO - Generated bibcodes for arXiv ID 2404.12696: 2024arXiv240412696W, 2024arXiv2404.12696W\n",
      "2024-04-22 15:46:29,334 - WARNING - No references found for bibcodes: 2024arXiv240412696W, 2024arXiv2404.12696W\n",
      "2024-04-22 15:46:29,337 - INFO - Generated bibcodes for arXiv ID 2404.12756: 2024arXiv240412756C, 2024arXiv2404.12756C\n",
      "2024-04-22 15:46:29,338 - WARNING - No references found for bibcodes: 2024arXiv240412756C, 2024arXiv2404.12756C\n",
      "2024-04-22 15:46:29,340 - INFO - Generated bibcodes for arXiv ID 2404.12812: 2024arXiv240412812C, 2024arXiv2404.12812C\n",
      "2024-04-22 15:46:29,342 - WARNING - No references found for bibcodes: 2024arXiv240412812C, 2024arXiv2404.12812C\n",
      "2024-04-22 15:46:29,343 - INFO - Generated bibcodes for arXiv ID 2404.12828: 2024arXiv240412828M, 2024arXiv2404.12828M\n",
      "2024-04-22 15:46:29,344 - WARNING - No references found for bibcodes: 2024arXiv240412828M, 2024arXiv2404.12828M\n",
      "2024-04-22 15:46:29,345 - INFO - Generated bibcodes for arXiv ID 2404.12862: 2024arXiv240412862E, 2024arXiv2404.12862E\n",
      "2024-04-22 15:46:29,345 - WARNING - No references found for bibcodes: 2024arXiv240412862E, 2024arXiv2404.12862E\n",
      "2024-04-22 15:46:29,346 - INFO - Generated bibcodes for arXiv ID 2404.12889: 2024arXiv240412889K, 2024arXiv2404.12889K\n",
      "2024-04-22 15:46:29,347 - WARNING - No references found for bibcodes: 2024arXiv240412889K, 2024arXiv2404.12889K\n",
      "2024-04-22 15:46:29,348 - INFO - Generated bibcodes for arXiv ID 2404.12923: 2024arXiv240412923L, 2024arXiv2404.12923L\n",
      "2024-04-22 15:46:29,350 - WARNING - No references found for bibcodes: 2024arXiv240412923L, 2024arXiv2404.12923L\n",
      "2024-04-22 15:46:29,350 - INFO - Generated bibcodes for arXiv ID 2404.12940: 2024arXiv240412940B, 2024arXiv2404.12940B\n",
      "2024-04-22 15:46:29,351 - WARNING - No references found for bibcodes: 2024arXiv240412940B, 2024arXiv2404.12940B\n",
      "2024-04-22 15:46:29,353 - INFO - Generated bibcodes for arXiv ID 2404.12943: 2024arXiv240412943C, 2024arXiv2404.12943C\n",
      "2024-04-22 15:46:29,354 - WARNING - No references found for bibcodes: 2024arXiv240412943C, 2024arXiv2404.12943C\n",
      "2024-04-22 15:46:29,355 - INFO - Generated bibcodes for arXiv ID 2404.12949: 2024arXiv240412949G, 2024arXiv2404.12949G\n",
      "2024-04-22 15:46:29,358 - WARNING - No references found for bibcodes: 2024arXiv240412949G, 2024arXiv2404.12949G\n",
      "2024-04-22 15:46:29,359 - INFO - Generated bibcodes for arXiv ID 2404.12967: 2024arXiv240412967J, 2024arXiv2404.12967J\n",
      "2024-04-22 15:46:29,361 - WARNING - No references found for bibcodes: 2024arXiv240412967J, 2024arXiv2404.12967J\n",
      "2024-04-22 15:46:29,362 - INFO - Generated bibcodes for arXiv ID 2404.12968: 2024arXiv240412968K, 2024arXiv2404.12968K\n",
      "2024-04-22 15:46:29,362 - WARNING - No references found for bibcodes: 2024arXiv240412968K, 2024arXiv2404.12968K\n",
      "2024-04-22 15:46:29,363 - INFO - Generated bibcodes for arXiv ID 2404.13016: 2024arXiv240413016L, 2024arXiv2404.13016L\n",
      "2024-04-22 15:46:29,364 - WARNING - No references found for bibcodes: 2024arXiv240413016L, 2024arXiv2404.13016L\n",
      "2024-04-22 15:46:29,365 - INFO - Generated bibcodes for arXiv ID 1906.02358: 2019arXiv190602358S, 2019arXiv1906.02358S\n",
      "2024-04-22 15:46:29,366 - WARNING - No references found for bibcodes: 2019arXiv190602358S, 2019arXiv1906.02358S\n",
      "2024-04-22 15:46:29,366 - INFO - Generated bibcodes for arXiv ID 1907.05325: 2019arXiv190705325M, 2019arXiv1907.05325M\n",
      "2024-04-22 15:46:29,367 - WARNING - No references found for bibcodes: 2019arXiv190705325M, 2019arXiv1907.05325M\n",
      "2024-04-22 15:46:29,368 - INFO - Generated bibcodes for arXiv ID 1912.10642: 2019arXiv191210642P, 2019arXiv1912.10642P\n",
      "2024-04-22 15:46:29,369 - WARNING - No references found for bibcodes: 2019arXiv191210642P, 2019arXiv1912.10642P\n",
      "2024-04-22 15:46:29,370 - INFO - Generated bibcodes for arXiv ID 2101.07223: 2021arXiv210107223A, 2021arXiv2101.07223A\n",
      "2024-04-22 15:46:29,371 - WARNING - No references found for bibcodes: 2021arXiv210107223A, 2021arXiv2101.07223A\n",
      "2024-04-22 15:46:29,371 - INFO - Generated bibcodes for arXiv ID 2101.09180: 2021arXiv210109180Z, 2021arXiv2101.09180Z\n",
      "2024-04-22 15:46:29,372 - WARNING - No references found for bibcodes: 2021arXiv210109180Z, 2021arXiv2101.09180Z\n",
      "2024-04-22 15:46:29,373 - INFO - Generated bibcodes for arXiv ID 2105.05716: 2021arXiv210505716R, 2021arXiv2105.05716R\n",
      "2024-04-22 15:46:29,374 - WARNING - No references found for bibcodes: 2021arXiv210505716R, 2021arXiv2105.05716R\n",
      "2024-04-22 15:46:29,377 - INFO - Generated bibcodes for arXiv ID 2111.01993: 2021arXiv211101993U, 2021arXiv2111.01993U\n",
      "2024-04-22 15:46:29,378 - WARNING - No references found for bibcodes: 2021arXiv211101993U, 2021arXiv2111.01993U\n",
      "2024-04-22 15:46:29,379 - INFO - Generated bibcodes for arXiv ID 2202.07595: 2022arXiv220207595H, 2022arXiv2202.07595H\n",
      "2024-04-22 15:46:29,379 - WARNING - No references found for bibcodes: 2022arXiv220207595H, 2022arXiv2202.07595H\n",
      "2024-04-22 15:46:29,383 - INFO - Generated bibcodes for arXiv ID 2203.07976: 2022arXiv220307976R, 2022arXiv2203.07976R\n",
      "2024-04-22 15:46:29,384 - WARNING - No references found for bibcodes: 2022arXiv220307976R, 2022arXiv2203.07976R\n",
      "2024-04-22 15:46:29,385 - INFO - Generated bibcodes for arXiv ID 2203.11593: 2022arXiv220311593J, 2022arXiv2203.11593J\n",
      "2024-04-22 15:46:29,392 - WARNING - No references found for bibcodes: 2022arXiv220311593J, 2022arXiv2203.11593J\n",
      "2024-04-22 15:46:29,393 - INFO - Generated bibcodes for arXiv ID 2204.10325: 2022arXiv220410325D, 2022arXiv2204.10325D\n",
      "2024-04-22 15:46:29,397 - WARNING - No references found for bibcodes: 2022arXiv220410325D, 2022arXiv2204.10325D\n",
      "2024-04-22 15:46:29,401 - INFO - Generated bibcodes for arXiv ID 2205.01438: 2022arXiv220501438Z, 2022arXiv2205.01438Z\n",
      "2024-04-22 15:46:29,403 - WARNING - No references found for bibcodes: 2022arXiv220501438Z, 2022arXiv2205.01438Z\n",
      "2024-04-22 15:46:29,403 - INFO - Generated bibcodes for arXiv ID 2205.02533: 2022arXiv220502533X, 2022arXiv2205.02533X\n",
      "2024-04-22 15:46:29,406 - INFO - References found for bibcodes: 2022arXiv220502533X, 2022arXiv2205.02533X\n",
      "2024-04-22 15:46:29,407 - INFO - Generated bibcodes for arXiv ID 2205.14223: 2022arXiv220514223Z, 2022arXiv2205.14223Z\n",
      "2024-04-22 15:46:29,407 - WARNING - No references found for bibcodes: 2022arXiv220514223Z, 2022arXiv2205.14223Z\n",
      "2024-04-22 15:46:29,408 - INFO - Generated bibcodes for arXiv ID 2206.09325: 2022arXiv220609325Z, 2022arXiv2206.09325Z\n",
      "2024-04-22 15:46:29,410 - WARNING - No references found for bibcodes: 2022arXiv220609325Z, 2022arXiv2206.09325Z\n",
      "2024-04-22 15:46:29,410 - INFO - Generated bibcodes for arXiv ID 2206.15101: 2022arXiv220615101R, 2022arXiv2206.15101R\n",
      "2024-04-22 15:46:29,411 - WARNING - No references found for bibcodes: 2022arXiv220615101R, 2022arXiv2206.15101R\n",
      "2024-04-22 15:46:29,412 - INFO - Generated bibcodes for arXiv ID 2209.01621: 2022arXiv220901621B, 2022arXiv2209.01621B\n",
      "2024-04-22 15:46:29,413 - WARNING - No references found for bibcodes: 2022arXiv220901621B, 2022arXiv2209.01621B\n",
      "2024-04-22 15:46:29,414 - INFO - Generated bibcodes for arXiv ID 2209.10192: 2022arXiv220910192J, 2022arXiv2209.10192J\n",
      "2024-04-22 15:46:29,415 - WARNING - No references found for bibcodes: 2022arXiv220910192J, 2022arXiv2209.10192J\n",
      "2024-04-22 15:46:29,415 - INFO - Generated bibcodes for arXiv ID 2210.01988: 2022arXiv221001988Z, 2022arXiv2210.01988Z\n",
      "2024-04-22 15:46:29,416 - WARNING - No references found for bibcodes: 2022arXiv221001988Z, 2022arXiv2210.01988Z\n",
      "2024-04-22 15:46:29,418 - INFO - Generated bibcodes for arXiv ID 2210.08298: 2022arXiv221008298F, 2022arXiv2210.08298F\n",
      "2024-04-22 15:46:29,418 - WARNING - No references found for bibcodes: 2022arXiv221008298F, 2022arXiv2210.08298F\n",
      "2024-04-22 15:46:29,419 - INFO - Generated bibcodes for arXiv ID 2211.02866: 2022arXiv221102866B, 2022arXiv2211.02866B\n",
      "2024-04-22 15:46:29,420 - WARNING - No references found for bibcodes: 2022arXiv221102866B, 2022arXiv2211.02866B\n",
      "2024-04-22 15:46:29,420 - INFO - Generated bibcodes for arXiv ID 2211.07440: 2022arXiv221107440R, 2022arXiv2211.07440R\n",
      "2024-04-22 15:46:29,421 - WARNING - No references found for bibcodes: 2022arXiv221107440R, 2022arXiv2211.07440R\n",
      "2024-04-22 15:46:29,422 - INFO - Generated bibcodes for arXiv ID 2211.11424: 2022arXiv221111424X, 2022arXiv2211.11424X\n",
      "2024-04-22 15:46:29,422 - WARNING - No references found for bibcodes: 2022arXiv221111424X, 2022arXiv2211.11424X\n",
      "2024-04-22 15:46:29,423 - INFO - Generated bibcodes for arXiv ID 2212.03218: 2022arXiv221203218K, 2022arXiv2212.03218K\n",
      "2024-04-22 15:46:29,424 - WARNING - No references found for bibcodes: 2022arXiv221203218K, 2022arXiv2212.03218K\n",
      "2024-04-22 15:46:29,425 - INFO - Generated bibcodes for arXiv ID 2301.00185: 2022arXiv230100185S, 2022arXiv2301.00185S\n",
      "2024-04-22 15:46:29,425 - WARNING - No references found for bibcodes: 2022arXiv230100185S, 2022arXiv2301.00185S\n",
      "2024-04-22 15:46:29,426 - INFO - Generated bibcodes for arXiv ID 2301.00812: 2022arXiv230100812Y, 2022arXiv2301.00812Y\n",
      "2024-04-22 15:46:29,426 - WARNING - No references found for bibcodes: 2022arXiv230100812Y, 2022arXiv2301.00812Y\n",
      "2024-04-22 15:46:29,427 - INFO - Generated bibcodes for arXiv ID 2301.06280: 2023arXiv230106280J, 2023arXiv2301.06280J\n",
      "2024-04-22 15:46:29,428 - WARNING - No references found for bibcodes: 2023arXiv230106280J, 2023arXiv2301.06280J\n",
      "2024-04-22 15:46:29,428 - INFO - Generated bibcodes for arXiv ID 2301.06520: 2023arXiv230106520M, 2023arXiv2301.06520M\n",
      "2024-04-22 15:46:29,429 - WARNING - No references found for bibcodes: 2023arXiv230106520M, 2023arXiv2301.06520M\n",
      "2024-04-22 15:46:29,430 - INFO - Generated bibcodes for arXiv ID 2303.04467: 2023arXiv230304467S, 2023arXiv2303.04467S\n",
      "2024-04-22 15:46:29,430 - WARNING - No references found for bibcodes: 2023arXiv230304467S, 2023arXiv2303.04467S\n",
      "2024-04-22 15:46:29,431 - INFO - Generated bibcodes for arXiv ID 2303.10216: 2023arXiv230310216K, 2023arXiv2303.10216K\n",
      "2024-04-22 15:46:29,431 - WARNING - No references found for bibcodes: 2023arXiv230310216K, 2023arXiv2303.10216K\n",
      "2024-04-22 15:46:29,432 - INFO - Generated bibcodes for arXiv ID 2303.10322: 2023arXiv230310322S, 2023arXiv2303.10322S\n",
      "2024-04-22 15:46:29,433 - WARNING - No references found for bibcodes: 2023arXiv230310322S, 2023arXiv2303.10322S\n",
      "2024-04-22 15:46:29,433 - INFO - Generated bibcodes for arXiv ID 2303.12342: 2023arXiv230312342L, 2023arXiv2303.12342L\n",
      "2024-04-22 15:46:29,434 - WARNING - No references found for bibcodes: 2023arXiv230312342L, 2023arXiv2303.12342L\n",
      "2024-04-22 15:46:29,435 - INFO - Generated bibcodes for arXiv ID 2303.16421: 2023arXiv230316421B, 2023arXiv2303.16421B\n",
      "2024-04-22 15:46:29,435 - INFO - References found for bibcodes: 2023arXiv230316421B, 2023arXiv2303.16421B\n",
      "2024-04-22 15:46:29,436 - INFO - Generated bibcodes for arXiv ID 2304.00372: 2023arXiv230400372L, 2023arXiv2304.00372L\n",
      "2024-04-22 15:46:29,436 - WARNING - No references found for bibcodes: 2023arXiv230400372L, 2023arXiv2304.00372L\n",
      "2024-04-22 15:46:29,437 - INFO - Generated bibcodes for arXiv ID 2304.05166: 2023arXiv230405166M, 2023arXiv2304.05166M\n",
      "2024-04-22 15:46:29,438 - WARNING - No references found for bibcodes: 2023arXiv230405166M, 2023arXiv2304.05166M\n",
      "2024-04-22 15:46:29,438 - INFO - Generated bibcodes for arXiv ID 2304.07468: 2023arXiv230407468K, 2023arXiv2304.07468K\n",
      "2024-04-22 15:46:29,439 - WARNING - No references found for bibcodes: 2023arXiv230407468K, 2023arXiv2304.07468K\n",
      "2024-04-22 15:46:29,439 - INFO - Generated bibcodes for arXiv ID 2304.09779: 2023arXiv230409779S, 2023arXiv2304.09779S\n",
      "2024-04-22 15:46:29,440 - WARNING - No references found for bibcodes: 2023arXiv230409779S, 2023arXiv2304.09779S\n",
      "2024-04-22 15:46:29,441 - INFO - Generated bibcodes for arXiv ID 2304.10286: 2023arXiv230410286P, 2023arXiv2304.10286P\n",
      "2024-04-22 15:46:29,441 - WARNING - No references found for bibcodes: 2023arXiv230410286P, 2023arXiv2304.10286P\n",
      "2024-04-22 15:46:29,442 - INFO - Generated bibcodes for arXiv ID 2304.13029: 2023arXiv230413029M, 2023arXiv2304.13029M\n",
      "2024-04-22 15:46:29,442 - INFO - References found for bibcodes: 2023arXiv230413029M, 2023arXiv2304.13029M\n",
      "2024-04-22 15:46:29,444 - INFO - Generated bibcodes for arXiv ID 2305.03803: 2023arXiv230503803H, 2023arXiv2305.03803H\n",
      "2024-04-22 15:46:29,445 - WARNING - No references found for bibcodes: 2023arXiv230503803H, 2023arXiv2305.03803H\n",
      "2024-04-22 15:46:29,445 - INFO - Generated bibcodes for arXiv ID 2305.07877: 2023arXiv230507877G, 2023arXiv2305.07877G\n",
      "2024-04-22 15:46:29,446 - WARNING - No references found for bibcodes: 2023arXiv230507877G, 2023arXiv2305.07877G\n",
      "2024-04-22 15:46:29,446 - INFO - Generated bibcodes for arXiv ID 2305.18453: 2023arXiv230518453D, 2023arXiv2305.18453D\n",
      "2024-04-22 15:46:29,447 - INFO - References found for bibcodes: 2023arXiv230518453D, 2023arXiv2305.18453D\n",
      "2024-04-22 15:46:29,448 - INFO - Generated bibcodes for arXiv ID 2306.03027: 2023arXiv230603027G, 2023arXiv2306.03027G\n",
      "2024-04-22 15:46:29,448 - WARNING - No references found for bibcodes: 2023arXiv230603027G, 2023arXiv2306.03027G\n",
      "2024-04-22 15:46:29,449 - INFO - Generated bibcodes for arXiv ID 2306.06449: 2023arXiv230606449B, 2023arXiv2306.06449B\n",
      "2024-04-22 15:46:29,449 - WARNING - No references found for bibcodes: 2023arXiv230606449B, 2023arXiv2306.06449B\n",
      "2024-04-22 15:46:29,450 - INFO - Generated bibcodes for arXiv ID 2306.08386: 2023arXiv230608386L, 2023arXiv2306.08386L\n",
      "2024-04-22 15:46:29,450 - WARNING - No references found for bibcodes: 2023arXiv230608386L, 2023arXiv2306.08386L\n",
      "2024-04-22 15:46:29,451 - INFO - Generated bibcodes for arXiv ID 2306.08553: 2023arXiv230608553J, 2023arXiv2306.08553J\n",
      "2024-04-22 15:46:29,452 - WARNING - No references found for bibcodes: 2023arXiv230608553J, 2023arXiv2306.08553J\n",
      "2024-04-22 15:46:29,453 - INFO - Generated bibcodes for arXiv ID 2306.10091: 2023arXiv230610091P, 2023arXiv2306.10091P\n",
      "2024-04-22 15:46:29,453 - WARNING - No references found for bibcodes: 2023arXiv230610091P, 2023arXiv2306.10091P\n",
      "2024-04-22 15:46:29,454 - INFO - Generated bibcodes for arXiv ID 2306.17808: 2023arXiv230617808D, 2023arXiv2306.17808D\n",
      "2024-04-22 15:46:29,455 - WARNING - No references found for bibcodes: 2023arXiv230617808D, 2023arXiv2306.17808D\n",
      "2024-04-22 15:46:29,455 - INFO - Generated bibcodes for arXiv ID 2307.01004: 2023arXiv230701004Y, 2023arXiv2307.01004Y\n",
      "2024-04-22 15:46:29,456 - WARNING - No references found for bibcodes: 2023arXiv230701004Y, 2023arXiv2307.01004Y\n",
      "2024-04-22 15:46:29,456 - INFO - Generated bibcodes for arXiv ID 2307.02620: 2023arXiv230702620B, 2023arXiv2307.02620B\n",
      "2024-04-22 15:46:29,457 - WARNING - No references found for bibcodes: 2023arXiv230702620B, 2023arXiv2307.02620B\n",
      "2024-04-22 15:46:29,457 - INFO - Generated bibcodes for arXiv ID 2307.02913: 2023arXiv230702913P, 2023arXiv2307.02913P\n",
      "2024-04-22 15:46:29,459 - WARNING - No references found for bibcodes: 2023arXiv230702913P, 2023arXiv2307.02913P\n",
      "2024-04-22 15:46:29,460 - INFO - Generated bibcodes for arXiv ID 2307.06917: 2023arXiv230706917M, 2023arXiv2307.06917M\n",
      "2024-04-22 15:46:29,460 - WARNING - No references found for bibcodes: 2023arXiv230706917M, 2023arXiv2307.06917M\n",
      "2024-04-22 15:46:29,461 - INFO - Generated bibcodes for arXiv ID 2307.07520: 2023arXiv230707520B, 2023arXiv2307.07520B\n",
      "2024-04-22 15:46:29,462 - WARNING - No references found for bibcodes: 2023arXiv230707520B, 2023arXiv2307.07520B\n",
      "2024-04-22 15:46:29,462 - INFO - Generated bibcodes for arXiv ID 2307.08523: 2023arXiv230708523F, 2023arXiv2307.08523F\n",
      "2024-04-22 15:46:29,463 - WARNING - No references found for bibcodes: 2023arXiv230708523F, 2023arXiv2307.08523F\n",
      "2024-04-22 15:46:29,475 - INFO - Generated bibcodes for arXiv ID 2307.12162: 2023arXiv230712162C, 2023arXiv2307.12162C\n",
      "2024-04-22 15:46:29,476 - WARNING - No references found for bibcodes: 2023arXiv230712162C, 2023arXiv2307.12162C\n",
      "2024-04-22 15:46:29,477 - INFO - Generated bibcodes for arXiv ID 2307.12513: 2023arXiv230712513W, 2023arXiv2307.12513W\n",
      "2024-04-22 15:46:29,478 - WARNING - No references found for bibcodes: 2023arXiv230712513W, 2023arXiv2307.12513W\n",
      "2024-04-22 15:46:29,479 - INFO - Generated bibcodes for arXiv ID 2307.15517: 2023arXiv230715517C, 2023arXiv2307.15517C\n",
      "2024-04-22 15:46:29,480 - WARNING - No references found for bibcodes: 2023arXiv230715517C, 2023arXiv2307.15517C\n",
      "2024-04-22 15:46:29,481 - INFO - Generated bibcodes for arXiv ID 2308.04725: 2023arXiv230804725F, 2023arXiv2308.04725F\n",
      "2024-04-22 15:46:29,481 - WARNING - No references found for bibcodes: 2023arXiv230804725F, 2023arXiv2308.04725F\n",
      "2024-04-22 15:46:29,482 - INFO - Generated bibcodes for arXiv ID 2308.06979: 2023arXiv230806979F, 2023arXiv2308.06979F\n",
      "2024-04-22 15:46:29,482 - WARNING - No references found for bibcodes: 2023arXiv230806979F, 2023arXiv2308.06979F\n",
      "2024-04-22 15:46:29,483 - INFO - Generated bibcodes for arXiv ID 2308.08945: 2023arXiv230808945A, 2023arXiv2308.08945A\n",
      "2024-04-22 15:46:29,484 - WARNING - No references found for bibcodes: 2023arXiv230808945A, 2023arXiv2308.08945A\n",
      "2024-04-22 15:46:29,484 - INFO - Generated bibcodes for arXiv ID 2308.09084: 2023arXiv230809084Y, 2023arXiv2308.09084Y\n",
      "2024-04-22 15:46:29,485 - WARNING - No references found for bibcodes: 2023arXiv230809084Y, 2023arXiv2308.09084Y\n",
      "2024-04-22 15:46:29,486 - INFO - Generated bibcodes for arXiv ID 2308.10838: 2023arXiv230810838P, 2023arXiv2308.10838P\n",
      "2024-04-22 15:46:29,486 - WARNING - No references found for bibcodes: 2023arXiv230810838P, 2023arXiv2308.10838P\n",
      "2024-04-22 15:46:29,487 - INFO - Generated bibcodes for arXiv ID 2308.11098: 2023arXiv230811098P, 2023arXiv2308.11098P\n",
      "2024-04-22 15:46:29,487 - INFO - References found for bibcodes: 2023arXiv230811098P, 2023arXiv2308.11098P\n",
      "2024-04-22 15:46:29,489 - INFO - Generated bibcodes for arXiv ID 2308.12462: 2023arXiv230812462Z, 2023arXiv2308.12462Z\n",
      "2024-04-22 15:46:29,489 - WARNING - No references found for bibcodes: 2023arXiv230812462Z, 2023arXiv2308.12462Z\n",
      "2024-04-22 15:46:29,490 - INFO - Generated bibcodes for arXiv ID 2308.16042: 2023arXiv230816042L, 2023arXiv2308.16042L\n",
      "2024-04-22 15:46:29,490 - WARNING - No references found for bibcodes: 2023arXiv230816042L, 2023arXiv2308.16042L\n",
      "2024-04-22 15:46:29,491 - INFO - Generated bibcodes for arXiv ID 2309.00380: 2023arXiv230900380H, 2023arXiv2309.00380H\n",
      "2024-04-22 15:46:29,492 - WARNING - No references found for bibcodes: 2023arXiv230900380H, 2023arXiv2309.00380H\n",
      "2024-04-22 15:46:29,492 - INFO - Generated bibcodes for arXiv ID 2309.02208: 2023arXiv230902208F, 2023arXiv2309.02208F\n",
      "2024-04-22 15:46:29,493 - WARNING - No references found for bibcodes: 2023arXiv230902208F, 2023arXiv2309.02208F\n",
      "2024-04-22 15:46:29,494 - INFO - Generated bibcodes for arXiv ID 2309.02354: 2023arXiv230902354L, 2023arXiv2309.02354L\n",
      "2024-04-22 15:46:29,494 - WARNING - No references found for bibcodes: 2023arXiv230902354L, 2023arXiv2309.02354L\n",
      "2024-04-22 15:46:29,495 - INFO - Generated bibcodes for arXiv ID 2309.04001: 2023arXiv230904001R, 2023arXiv2309.04001R\n",
      "2024-04-22 15:46:29,495 - WARNING - No references found for bibcodes: 2023arXiv230904001R, 2023arXiv2309.04001R\n",
      "2024-04-22 15:46:29,496 - INFO - Generated bibcodes for arXiv ID 2309.05442: 2023arXiv230905442M, 2023arXiv2309.05442M\n",
      "2024-04-22 15:46:29,497 - WARNING - No references found for bibcodes: 2023arXiv230905442M, 2023arXiv2309.05442M\n",
      "2024-04-22 15:46:29,501 - INFO - Generated bibcodes for arXiv ID 2309.05927: 2023arXiv230905927L, 2023arXiv2309.05927L\n",
      "2024-04-22 15:46:29,502 - WARNING - No references found for bibcodes: 2023arXiv230905927L, 2023arXiv2309.05927L\n",
      "2024-04-22 15:46:29,503 - INFO - Generated bibcodes for arXiv ID 2309.06978: 2023arXiv230906978R, 2023arXiv2309.06978R\n",
      "2024-04-22 15:46:29,504 - WARNING - No references found for bibcodes: 2023arXiv230906978R, 2023arXiv2309.06978R\n",
      "2024-04-22 15:46:29,504 - INFO - Generated bibcodes for arXiv ID 2309.07918: 2023arXiv230907918X, 2023arXiv2309.07918X\n",
      "2024-04-22 15:46:29,505 - WARNING - No references found for bibcodes: 2023arXiv230907918X, 2023arXiv2309.07918X\n",
      "2024-04-22 15:46:29,505 - INFO - Generated bibcodes for arXiv ID 2309.12830: 2023arXiv230912830S, 2023arXiv2309.12830S\n",
      "2024-04-22 15:46:29,506 - WARNING - No references found for bibcodes: 2023arXiv230912830S, 2023arXiv2309.12830S\n",
      "2024-04-22 15:46:29,506 - INFO - Generated bibcodes for arXiv ID 2309.15136: 2023arXiv230915136P, 2023arXiv2309.15136P\n",
      "2024-04-22 15:46:29,507 - WARNING - No references found for bibcodes: 2023arXiv230915136P, 2023arXiv2309.15136P\n",
      "2024-04-22 15:46:29,508 - INFO - Generated bibcodes for arXiv ID 2309.16108: 2023arXiv230916108B, 2023arXiv2309.16108B\n",
      "2024-04-22 15:46:29,508 - WARNING - No references found for bibcodes: 2023arXiv230916108B, 2023arXiv2309.16108B\n",
      "2024-04-22 15:46:29,509 - INFO - Generated bibcodes for arXiv ID 2310.00074: 2023arXiv231000074H, 2023arXiv2310.00074H\n",
      "2024-04-22 15:46:29,509 - WARNING - No references found for bibcodes: 2023arXiv231000074H, 2023arXiv2310.00074H\n",
      "2024-04-22 15:46:29,510 - INFO - Generated bibcodes for arXiv ID 2310.00132: 2023arXiv231000132L, 2023arXiv2310.00132L\n",
      "2024-04-22 15:46:29,510 - INFO - References found for bibcodes: 2023arXiv231000132L, 2023arXiv2310.00132L\n",
      "2024-04-22 15:46:29,511 - INFO - Generated bibcodes for arXiv ID 2310.01143: 2023arXiv231001143G, 2023arXiv2310.01143G\n",
      "2024-04-22 15:46:29,511 - WARNING - No references found for bibcodes: 2023arXiv231001143G, 2023arXiv2310.01143G\n",
      "2024-04-22 15:46:29,512 - INFO - Generated bibcodes for arXiv ID 2310.01880: 2023arXiv231001880Y, 2023arXiv2310.01880Y\n",
      "2024-04-22 15:46:29,512 - WARNING - No references found for bibcodes: 2023arXiv231001880Y, 2023arXiv2310.01880Y\n",
      "2024-04-22 15:46:29,513 - INFO - Generated bibcodes for arXiv ID 2310.01945: 2023arXiv231001945K, 2023arXiv2310.01945K\n",
      "2024-04-22 15:46:29,514 - WARNING - No references found for bibcodes: 2023arXiv231001945K, 2023arXiv2310.01945K\n",
      "2024-04-22 15:46:29,514 - INFO - Generated bibcodes for arXiv ID 2310.03624: 2023arXiv231003624S, 2023arXiv2310.03624S\n",
      "2024-04-22 15:46:29,515 - WARNING - No references found for bibcodes: 2023arXiv231003624S, 2023arXiv2310.03624S\n",
      "2024-04-22 15:46:29,515 - INFO - Generated bibcodes for arXiv ID 2310.05898: 2023arXiv231005898C, 2023arXiv2310.05898C\n",
      "2024-04-22 15:46:29,516 - WARNING - No references found for bibcodes: 2023arXiv231005898C, 2023arXiv2310.05898C\n",
      "2024-04-22 15:46:29,517 - INFO - Generated bibcodes for arXiv ID 2310.07446: 2023arXiv231007446Z, 2023arXiv2310.07446Z\n",
      "2024-04-22 15:46:29,517 - WARNING - No references found for bibcodes: 2023arXiv231007446Z, 2023arXiv2310.07446Z\n",
      "2024-04-22 15:46:29,518 - INFO - Generated bibcodes for arXiv ID 2310.07983: 2023arXiv231007983G, 2023arXiv2310.07983G\n",
      "2024-04-22 15:46:29,518 - WARNING - No references found for bibcodes: 2023arXiv231007983G, 2023arXiv2310.07983G\n",
      "2024-04-22 15:46:29,519 - INFO - Generated bibcodes for arXiv ID 2310.10404: 2023arXiv231010404K, 2023arXiv2310.10404K\n",
      "2024-04-22 15:46:29,519 - WARNING - No references found for bibcodes: 2023arXiv231010404K, 2023arXiv2310.10404K\n",
      "2024-04-22 15:46:29,520 - INFO - Generated bibcodes for arXiv ID 2310.12079: 2023arXiv231012079L, 2023arXiv2310.12079L\n",
      "2024-04-22 15:46:29,520 - WARNING - No references found for bibcodes: 2023arXiv231012079L, 2023arXiv2310.12079L\n",
      "2024-04-22 15:46:29,521 - INFO - Generated bibcodes for arXiv ID 2310.14724: 2023arXiv231014724W, 2023arXiv2310.14724W\n",
      "2024-04-22 15:46:29,521 - INFO - References found for bibcodes: 2023arXiv231014724W, 2023arXiv2310.14724W\n",
      "2024-04-22 15:46:29,522 - INFO - Generated bibcodes for arXiv ID 2310.17304: 2023arXiv231017304X, 2023arXiv2310.17304X\n",
      "2024-04-22 15:46:29,523 - WARNING - No references found for bibcodes: 2023arXiv231017304X, 2023arXiv2310.17304X\n",
      "2024-04-22 15:46:29,523 - INFO - Generated bibcodes for arXiv ID 2310.18784: 2023arXiv231018784A, 2023arXiv2310.18784A\n",
      "2024-04-22 15:46:29,524 - WARNING - No references found for bibcodes: 2023arXiv231018784A, 2023arXiv2310.18784A\n",
      "2024-04-22 15:46:29,524 - INFO - Generated bibcodes for arXiv ID 2310.19454: 2023arXiv231019454K, 2023arXiv2310.19454K\n",
      "2024-04-22 15:46:29,525 - WARNING - No references found for bibcodes: 2023arXiv231019454K, 2023arXiv2310.19454K\n",
      "2024-04-22 15:46:29,525 - INFO - Generated bibcodes for arXiv ID 2311.00944: 2023arXiv231100944S, 2023arXiv2311.00944S\n",
      "2024-04-22 15:46:29,526 - WARNING - No references found for bibcodes: 2023arXiv231100944S, 2023arXiv2311.00944S\n",
      "2024-04-22 15:46:29,526 - INFO - Generated bibcodes for arXiv ID 2311.02909: 2023arXiv231102909T, 2023arXiv2311.02909T\n",
      "2024-04-22 15:46:29,527 - WARNING - No references found for bibcodes: 2023arXiv231102909T, 2023arXiv2311.02909T\n",
      "2024-04-22 15:46:29,527 - INFO - Generated bibcodes for arXiv ID 2311.03056: 2023arXiv231103056G, 2023arXiv2311.03056G\n",
      "2024-04-22 15:46:29,528 - WARNING - No references found for bibcodes: 2023arXiv231103056G, 2023arXiv2311.03056G\n",
      "2024-04-22 15:46:29,529 - INFO - Generated bibcodes for arXiv ID 2311.04205: 2023arXiv231104205D, 2023arXiv2311.04205D\n",
      "2024-04-22 15:46:29,529 - INFO - References found for bibcodes: 2023arXiv231104205D, 2023arXiv2311.04205D\n",
      "2024-04-22 15:46:29,530 - INFO - Generated bibcodes for arXiv ID 2311.04253: 2023arXiv231104253R, 2023arXiv2311.04253R\n",
      "2024-04-22 15:46:29,530 - WARNING - No references found for bibcodes: 2023arXiv231104253R, 2023arXiv2311.04253R\n",
      "2024-04-22 15:46:29,531 - INFO - Generated bibcodes for arXiv ID 2311.05734: 2023arXiv231105734S, 2023arXiv2311.05734S\n",
      "2024-04-22 15:46:29,531 - WARNING - No references found for bibcodes: 2023arXiv231105734S, 2023arXiv2311.05734S\n",
      "2024-04-22 15:46:29,532 - INFO - Generated bibcodes for arXiv ID 2311.08097: 2023arXiv231108097R, 2023arXiv2311.08097R\n",
      "2024-04-22 15:46:29,532 - WARNING - No references found for bibcodes: 2023arXiv231108097R, 2023arXiv2311.08097R\n",
      "2024-04-22 15:46:29,533 - INFO - Generated bibcodes for arXiv ID 2311.08990: 2023arXiv231108990K, 2023arXiv2311.08990K\n",
      "2024-04-22 15:46:29,533 - WARNING - No references found for bibcodes: 2023arXiv231108990K, 2023arXiv2311.08990K\n",
      "2024-04-22 15:46:29,534 - INFO - Generated bibcodes for arXiv ID 2311.09049: 2023arXiv231109049Z, 2023arXiv2311.09049Z\n",
      "2024-04-22 15:46:29,534 - WARNING - No references found for bibcodes: 2023arXiv231109049Z, 2023arXiv2311.09049Z\n",
      "2024-04-22 15:46:29,535 - INFO - Generated bibcodes for arXiv ID 2110.15517: 2021arXiv211015517H, 2021arXiv2110.15517H\n",
      "2024-04-22 15:46:29,535 - WARNING - No references found for bibcodes: 2021arXiv211015517H, 2021arXiv2110.15517H\n",
      "2024-04-22 15:46:29,536 - INFO - Generated bibcodes for arXiv ID 2202.04573: 2022arXiv220204573H, 2022arXiv2202.04573H\n",
      "2024-04-22 15:46:29,536 - WARNING - No references found for bibcodes: 2022arXiv220204573H, 2022arXiv2202.04573H\n",
      "2024-04-22 15:46:29,537 - INFO - Generated bibcodes for arXiv ID 2211.13605: 2022arXiv221113605V, 2022arXiv2211.13605V\n",
      "2024-04-22 15:46:29,537 - WARNING - No references found for bibcodes: 2022arXiv221113605V, 2022arXiv2211.13605V\n",
      "2024-04-22 15:46:29,538 - INFO - Generated bibcodes for arXiv ID 2307.12695: 2023arXiv230712695B, 2023arXiv2307.12695B\n",
      "2024-04-22 15:46:29,538 - WARNING - No references found for bibcodes: 2023arXiv230712695B, 2023arXiv2307.12695B\n",
      "2024-04-22 15:46:29,539 - INFO - Generated bibcodes for arXiv ID 2308.00179: 2023arXiv230800179A, 2023arXiv2308.00179A\n",
      "2024-04-22 15:46:29,540 - WARNING - No references found for bibcodes: 2023arXiv230800179A, 2023arXiv2308.00179A\n",
      "2024-04-22 15:46:29,540 - INFO - Generated bibcodes for arXiv ID 2310.05971: 2023arXiv231005971O, 2023arXiv2310.05971O\n",
      "2024-04-22 15:46:29,541 - WARNING - No references found for bibcodes: 2023arXiv231005971O, 2023arXiv2310.05971O\n",
      "2024-04-22 15:46:29,541 - INFO - Generated bibcodes for arXiv ID 2311.05883: 2023arXiv231105883C, 2023arXiv2311.05883C\n",
      "2024-04-22 15:46:29,542 - WARNING - No references found for bibcodes: 2023arXiv231105883C, 2023arXiv2311.05883C\n",
      "2024-04-22 15:46:29,542 - INFO - Generated bibcodes for arXiv ID 2402.11394: 2024arXiv240211394P, 2024arXiv2402.11394P\n",
      "2024-04-22 15:46:29,543 - WARNING - No references found for bibcodes: 2024arXiv240211394P, 2024arXiv2402.11394P\n",
      "2024-04-22 15:46:29,543 - INFO - Generated bibcodes for arXiv ID 2403.18521: 2024arXiv240318521G, 2024arXiv2403.18521G\n",
      "2024-04-22 15:46:29,544 - WARNING - No references found for bibcodes: 2024arXiv240318521G, 2024arXiv2403.18521G\n",
      "2024-04-22 15:46:29,545 - INFO - Generated bibcodes for arXiv ID 2404.12462: 2024arXiv240412462Z, 2024arXiv2404.12462Z\n",
      "2024-04-22 15:46:29,545 - WARNING - No references found for bibcodes: 2024arXiv240412462Z, 2024arXiv2404.12462Z\n",
      "2024-04-22 15:46:29,546 - INFO - Generated bibcodes for arXiv ID 2404.12477: 2024arXiv240412477S, 2024arXiv2404.12477S\n",
      "2024-04-22 15:46:29,546 - WARNING - No references found for bibcodes: 2024arXiv240412477S, 2024arXiv2404.12477S\n",
      "2024-04-22 15:46:29,547 - INFO - Generated bibcodes for arXiv ID 2404.12581: 2024arXiv240412581W, 2024arXiv2404.12581W\n",
      "2024-04-22 15:46:29,547 - WARNING - No references found for bibcodes: 2024arXiv240412581W, 2024arXiv2404.12581W\n",
      "2024-04-22 15:46:29,548 - INFO - Generated bibcodes for arXiv ID 2404.12882: 2024arXiv240412882K, 2024arXiv2404.12882K\n",
      "2024-04-22 15:46:29,548 - WARNING - No references found for bibcodes: 2024arXiv240412882K, 2024arXiv2404.12882K\n",
      "2024-04-22 15:46:29,549 - INFO - Generated bibcodes for arXiv ID 2404.12974: 2024arXiv240412974F, 2024arXiv2404.12974F\n",
      "2024-04-22 15:46:29,549 - WARNING - No references found for bibcodes: 2024arXiv240412974F, 2024arXiv2404.12974F\n",
      "2024-04-22 15:46:29,550 - INFO - Generated bibcodes for arXiv ID 2404.12988: 2024arXiv240412988Z, 2024arXiv2404.12988Z\n",
      "2024-04-22 15:46:29,550 - WARNING - No references found for bibcodes: 2024arXiv240412988Z, 2024arXiv2404.12988Z\n",
      "2024-04-22 15:46:29,551 - INFO - Generated bibcodes for arXiv ID 2404.12997: 2024arXiv240412997H, 2024arXiv2404.12997H\n",
      "2024-04-22 15:46:29,551 - WARNING - No references found for bibcodes: 2024arXiv240412997H, 2024arXiv2404.12997H\n"
     ]
    }
   ],
   "source": [
    "merge_and_fetch_references(files, output_file, references_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae34ef306687e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_references_json(references_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00f1d3-2773-4318-9f50-43c6504648b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
